{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **I. Import all the necessary libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **II. Import and Preprocess the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Student_Performance.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = test[['Hours Studied', 'Previous Scores', 'Extracurricular Activities',\n",
    "       'Sleep Hours', 'Sample Question Papers Practiced']]\n",
    "y_test = test['Performance Index']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train[['Hours Studied', 'Previous Scores', 'Extracurricular Activities',\n",
    "       'Sleep Hours', 'Sample Question Papers Practiced']]\n",
    "y_train = train['Performance Index']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hours Studied</th>\n",
       "      <th>Previous Scores</th>\n",
       "      <th>Extracurricular Activities</th>\n",
       "      <th>Sleep Hours</th>\n",
       "      <th>Sample Question Papers Practiced</th>\n",
       "      <th>Performance Index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>99</td>\n",
       "      <td>Yes</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>91.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>82</td>\n",
       "      <td>No</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>51</td>\n",
       "      <td>Yes</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>52</td>\n",
       "      <td>Yes</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>75</td>\n",
       "      <td>No</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>66.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "      <td>Yes</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>7</td>\n",
       "      <td>64</td>\n",
       "      <td>Yes</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>58.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>6</td>\n",
       "      <td>83</td>\n",
       "      <td>Yes</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>74.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>9</td>\n",
       "      <td>97</td>\n",
       "      <td>Yes</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>95.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>7</td>\n",
       "      <td>74</td>\n",
       "      <td>No</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>64.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Hours Studied  Previous Scores Extracurricular Activities  Sleep Hours  \\\n",
       "0                 7               99                        Yes            9   \n",
       "1                 4               82                         No            4   \n",
       "2                 8               51                        Yes            7   \n",
       "3                 5               52                        Yes            5   \n",
       "4                 7               75                         No            8   \n",
       "...             ...              ...                        ...          ...   \n",
       "9995              1               49                        Yes            4   \n",
       "9996              7               64                        Yes            8   \n",
       "9997              6               83                        Yes            8   \n",
       "9998              9               97                        Yes            7   \n",
       "9999              7               74                         No            8   \n",
       "\n",
       "      Sample Question Papers Practiced  Performance Index  \n",
       "0                                    1               91.0  \n",
       "1                                    2               65.0  \n",
       "2                                    2               45.0  \n",
       "3                                    2               36.0  \n",
       "4                                    5               66.0  \n",
       "...                                ...                ...  \n",
       "9995                                 2               23.0  \n",
       "9996                                 5               58.0  \n",
       "9997                                 5               74.0  \n",
       "9998                                 0               95.0  \n",
       "9999                                 1               64.0  \n",
       "\n",
       "[10000 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns to normalize: All except for extracurricular activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['Hours Studied', 'Previous Scores', 'Extracurricular Activities', 'Sleep Hours', 'Sample Question Papers Practiced']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_meta = [x_test,x_train]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/36/khn_y8zn7qqflczfpst8cbfc0000gn/T/ipykernel_3848/1995739649.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  j[i] = z_score_normalize(j[i])\n",
      "/var/folders/36/khn_y8zn7qqflczfpst8cbfc0000gn/T/ipykernel_3848/1995739649.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  j[i] = z_score_normalize(j[i])\n",
      "/var/folders/36/khn_y8zn7qqflczfpst8cbfc0000gn/T/ipykernel_3848/1995739649.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  j[i] = z_score_normalize(j[i])\n",
      "/var/folders/36/khn_y8zn7qqflczfpst8cbfc0000gn/T/ipykernel_3848/1995739649.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  j[i] = z_score_normalize(j[i])\n",
      "/var/folders/36/khn_y8zn7qqflczfpst8cbfc0000gn/T/ipykernel_3848/1995739649.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  j['Extracurricular Activities'] = j['Extracurricular Activities'].map(dict_map)\n",
      "/var/folders/36/khn_y8zn7qqflczfpst8cbfc0000gn/T/ipykernel_3848/1995739649.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  j['Extracurricular Activities'] = j['Extracurricular Activities'].map(dict_map)\n",
      "/var/folders/36/khn_y8zn7qqflczfpst8cbfc0000gn/T/ipykernel_3848/1995739649.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  j[i] = z_score_normalize(j[i])\n",
      "/var/folders/36/khn_y8zn7qqflczfpst8cbfc0000gn/T/ipykernel_3848/1995739649.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  j[i] = z_score_normalize(j[i])\n",
      "/var/folders/36/khn_y8zn7qqflczfpst8cbfc0000gn/T/ipykernel_3848/1995739649.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  j[i] = z_score_normalize(j[i])\n",
      "/var/folders/36/khn_y8zn7qqflczfpst8cbfc0000gn/T/ipykernel_3848/1995739649.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  j[i] = z_score_normalize(j[i])\n"
     ]
    }
   ],
   "source": [
    "x_meta = [x_test,x_train]\n",
    "y_meta = [y_test,y_train]\n",
    "for i in features:\n",
    "    for j in x_meta:\n",
    "        if i == 'Extracurricular Activities':\n",
    "            dict_map = {\n",
    "                'Yes' : 1,\n",
    "                'No' : 0,\n",
    "            }\n",
    "            j['Extracurricular Activities'] = j['Extracurricular Activities'].map(dict_map)\n",
    "        else:\n",
    "            j[i] = z_score_normalize(j[i])\n",
    "del x_meta,y_meta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hours Studied</th>\n",
       "      <th>Previous Scores</th>\n",
       "      <th>Extracurricular Activities</th>\n",
       "      <th>Sleep Hours</th>\n",
       "      <th>Sample Question Papers Practiced</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8700</th>\n",
       "      <td>0.875</td>\n",
       "      <td>0.406780</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6833</th>\n",
       "      <td>0.500</td>\n",
       "      <td>0.067797</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6713</th>\n",
       "      <td>1.000</td>\n",
       "      <td>0.796610</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7161</th>\n",
       "      <td>0.375</td>\n",
       "      <td>0.542373</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6907</th>\n",
       "      <td>0.500</td>\n",
       "      <td>0.271186</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7990</th>\n",
       "      <td>0.500</td>\n",
       "      <td>0.898305</td>\n",
       "      <td>1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.555556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5563</th>\n",
       "      <td>0.875</td>\n",
       "      <td>0.203390</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>0.250</td>\n",
       "      <td>0.237288</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6872</th>\n",
       "      <td>0.125</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2248</th>\n",
       "      <td>0.375</td>\n",
       "      <td>0.440678</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7500 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Hours Studied  Previous Scores  Extracurricular Activities  Sleep Hours  \\\n",
       "8700          0.875         0.406780                           1          0.6   \n",
       "6833          0.500         0.067797                           1          0.6   \n",
       "6713          1.000         0.796610                           1          0.6   \n",
       "7161          0.375         0.542373                           0          0.2   \n",
       "6907          0.500         0.271186                           1          1.0   \n",
       "...             ...              ...                         ...          ...   \n",
       "7990          0.500         0.898305                           1          0.4   \n",
       "5563          0.875         0.203390                           0          0.4   \n",
       "276           0.250         0.237288                           0          0.8   \n",
       "6872          0.125         0.033898                           1          0.4   \n",
       "2248          0.375         0.440678                           1          0.6   \n",
       "\n",
       "      Sample Question Papers Practiced  \n",
       "8700                          0.444444  \n",
       "6833                          0.555556  \n",
       "6713                          0.222222  \n",
       "7161                          0.333333  \n",
       "6907                          0.666667  \n",
       "...                                ...  \n",
       "7990                          0.555556  \n",
       "5563                          0.000000  \n",
       "276                           0.777778  \n",
       "6872                          0.000000  \n",
       "2248                          1.000000  \n",
       "\n",
       "[7500 rows x 5 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8700    58.0\n",
       "6833    27.0\n",
       "6713    85.0\n",
       "7161    56.0\n",
       "6907    46.0\n",
       "        ... \n",
       "7990    80.0\n",
       "5563    43.0\n",
       "276     35.0\n",
       "6872    17.0\n",
       "2248    48.0\n",
       "Name: Performance Index, Length: 7500, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **III. Train the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 5) [[-443335.65497888 -482875.01310323 -409235.36487639 -418586.03102112\n",
      "  -418304.65709153]]\n",
      "Training MSE Loss:  [ 9252300.09928978  1470000.86295494 21702356.47468669 ...\n",
      "  3118492.97583986   521355.20893311  5867897.59481394]\n",
      "(1, 5) [[-395378.13706152 -435481.06006074 -358301.67709946 -370027.67997838\n",
      "  -369875.35973864]]\n",
      "Training MSE Loss:  [ 6983056.46054809   766735.77054854 17857477.19633475 ...\n",
      "  2204514.59167392   243683.29574304  4070856.87819131]\n",
      "(1, 5) [[-352764.15647886 -393343.87608379 -313130.29239743 -326899.96931981\n",
      "  -326861.66161688]]\n",
      "Training MSE Loss:  [ 5237417.89733524   333848.05733133 14755087.43162531 ...\n",
      "  1525442.79460905    85245.76818582  2752572.90192003]\n",
      "(1, 5) [[-314896.51827711 -355876.06380369 -273076.07571556 -288595.86203528\n",
      "  -288658.19249881]]\n",
      "Training MSE Loss:  [ 3899354.60717968    98943.03653375 12246232.20463235 ...\n",
      "  1026963.12477233    13001.64425687  1799773.00412688]\n",
      "(1, 5) [[-281244.79215385 -322555.89495648 -237566.03126211 -254576.18645156\n",
      "  -254727.26160334]]\n",
      "Training MSE Loss:  [2.87799665e+06 6.72791081e+03 1.02125186e+07 ... 6.66752454e+05\n",
      " 1.83660417e+03 1.12454739e+06]\n",
      "(1, 5) [[-251337.8480879  -292919.96849324 -206091.23704957 -224362.04880624\n",
      "  -224591.29094401]]\n",
      "Training MSE Loss:  [2102242.19417504   15186.75412207 8559763.66245929 ...  411889.33475994\n",
      "   32775.65681089  658823.13780733]\n",
      "(1, 5) [[-224757.22649852 -266556.68952623 -178199.68117062 -197528.0941087\n",
      "  -197826.09464194]]\n",
      "Training MSE Loss:  [1516513.64805009   92588.31927753 7212972.55172153 ...  236817.44863309\n",
      "   91586.76641431  350025.09375224]\n",
      "(1, 5) [[-201131.24963209 -243100.47734007 -153489.89899332 -173696.52044825\n",
      "  -174054.90962303]]\n",
      "Training MSE Loss:  [1077418.31049749  215147.86361384 6112366.10216366 ...  121744.70451793\n",
      "  167691.27054666  157670.73867373]\n",
      "(1, 5) [[-180129.79130577 -222226.6209571  -131605.32173122 -152531.76251272\n",
      "  -152943.09369347]]\n",
      "Training MSE Loss:  [ 751121.43021062  365202.2862417  5210238.63177041 ...   51385.37863624\n",
      "  253314.88527364   50699.68337563]\n",
      "(1, 5) [[-161459.63140258 -203646.70985934 -112229.25685547 -133735.7694985\n",
      "  -134193.41637935]]\n",
      "Training MSE Loss:  [ 511280.44728333  529787.88637525 4468471.0045155  ...   13972.32851311\n",
      "  342827.24194026    5380.86489818]\n",
      "(1, 5) [[-144860.32974178 -187104.5755634   -95080.42970779 -117043.81095815\n",
      "  -117541.87625834]]\n",
      "Training MSE Loss:  [3.37421127e+05 6.99533695e+05 3.85656174e+06 ... 4.81766940e+02\n",
      " 4.32229041e+05 3.67382720e+03]\n",
      "(1, 5) [[-130100.56125738 -172372.68693414  -79909.02357107 -102220.75156112\n",
      "  -102753.98592073]]\n",
      "Training MSE Loss:  [ 213661.59872122  867801.94843272 3350067.5623099  ...    4025.28054465\n",
      "  518754.68019217   31946.77250038]\n",
      "(1, 5) [[-116974.86090973 -159248.94850833  -66493.16246906  -89057.74234234\n",
      "   -89621.47227803]]\n",
      "Training MSE Loss:  [ 127710.24785289 1030021.94112259 2929367.65663608 ...   19373.39185037\n",
      "  600565.11420899   79974.78425456]\n",
      "(1, 5) [[-105300.73252135 -147553.85677101  -54635.7871973   -77369.28187456\n",
      "   -77959.34578254]]\n",
      "Training MSE Loss:  [  70079.09218213 1183175.0163708  2578683.91156372 ...   42582.54350485\n",
      "  676511.14169162  140157.94526621]\n",
      "(1, 5) [[ -94916.08084965 -137127.9743648   -44161.88062121  -66990.60600635\n",
      "   -67603.29731293]]\n",
      "Training MSE Loss:  [  33466.69057938 1325397.55776088 2285303.66416622 ...   70703.35834428\n",
      "  745951.57954635  206911.93321583]\n",
      "(1, 5) [[ -85676.93075798 -127829.68668682  -34916.00319281  -57775.36943152\n",
      "   -58407.38609234]]\n",
      "Training MSE Loss:  [  12274.36772079 1455675.97027908 2038962.6838029  ...  101552.7384519\n",
      "  808614.15123719  276193.80414025]\n",
      "(1, 5) [[ -77455.40138704 -119533.20930159  -26760.10400282  -49593.58646272\n",
      "   -50241.98610087]]\n",
      "Training MSE Loss:  [   2227.24559436 1573613.26816022 1831355.00238753 ...  133536.07793721\n",
      "  864489.55319832  345133.65054834]\n",
      "(1, 5) [[ -70137.90681709 -112126.81812849  -19571.57656292  -42329.80202969\n",
      "   -42991.96208253]]\n",
      "Training MSE Loss:  [7.76324840e+01 1.67925130e+06 1.65574320e+06 ... 1.65508789e+05\n",
      " 9.13751239e+05 4.11749101e+05]\n",
      "(1, 5) [[ -63623.55789888 -105511.27749689  -13241.53195688  -35881.46716272\n",
      "   -36555.0494774 ]]\n",
      "Training MSE Loss:  [   3373.0987964  1772936.12400302 1506648.26726483 ...  196668.64426181\n",
      "  956695.09373298  474724.56999817]\n",
      "(1, 5) [[-57822.74276237 -99598.44394736  -7673.26505823 -30157.49609943\n",
      "  -30840.41547996]]\n",
      "Training MSE Loss:  [  10275.33386051 1855216.73590309 1379602.58792556 ...  226472.25679739\n",
      "  993694.43781421  533241.06190501]\n",
      "(1, 5) [[-52655.86602664 -94310.02612986  -2780.89222932 -25076.98470917\n",
      "  -25767.38097323]]\n",
      "Training MSE Loss:  [  19418.84367165 1926769.56676776 1270952.92203956 ...  254570.4425436\n",
      " 1025166.82698062  586845.38375533]\n",
      "(1, 5) [[-48052.22896802 -89576.48334732   1511.8586702  -20568.07219934\n",
      "  -21264.28535267]]\n",
      "Training MSE Loss:  [  29800.88562947 1988342.75478513 1177703.13444233 ...  280758.34355445\n",
      " 1051549.87108594  635350.04046829]\n",
      "(1, 5) [[-43949.03488703 -85336.04724361   5272.72299392 -16566.93008451\n",
      "  -17267.47826438]]\n",
      "Training MSE Loss:  [  40695.87602466 2040715.58598526 1097388.45416613 ...  304937.07037107\n",
      " 1073283.93457129  678756.96889167]\n",
      "(1, 5) [[-40290.50567659 -81533.85286821   8561.85386298 -13016.86419023\n",
      "  -13720.42406844]]\n",
      "Training MSE Loss:  [  51588.95453578 2084669.49343508 1027974.80772183 ...  327084.32245594\n",
      " 1090800.05121597  717199.75879325]\n",
      "(1, 5) [[-37027.09715925 -78121.16688913  11432.66999784  -9867.51705385\n",
      "  -10572.90642454]]\n",
      "Training MSE Loss:  [  62124.53023315 2120967.82253595  967778.0980624  ...  347231.99345818\n",
      " 1104511.76599458  750900.17557486]\n",
      "(1, 5) [[-34114.80215056 -75054.70209259  13932.60875918  -7074.15949801\n",
      "   -7780.32180593]]\n",
      "Training MSE Loss:  [  72066.53092912 2150342.20018587  915399.36639391 ...  365449.19975604\n",
      " 1114809.91059435  780135.71657935]\n",
      "(1, 5) [[-31514.53144101 -72296.00852264  16103.79499599  -4597.061407\n",
      "   -5303.05199954]]\n",
      "Training MSE Loss:  [  81267.78370315 2173483.83879658  869672.61499031 ...  381829.50999633\n",
      " 1122059.54853471  805215.65225767]\n",
      "(1, 5) [[-29191.56398474 -69810.93269205  17983.63511346  -2400.93285106\n",
      "   -3105.90676133]]\n",
      "Training MSE Loss:  [  89646.50969445 2191038.49091933  829622.73450332 ...  396481.41994473\n",
      " 1126598.50443819  826463.56749824]\n",
      "(1, 5) [[-27115.05855796 -67569.13725418  19605.34472157   -454.42769352\n",
      "   -1157.62878346]]\n",
      "Training MSE Loss:  [  97168.35281716 2203604.06994274  794431.50580668 ...  409521.32633432\n",
      " 1128737.03083155  844204.86030638]\n",
      "(1, 5) [[-25257.62001429 -65543.674376    20998.41728992   1270.29730484\n",
      "     569.54599335]]\n",
      "Training MSE Loss:  [ 103832.70515115 2211730.18538871  763410.06328787 ...  421068.41773442\n",
      " 1128758.27355384  858758.00090927]\n",
      "(1, 5) [[-23594.91403332 -63710.60680854  22189.04040465   2797.97408889\n",
      "    2100.27890981]]\n",
      "Training MSE Loss:  [ 109662.36128398 2215919.02221813  735976.53668433 ...  431241.02935734\n",
      " 1126919.28020402  870428.62483841]\n",
      "(1, 5) [[-22105.32494056 -62048.67132199  23200.46548549   4150.57113079\n",
      "    3456.47545648]]\n",
      "Training MSE Loss:  [ 114695.74549742 2216627.13363151  711637.85020971 ...  440154.10974019\n",
      " 1123452.3604077   879505.74481139]\n",
      "(1, 5) [[-20769.65178338 -60538.97976912  24053.33616629   5347.60236485\n",
      "    4657.593362  ]]\n",
      "Training MSE Loss:  [ 118981.12175074 2214267.82503802  689974.86516443 ...  447917.52633974\n",
      " 1118566.65620064  886259.53104555]\n",
      "(1, 5) [[-19570.83838588 -59164.75357     24765.97996051   6406.40158339\n",
      "    5720.91623671]]\n",
      "Training MSE Loss:  [ 122572.32663752 2209213.89017415  670630.21679934 ...  454634.99895313\n",
      " 1112449.81879709  890940.2380164 ]\n",
      "(1, 5) [[-18493.7335838  -57911.08788159  25354.66731654   7342.36615501\n",
      "    6661.79662281]]\n",
      "Training MSE Loss:  [ 125525.66754227 2201800.52427275  653298.32684955 ...  460403.4982198\n",
      " 1105269.71700188  893777.95550857]\n",
      "(1, 5) [[-17524.87826522 -56764.74213325  25833.84170865   8169.17349503\n",
      "    7493.87187134]]\n",
      "Training MSE Loss:  [ 127897.70812783 2192328.28802206  637717.17694653 ...  465312.98417902\n",
      " 1097176.12454762  894982.93929298]\n",
      "(1, 5) [[-16652.31622022 -55713.95398049  26216.32400198   8898.97333465\n",
      "    8229.25588398]]\n",
      "Training MSE Loss:  [ 129743.72579252 2181066.03318147  623661.51063984 ...  469446.38924133\n",
      " 1088302.35026503  894746.33677054]\n",
      "(1, 5) [[-15865.42613752 -54748.27405883  26513.49396773   9542.55849429\n",
      "    8878.70941807]]\n",
      "Training MSE Loss:  [ 131116.67458308 2168253.72877587  610937.19743132 ...  472879.77278194\n",
      " 1078766.78745421  893241.16927221]\n",
      "(1, 5) [[-15154.77238391 -53858.41921208  26735.45150315  10109.51656466\n",
      "    9451.79135155]]\n",
      "Training MSE Loss:  [ 132066.52517649 2154105.14783314  599376.54453854 ...  475682.59229476\n",
      " 1068674.36807521  890623.46839848]\n",
      "(1, 5) [[-14511.9724665  -53036.14212981  26891.15982561  10608.36462985\n",
      "    9956.99303639]]\n",
      "Training MSE Loss:  [ 132639.88326218 2138810.39026898  588834.38382315 ...  477918.04977158\n",
      " 1058117.91416459  887033.49117086]\n",
      "(1, 5) [[-13929.5793126  -52274.11555923  26988.57265579  11046.66892861\n",
      "   10401.85763143]]\n",
      "Training MSE Loss:  [ 132879.81078596 2122538.22901445  579184.79462914 ...  479643.4825653\n",
      " 1047179.38379217  882596.95969751]\n",
      "(1, 5) [[-13400.97671063 -51565.82946224  27034.74718031  11431.15113748\n",
      "   10793.08609379]]\n",
      "Training MSE Loss:  [ 132825.79248519 2105438.27479864  570318.34990722 ...  480910.77614652\n",
      " 1035931.01234805  877426.28696978]\n",
      "(1, 5) [[-12920.28644065 -50905.49967015  27035.9443836   11767.78277175\n",
      "   11136.63132052]]\n",
      "Training MSE Loss:  [ 132513.80407674 2087642.96090183  562139.79432366 ...  481766.78240772\n",
      " 1024436.35234535  871621.76243371]\n",
      "(1, 5) [[-12482.28578755 -50287.98675078  26997.71816115  12061.86903251\n",
      "   11437.78176535]]\n",
      "Training MSE Loss:  [ 131976.44923486 2069269.35326924  554566.08015057 ...  482253.73193182\n",
      " 1012751.21650751  865272.68000567]\n",
      "(1, 5) [[-12082.33427607 -49708.72394611  26924.99446846  12318.12327972\n",
      "   11701.23570714]]\n",
      "Training MSE Loss:  [ 131243.14080856 2050420.79407739  547524.70047042 ...  482409.63225278\n",
      " 1000924.52988875  858458.39791098]\n",
      "(1, 5) [[-11716.30859683 -49163.65316633  26822.14161971  12540.73317948\n",
      "   11931.16721526]]\n",
      "Training MSE Loss:  [ 130340.30811968 2031188.3885242   540952.27028195 ...  482268.64685459\n",
      "  988999.09731215  851249.32465743]\n",
      "(1, 5) [[-11380.5448073  -48649.16813973  26693.03272565  12733.41945618\n",
      "   12131.28474016]]\n",
      "Training MSE Loss:  [ 129291.61707938 2011652.34553951  534793.31500927 ...  481861.45168444\n",
      "  977012.29262384  843707.82902838]\n",
      "(1, 5) [[-11071.78699466 -48162.06391819  26541.10114962  12899.48807637\n",
      "   12304.88315364]]\n",
      "Training MSE Loss:  [ 128118.19359554 1991883.18349099  528999.23312099 ...  481215.56745384\n",
      "  964996.67624649  835889.07451926]\n",
      "(1, 5) [[-10787.14167791 -47699.49202793  26369.38976218  13041.87659869\n",
      "   12454.88997119]]\n",
      "Training MSE Loss:  [ 126838.84357658 1971942.81194814  523527.40540193 ...  480355.66709237\n",
      "  952980.54734149  827841.78039575]\n",
      "(1, 5) [[-10524.03730762 -47258.92063412  26180.59468788  13163.19534214\n",
      "   12583.90640684]]\n",
      "Training MSE Loss:  [ 125470.26497393 1951885.50028384  518340.42815998 ...  479303.85849774\n",
      "  940988.43660996  819608.91271935]\n",
      "(1, 5) [[-10280.18829346 -46838.09915907  25977.10415977  13265.76395193\n",
      "   12694.24383814]]\n",
      "Training MSE Loss:  [ 124027.24890278 1931758.74342748  513405.4515089  ...  478079.94326862\n",
      "  929041.54541586  811228.30941354]\n",
      "(1, 5) [[-10053.56305323 -46435.02685584  25761.03302877  13351.64387748\n",
      "   12787.95619465]]\n",
      "Training MSE Loss:  [ 122522.86806532 1911604.03450367  508693.60701895 ...  476701.65247071\n",
      "  917158.13653165  802733.24384911]\n",
      "(1, 5) [[ -9842.35563384 -46047.92489518  25534.2534137   13422.66721966\n",
      "   12866.8687255 ]]\n",
      "Training MSE Loss:  [ 120968.6515669  1891457.55344479  504179.51160653 ...  475184.86071641\n",
      "  905353.8814065   794152.93159708]\n",
      "(1, 5) [[ -9644.96050475 -45675.21157305  25298.42192338  13480.46235315\n",
      "   12932.60355087]]\n",
      "Training MSE Loss:  [ 119374.7458402  1871350.77999043  499840.83665306 ...  473543.77996994\n",
      "  893642.16845597  785512.98499975]\n",
      "(1, 5) [[ -9459.95016948 -45315.48028966  25055.00383417  13526.47668439\n",
      "   12986.60235708]]\n",
      "Training MSE Loss:  [ 117750.06183295 1851311.03880765  495657.9330888  ...  471791.1345475\n",
      "  882034.37648016  776835.82009636]\n",
      "(1, 5) [[ -9286.05527975 -44967.47999036  24805.29456323  13561.99686551\n",
      "   13030.14655447]]\n",
      "Training MSE Loss:  [ 116102.4089163  1831361.98380135  491613.50462029 ...  469938.31878869\n",
      "  870540.11694151  768141.02024704]\n",
      "(1, 5) [[ -9122.14697281 -44630.09779302  24550.43874004  13588.16674855\n",
      "   13064.37518182]]\n",
      "Training MSE Loss:  [ 114438.6161688  1811524.02804555  487692.32247642 ...  467995.53884435\n",
      "  859167.44847849  759445.66055614]\n",
      "(1, 5) [[ -8967.22118314 -44302.34355753  24291.44714448  13606.00333261\n",
      "   13090.3008093 ]]\n",
      "Training MSE Loss:  [ 112764.64181131 1791814.72516249  483880.97604391 ...  465971.93997044\n",
      "  847923.06669987  750764.59692558]\n",
      "(1, 5) [[ -8820.38470794 -43983.33618023  24029.21175023  13616.41092842\n",
      "   13108.82366353]]\n",
      "Training MSE Loss:  [ 111085.67163202 1772249.10741111  480167.65459425 ...  463875.72064541\n",
      "  836812.47199737  742110.72328462]\n",
      "(1, 5) [[ -8680.84283027 -43672.29142041  23764.51908518  13620.19373951\n",
      "   13120.74417377]]\n",
      "Training MSE Loss:  [ 109406.20726392 1752839.9852226   476541.95600024 ...  461714.23474718\n",
      "  825840.11783203  733495.20025655]\n",
      "(1, 5) [[ -8547.88832572 -43368.51108772  23498.0620971   13618.06703699\n",
      "   13126.77411551]]\n",
      "Training MSE Loss:  [ 107730.14517101 1733598.21243807  472994.71892598 ...  459494.08293889\n",
      "  815009.54169213  724927.65824263]\n",
      "(1, 5) [[ -8420.89169796 -43071.37343814  23230.45069167  13610.66708536\n",
      "   13127.54650845]]\n",
      "Training MSE Loss:  [ 106060.84717351 1714532.92106205  469517.87546769 ...  457221.19432518\n",
      "  804323.48068598  716416.37763451]\n",
      "(1, 5) [[ -8299.29250564 -42780.32464361  22962.22109131  13598.55995871\n",
      "   13123.62440807]]\n",
      "Training MSE Loss:  [ 104401.20330343 1695651.72894349  466104.3216404  ...  454900.89935372\n",
      "  793783.97352042  707968.44861113]\n",
      "(1, 5) [[ -8182.59165886 -42494.8712151   22693.84414674  13582.24937159\n",
      "   13115.50871447]]\n",
      "Training MSE Loss:  [ 102753.68773453 1676960.9234313   462747.80346047 ...  452537.99485289\n",
      "  783392.45042462  699589.91273793]\n",
      "(1, 5) [[ -8070.34457664 -42214.57327264  22425.73271819  13562.18363464\n",
      "   13103.64510829]]\n",
      "Training MSE Loss:  [ 101120.40847907 1658465.62372233  459442.81667552 ...  450136.80201678\n",
      "  773149.81240636  691285.88836473]\n",
      "(1, 5) [[ -7962.15510911 -41939.03856755  22158.24823052  13538.76183268\n",
      "   13088.43021152]]\n",
      "Training MSE Loss:  [  99503.1514901  1640169.92432261  456184.51845185 ...  447701.21807314\n",
      "  763056.50107329  683060.68161511]\n",
      "(1, 5) [[ -7857.67013913 -41667.9171729   21891.70649442  13512.3393125\n",
      "   13070.21705953]]\n",
      "Training MSE Loss:  [  97903.41975506 1622077.02177636  452968.64954976 ...  445234.76230007\n",
      "  753112.56011343  674917.88457367]\n",
      "(1, 5) [[ -7756.57478715 -41400.89676728  21626.38287589  13483.2325572\n",
      "   13049.31996166]]\n",
      "Training MSE Loss:  [  96322.46791423 1604189.32657783  449791.46570738 ...  442740.61699201\n",
      "  743317.68940525  666860.46210789]\n",
      "(1, 5) [[ -7658.58815197 -41137.69844577  21362.51688682  13451.72351587\n",
      "   13026.01881846]]\n",
      "Training MSE Loss:  [  94761.33288815 1586508.56196726  446649.67711704 ...  440221.66391604\n",
      "  733671.29261755  658890.82860756]\n",
      "(1, 5) [[ -7563.45952755 -40878.07299901  21100.31626137  13418.06344942\n",
      "   13000.56295646]]\n",
      "Training MSE Loss:  [  93220.86095148 1569035.85112107  443540.39501902 ...  437680.51674451\n",
      "  724172.51906086  651010.91578545]\n",
      "(1, 5) [[ -7470.9650425  -40621.79760816  20839.96057574  13382.47634668\n",
      "   12973.17453445]]\n",
      "Training MSE Loss:  [  91701.73164721 1551771.79407586  440461.08455996 ...  435119.54990051\n",
      "  714820.30046482  643222.23255812]\n",
      "(1, 5) [[ -7380.90467515 -40368.67290913  20581.60446234  13345.16195874\n",
      "   12944.05156896]]\n",
      "Training MSE Loss:  [  90204.47889481 1534716.53557355  437409.52316871 ...  432540.924207\n",
      "  705613.38327806  635525.91791286]\n",
      "(1, 5) [[ -7293.09960209 -40118.52038495  20325.3804637   13306.29849436\n",
      "   12913.37062182]]\n",
      "Training MSE Loss:  [  88729.50960925 1517869.8248802   434383.7637947  ...  429946.60968959\n",
      "  696550.35701823  627922.78756614]\n",
      "(1, 5) [[ -7207.38984302 -39871.18004938  20071.40156643  13266.04501417\n",
      "   12881.2891873 ]]\n",
      "Training MSE Loss:  [  87277.12011405 1501231.06851041  431382.10243372 ...  427338.40584587\n",
      "  687629.67913857  620413.37512876]\n",
      "(1, 5) [[ -7123.63216861 -39626.50838947  19819.76345105  13224.54355743\n",
      "   12847.94781258]]\n",
      "Training MSE Loss:  [  85847.51060087 1484799.37668246  428403.04943581 ...  424717.95966078\n",
      "  678849.69682326  612997.96841215]\n",
      "(1, 5) [[ -7041.69824219 -39384.37653789  19570.54648942  13181.92103123\n",
      "   12813.47198129]]\n",
      "Training MSE Loss:  [  84440.79786085 1468573.60423461  425445.30415108 ...  422086.78161736\n",
      "  670208.66607539  605676.64143848]\n",
      "(1, 5) [[ -6961.47296897 -39144.66864945  19323.81751799  13138.29088871\n",
      "   12777.97378665]]\n",
      "Training MSE Loss:  [  83057.02648774 1452552.38664862  422507.73252244 ...  419446.25992541\n",
      "  661704.76841924  598449.28265329]\n",
      "(1, 5) [[ -6882.85302956 -38907.280459    19079.63141197  13093.75461979\n",
      "   12741.5534177 ]]\n",
      "Training MSE Loss:  [  81696.17873054 1436734.17175207  419589.34728076 ...  416797.67316632\n",
      "  653336.12550071  591315.61978227]\n",
      "(1, 5) [[ -6805.74557741 -38672.11800037  18838.03248274  13048.4030755\n",
      "   12704.30047951]]\n",
      "Training MSE Loss:  [  80358.18315367 1421117.247605    416689.2904393  ...  414142.20153054\n",
      "  645100.81183632  584275.24172328]\n",
      "(1, 5) [[ -6730.06708159 -38439.09646836  18599.05571818  13002.31764441\n",
      "   12666.29516595]]\n",
      "Training MSE Loss:  [  79042.9222443  1405699.76701754  413806.81781958 ...  411480.93680502\n",
      "  636996.86593201  577327.61781962]\n",
      "(1, 5) [[ -6655.74229876 -38208.13920788  18362.7278835   12955.57129775\n",
      "   12627.60930155]]\n",
      "Training MSE Loss:  [  77750.23909085 1390479.76909373  410941.28537301 ...  408814.89125047\n",
      "  629022.29996672  570472.11482085]\n",
      "(1, 5) [[ -6582.70336002 -37979.17681585  18129.06849827  12908.22951787\n",
      "   12588.30726691]]\n",
      "Training MSE Loss:  [  76479.9432424  1375455.19815047  408092.13708971 ...  406145.00549276\n",
      "  621175.10821292  563708.01180183]\n",
      "(1, 5) [[ -6510.88895952 -37752.14634348  17898.09070345  12860.35112308\n",
      "   12548.44682092]]\n",
      "Training MSE Loss:  [  75231.81584587 1360623.9203201   405258.894311   ...  403472.15553933\n",
      "  613453.27434582  557034.51327947]\n",
      "(1, 5) [[ -6440.24363376 -37526.99058745  17669.80203063  12811.98900049\n",
      "   12508.0798311 ]]\n",
      "Training MSE Loss:  [  74005.61414677 1345983.7381092   402441.14628279 ...  400797.15901863\n",
      "  605854.77777511  550450.76073877]\n",
      "(1, 5) [[ -6370.71712117 -37303.65746031  17444.2050846   12763.19075704\n",
      "   12467.25292254]]\n",
      "Training MSE Loss:  [  72801.07542933 1331532.40315435  399638.54180685 ...  398120.78073022\n",
      "  598377.5991173   543955.84275525]\n",
      "(1, 5) [[ -6302.26379322 -37082.09943106  17221.29814886  12713.99929802\n",
      "   12426.00805431]]\n",
      "Training MSE Loss:  [  71617.92046294 1317267.62738742  396850.78186273 ...  395443.73758305\n",
      "  591019.72491259  537548.80387894]\n",
      "(1, 5) [[ -6234.84214894 -36862.27302814  17001.07572262  12664.45334098\n",
      "   12384.38303166]]\n",
      "Training MSE Loss:  [  70455.8565138  1303187.0927984   394077.61308875 ...  392766.7029908\n",
      "  583779.15167813  531228.65242598]\n",
      "(1, 5) [[ -6168.41436574 -36644.13839787  16783.52899716  12614.58787249\n",
      "   12342.41196107]]\n",
      "Training MSE Loss:  [  69314.57997413 1289288.45996145  391318.82202261 ...  390090.31078558\n",
      "  576653.88937848  524994.36730664]\n",
      "(1, 5) [[ -6102.94590037 -36427.65891208  16568.6462781   12564.43455387\n",
      "   12300.12565449]]\n",
      "Training MSE Loss:  [  68193.77865471 1275569.37547077  388574.23001422 ...  387415.15870424\n",
      "  569641.96438434  518844.90400388]\n",
      "(1, 5) [[ -6038.40513428 -36212.8008195   16356.41335983  12514.02208186\n",
      "   12257.55198866]]\n",
      "Training MSE Loss:  [  67093.13378141 1262027.47841558  385843.6887333  ...  384741.81149559\n",
      "  562741.42198262  512779.19980264]\n",
      "(1, 5) [[ -5974.76305851 -35999.53293597  16146.81385735  12463.37650903\n",
      "   12214.71622426]]\n",
      "Training MSE Loss:  [  66012.32173148 1248660.40600836  383127.0762031  ...  382070.80369127\n",
      "  555950.32849288  506796.17835899]\n",
      "(1, 5) [[ -5911.99299361 -35787.82636916  15939.82950036  12412.52152863\n",
      "   12171.64128959]]\n",
      "Training MSE Loss:  [  64951.0155409  1235465.79846719  380424.29329974 ...  379402.64207825\n",
      "  549266.77303899  500894.75368722]\n",
      "(1, 5) [[ -5850.07034077 -35577.65427394  15735.44039372  12361.47872767\n",
      "   12128.34803261]]\n",
      "Training MSE Loss:  [  63908.88621084 1222441.30324115  377735.26066343 ...  376737.80790661\n",
      "  542688.86901895  495073.83363424]\n",
      "(1, 5) [[ -5788.97236058 -35368.99163494  15533.62524823  12310.26781195\n",
      "   12084.85544497]]\n",
      "Training MSE Loss:  [  62885.60383746 1209584.5786574   375059.91597423 ...  374076.75886264\n",
      "  536214.75531059  489332.32290219]\n",
      "(1, 5) [[ -5728.67797645 -35161.81507332  15334.36158482  12258.90680603\n",
      "   12041.18086111]]\n",
      "Training MSE Loss:  [  61880.83858666 1196893.29705906  372398.21155023 ...  371419.9308335\n",
      "  529842.59724639  483669.12567329]\n",
      "(1, 5) [[ -5669.16759976 -34956.10267502  15137.6259154   12207.41223104\n",
      "   11997.34013519]]\n",
      "Training MSE Loss:  [  60894.26153275 1184365.14749537  369750.11223109 ...  368767.73948714\n",
      "  523570.58738684  478083.14788436]\n",
      "(1, 5) [[ -5610.42297448 -34751.83383803  14943.39390275  12155.79926273\n",
      "   11953.34779844]]\n",
      "Training MSE Loss:  [  59925.54537775 1171997.83801783  367115.59351396 ...  366120.58168829\n",
      "  517396.94611802  472573.29919297]\n",
      "(1, 5) [[ -5552.42703903 -34548.9891367   14751.64050192  12104.08187208\n",
      "   11909.21719893]]\n",
      "Training MSE Loss:  [  58974.36506603 1159789.09763016  364494.63991276 ...  363478.83676894\n",
      "  511319.92209616  467138.49467236]\n",
      "(1, 5) [[ -5495.1638034  -34347.55020101  14562.34008524  12052.27295022\n",
      "   11864.96062592]]\n",
      "Training MSE Loss:  [  58040.39830716 1147736.67793402  361887.24351495 ...  360842.86766971\n",
      "  505337.79255898  461777.65626766]\n",
      "(1, 5) [[ -5438.61823989 -34147.49960929  14375.46655268  12000.38441965\n",
      "   11820.58942039]]\n",
      "Training MSE Loss:  [  57123.32601839 1135838.35450765  359293.40271308 ...  358213.02196662\n",
      "  499448.86352168  456489.71404218]\n",
      "(1, 5) [[ -5382.77618596 -33948.82079271  14190.99342932  11948.42733312\n",
      "   11776.11407334]]\n",
      "Training MSE Loss:  [  56222.83269688 1124091.92805     356713.12109084 ...  355589.63279626\n",
      "  493651.46987264  451273.6072394 ]\n",
      "(1, 5) [[ -5327.62425773 -33751.49795049  14008.8939513   11896.41196159\n",
      "   11731.5443132 ]]\n",
      "Training MSE Loss:  [  55338.60673031 1112495.22531942  354146.40644581 ...  352973.01969052\n",
      "  487943.97538277  446128.28518285]\n",
      "(1, 5) [[ -5273.14977311 -33555.5159743   13829.14114152  11844.34787252\n",
      "   11686.88918359]]\n",
      "Training MSE Loss:  [  54470.34065374 1101046.09989214  351593.26993305 ...  350363.4893313\n",
      "  482324.77264008  441052.70803366]\n",
      "(1, 5) [[ -5219.34068331 -33360.86038111  13651.70787641  11792.24399957\n",
      "   11642.1571125 ]]\n",
      "Training MSE Loss:  [  53617.73135955 1089742.43276319  349053.72531557 ...  347761.3362338\n",
      "  476792.28292032  436045.84742346]\n",
      "(1, 5) [[ -5166.18551195 -33167.5172534   13476.56694452  11740.10870459\n",
      "   11597.35597381]]\n",
      "Training MSE Loss:  [  52780.48026635 1078582.13280939  346527.78830917 ...  345166.84336678\n",
      "  471344.95600254  431106.68697762]\n",
      "(1, 5) [[ -5113.67330079 -32975.47318592  13303.69109812  11687.94983285\n",
      "   11552.49314213]]\n",
      "Training MSE Loss:  [  51958.29345223 1067563.13713187  344015.4760109  ...  342580.28271636\n",
      "  465981.2699379   426234.22274267]\n",
      "(1, 5) [[ -5061.79356145 -32784.71523831  13133.05309839  11635.77476227\n",
      "   11507.57554147]]\n",
      "Training MSE Loss:  [  51150.88175693 1056683.41129368  341516.80640114 ...  340001.91579997\n",
      "  460699.73077869  421427.46352976]\n",
      "(1, 5) [[ -5010.53623236 -32595.23089295  12964.62575503  11583.59044719\n",
      "   11462.60968876]]\n",
      "Training MSE Loss:  [  50357.96085701 1045940.94946575  339031.79791092 ...  337431.99413574\n",
      "  455498.8722738   416685.43118459]\n",
      "(1, 5) [[ -4959.89164042 -32407.0080173   12798.38196089  11531.40345744\n",
      "   11417.60173241]]\n",
      "Training MSE Loss:  [  49579.2513176  1035333.77449337  336560.46904683 ...  334870.7596724\n",
      "  450377.25553608  412007.16079327]\n",
      "(1, 5) [[ -4909.85046686 -32220.03483047  12634.2947222   11479.22001318\n",
      "   11372.55748682]]\n",
      "Training MSE Loss:  [  48814.47862383 1024859.93789364  334102.83806666 ...  332318.44518388\n",
      "  445333.46868637  407391.70083209]\n",
      "(1, 5) [[ -4860.40371672 -32034.29987333  12472.33718492  11427.04601594\n",
      "   11327.48246304]]\n",
      "Training MSE Loss:  [  48063.37319475 1014517.5197931   331658.92269998 ...  329775.27463252\n",
      "  440366.12647834  402838.11326856]\n",
      "(1, 5) [[ -4811.5426917  -31849.79198191  12312.48265752  11374.88707631\n",
      "   11282.38189614]]\n",
      "Training MSE Loss:  [  47325.67038211 1004304.62881374  329228.73990827 ...  327241.46350437\n",
      "  435473.86990779  398345.47361994]\n",
      "(1, 5) [[ -4763.25896589 -31666.50026363  12154.70463082  11322.74853869\n",
      "   11237.26076962]]\n",
      "Training MSE Loss:  [ 46601.11045619 994219.4019146  326812.30567995 ... 324717.21911943\n",
      " 430655.3658096  393912.87097497]\n",
      "(1, 5) [[ -4715.54436407 -31484.41407612  11998.97679505  11270.63550342\n",
      "   11192.12383714]]\n",
      "Training MSE Loss:  [ 45889.4385804  984260.00419516 324409.63485617 ... 322202.74091967\n",
      " 425909.30644512 389539.40798357]\n",
      "(1, 5) [[ -4668.39094243 -31303.52300831  11845.27305445  11218.55284643\n",
      "   11146.97564201]]\n",
      "Training MSE Loss:  [ 45190.40477648 974424.62866626 322020.74098382 ... 319698.22073711\n",
      " 421234.40908232 385224.20081897]\n",
      "(1, 5) [[ -4621.79097119 -31123.81686346  11693.56753983  11166.50523698\n",
      "   11101.82053447]]\n",
      "Training MSE Loss:  [ 44503.76388154 964711.49599317 319645.63619224 ... 317203.84304407\n",
      " 416629.41557097 380966.37911595]\n",
      "(1, 5) [[ -4575.73691918 -30945.2856441   11543.83461919  11114.49715342\n",
      "   11056.66268725]]\n",
      "Training MSE Loss:  [ 43829.2754983  955118.85421535 317284.33109114 ... 314719.78518744\n",
      " 412093.09191449 376765.08588874]\n",
      "(1, 5) [[ -4530.22143994 -30767.91953848  11396.04890667  11062.53289735\n",
      "   11011.5061094 ]]\n",
      "Training MSE Loss:  [ 43166.70393957 945644.97844657 314936.83468685 ... 312246.21760874\n",
      " 407624.22784018 372619.47743123]\n",
      "(1, 5) [[ -4485.23735932 -30591.70890845  11250.18527003  11010.61660628\n",
      "   10966.35465869]]\n",
      "Training MSE Loss:  [ 42515.81816804 936288.17055856 312603.1543149  ... 309783.30405117\n",
      " 403221.63636912 368528.72320236]\n",
      "(1, 5) [[ -4440.77766436 -30416.64427864  11106.21883684  10958.75226502\n",
      "   10921.21205275]]\n",
      "Training MSE Loss:  [ 41876.39173194 927046.75885135 310283.29558684 ... 307331.20175525\n",
      " 398884.15338695 364492.00569878]\n",
      "(1, 5) [[ -4396.83549328 -30242.71632667  10964.12499946  10906.9437159\n",
      "   10876.08187896]]\n",
      "Training MSE Loss:  [ 41248.20269773 917919.09771261 307977.26234949 ... 304890.06164397\n",
      " 394610.63721655 360508.52031686]\n",
      "(1, 5) [[ -4353.4041265  -30069.91587449  10823.87941911  10855.19466794\n",
      "   10830.96760344]]\n",
      "Training MSE Loss:  [ 40631.03357999 908903.56726833 305685.05665526 ... 302460.02849854\n",
      " 390399.96819343 356577.4752058 ]\n",
      "(1, 5) [[ -4310.47697856 -29898.23388044  10685.4580289   10803.5087051\n",
      "   10785.872579  ]]\n",
      "Training MSE Loss:  [ 40024.67126943 899998.57302673 303406.67874198 ... 300041.24112576\n",
      " 386251.04824466 352698.09111328]\n",
      "(1, 5) [[ -4268.04759087 -29727.66143227  10548.83703621  10751.88929374\n",
      "   10740.80005238]]\n",
      "Training MSE Loss:  [ 39428.90695928 891202.54551713 301142.12702104 ... 297633.83251754\n",
      " 382162.80047185 348869.60122515]\n",
      "(1, 5) [[ -4226.10962511 -29558.18974071  10413.99292424  10700.33978934\n",
      "   10695.75317071]]\n",
      "Training MSE Loss:  [ 38843.53607061 882513.93992527 298891.39807296 ... 295237.9300036\n",
      " 378134.16873884 345091.25100019]\n",
      "(1, 5) [[ -4184.65685734 -29389.81013377  10280.90245308  10648.86344256\n",
      "   10650.73498734]]\n",
      "Training MSE Loss:  [ 38268.35817686 873931.23572629 296654.4866492  ... 292853.65539767\n",
      " 374164.1172644  341362.29800108]\n",
      "(1, 5) [[ -4143.68317262 -29222.51405152  10149.54266012  10597.46340474\n",
      "   10605.74846712]]\n",
      "Training MSE Loss:  [ 37703.1769279  865452.93631659 294431.38567949 ... 290481.12513804\n",
      " 370251.63022044 337682.01172237]\n",
      "(1, 5) [[ -4103.18256013 -29056.29304143  10019.89086005  10546.14273284\n",
      "   10560.79649113]]\n",
      "Training MSE Loss:  [ 37147.799974   857077.56864548 292222.08628395 ... 288120.45042271\n",
      " 366395.71133596 334049.67341639]\n",
      "(1, 5) [[ -4063.14910884 -28891.13875409   9891.92464446  10494.90439403\n",
      "   10515.881861  ]]\n",
      "Training MSE Loss:  [ 36602.03888974 848803.68284751 290026.57778936 ... 285771.73733971\n",
      " 362595.38350698 330464.57591759]\n",
      "(1, 5) [[ -4023.57700342 -28727.04293936   9765.62188101  10443.75126973\n",
      "   10471.0073028 ]]\n",
      "Training MSE Loss:  [ 36065.7090983  840629.85187612 287844.84774885 ... 283435.08699303\n",
      " 358849.68841264 326926.02346606]\n",
      "(1, 5) [[ -3984.46052062 -28563.99744286   9640.96071239  10392.6861594\n",
      "   10426.17547056]]\n",
      "Training MSE Loss:  [ 35538.62979609 832554.67113933 285676.88196476 ... 281110.59562428\n",
      " 355157.68613773 323433.33153069]\n",
      "(1, 5) [[ -3945.79402594 -28401.99420274   9517.91955486  10341.71178394\n",
      "   10381.3889495 ]]\n",
      "Training MSE Loss:  [ 35020.623878   824576.75813799 283522.66451399 ... 278798.35473072\n",
      " 351518.45480155 319985.82663237]\n",
      "(1, 5) [[ -3907.5719706  -28241.0252468    9396.4770967   10290.83078886\n",
      "   10336.65025899]]\n",
      "Training MSE Loss:  [ 34511.51786335 816694.75210699 281382.17777552 ... 276498.45117962\n",
      " 347931.09019342 316582.84616773]\n",
      "(1, 5) [[ -3869.78888868 -28081.08268977   9276.61229635  10240.0457471\n",
      "   10291.96185514]]\n",
      "Training MSE Loss:  [ 34011.14182263 808907.31365984 279255.40245987 ... 274210.96731937\n",
      " 344394.70541464 313223.73823364]\n",
      "(1, 5) [[ -3832.43939462 -27922.15873085   9158.3043804   10189.35916171\n",
      "   10247.32613336]]\n",
      "Training MSE Loss:  [ 33519.32930519 801213.12443706 277142.31763994 ... 271935.98108759\n",
      " 340908.43052722 309907.86145279]\n",
      "(1, 5) [[ -3795.5181808  -27764.2456515    9041.53284145  10138.77346828\n",
      "   10202.74543052]]\n",
      "Training MSE Loss:  [ 33035.91726778 793610.88675844 275042.90078321 ... 269673.56611625\n",
      " 337471.41220907 306634.58480067]\n",
      "(1, 5) [[ -3759.02001536 -27607.33581329   8926.27743581  10088.29103719\n",
      "   10158.22202702]]\n",
      "Training MSE Loss:  [ 32560.74600425 786099.32327961 272957.12778495 ... 267423.79183419\n",
      " 334082.81341589 303403.28743401]\n",
      "(1, 5) [[ -3722.9397402  -27451.42165601   8812.51818108  10037.9141757\n",
      "   10113.75814877]]\n",
      "Training MSE Loss:  [ 32093.65907627 778677.17665302 270884.97300215 ... 265186.72356703\n",
      " 330741.81304955 300213.35852102]\n",
      "(1, 5) [[ -3687.27226905 -27296.49569585   8700.2353537    9987.64512987\n",
      "   10069.35596883]]\n",
      "Training MSE Loss:  [ 31634.50324512 771343.2091934  268826.40928818 ... 262962.42263469\n",
      " 327447.60563304 297064.19707346]\n",
      "(1, 5) [[ -3652.01258576 -27142.55052377   8589.4094863    9937.48608639\n",
      "   10025.01760913]]\n",
      "Training MSE Loss:  [ 31183.12840472 764096.20254803 266781.40802786 ... 260750.94644667\n",
      " 324199.40099191 293955.21178067]\n",
      "(1, 5) [[ -3617.15574261 -26989.57880392   8480.02136514   9887.43917427\n",
      "    9980.74514195]]\n",
      "Training MSE Loss:  [ 30739.38751574 756934.95737169 264749.93917284 ... 258552.3485951\n",
      " 320996.42394202 290885.8208458 ]\n",
      "(1, 5) [[ -3582.69685883 -26837.57327222   8372.05202735   9837.50646639\n",
      "    9936.54059132]]\n",
      "Training MSE Loss:  [ 30303.13654095 749858.29300651 262731.97127717 ... 256366.67894574\n",
      " 317837.9139837  287855.45182408]\n",
      "(1, 5) [[ -3548.63111912 -26686.52673495   8265.48275823   9787.68998099\n",
      "    9892.40593433]]\n",
      "Training MSE Loss:  [ 29874.23438174 742865.04716675 260727.47153302 ... 254193.98372703\n",
      " 314723.12500214 284863.54146347]\n",
      "(1, 5) [[ -3514.95377228 -26536.43206754   8160.29508847   9737.99168309\n",
      "    9848.34310239]]\n",
      "Training MSE Loss:  [ 29452.54281587 735954.07562841 258736.40580633 ... 252034.30561721\n",
      " 311651.32497388 281909.53554755]\n",
      "(1, 5) [[ -3481.66012999 -26387.28221329   8056.47079133   9688.4134858\n",
      "    9804.35398235]]\n",
      "Training MSE Loss:  [ 29037.92643634 729124.25192392 256758.73867236 ... 249887.68382966\n",
      " 308621.79567948 278992.88874072]\n",
      "(1, 5) [[ -3448.74556551 -26239.07018227   7953.99187988   9638.95725155\n",
      "    9760.4404176 ]]\n",
      "Training MSE Loss:  [ 28630.25259161 722374.46704172 254794.43345116 ... 247754.15419642\n",
      " 305633.83242205 276113.06443581]\n",
      "(1, 5) [[ -3416.20551261 -26091.78905025   7852.84060411   9589.62479328\n",
      "    9716.60420911]]\n",
      "Training MSE Loss:  [ 28229.39132684 715703.62913073 252843.45224271 ... 245633.74925007\n",
      " 302686.74375172 273269.5346041 ]\n",
      "(1, 5) [[ -3384.03546439 -25945.43195763   7752.99944815   9540.41787558\n",
      "    9672.8471164 ]]\n",
      "Training MSE Loss:  [ 27835.21532647 709110.66320988 250905.75596187 ... 243526.49830396\n",
      " 299779.85119587 270461.7796477 ]\n",
      "(1, 5) [[ -3352.23097232 -25799.99210849   7654.45112743   9491.33821575\n",
      "    9629.17085842]]\n",
      "Training MSE Loss:  [ 27447.59985783 702594.51088237 248981.30437296 ... 241432.42753087\n",
      " 296912.48899502 267689.28825434]\n",
      "(1, 5) [[ -3320.78764516 -25655.46276967   7557.17858581   9442.38748484\n",
      "    9585.57711448]]\n",
      "Training MSE Loss:  [ 27066.4227161  696154.13005497 247070.05612399 ... 239351.56004007\n",
      " 294084.00384435 264951.55725459]\n",
      "(1, 5) [[ -3289.70114804 -25511.8372698    7461.16499282   9393.56730861\n",
      "    9542.06752506]]\n",
      "Training MSE Loss:  [ 26691.56417023 689788.49466195 245171.96878048 ... 237283.91595294\n",
      " 291293.75464055 262248.09148142]\n",
      "(1, 5) [[ -3258.96720155 -25369.10899852   7366.39374082   9344.8792685\n",
      "    9498.64369261]]\n",
      "Training MSE Loss:  [ 26322.90691012 683496.59439396 243286.99885889 ... 235229.51247714\n",
      " 288541.11223422 259578.40363224]\n",
      "(1, 5) [[ -3228.5815808  -25227.2714056    7272.84844221   9296.3249025\n",
      "    9455.30718231]]\n",
      "Training MSE Loss:  [ 25960.33599487 677277.43443147 241415.10185957 ... 233188.36397929\n",
      " 285825.45918735 256942.01413323]\n",
      "(1, 5) [[ -3198.54011462 -25086.31800019   7180.51292663   9247.90570607\n",
      "    9412.05952282]]\n",
      "Training MSE Loss:  [ 25603.73880218 671130.03518298 239556.23229922 ... 231160.48205635\n",
      " 283146.18953606 254338.45100601]\n",
      "(1, 5) [[ -3168.8386847  -24946.24234997   7089.37123824   9199.62313295\n",
      "    9368.90220697]]\n",
      "Training MSE Loss:  [ 25253.00497876 665053.43202781 237710.34374293 ... 229145.87560561\n",
      " 280502.7085584  251767.2497367 ]\n",
      "(1, 5) [[ -3139.4732248  -24807.03808047   6999.40763295   9151.47859599\n",
      "    9325.83669246]]\n",
      "Training MSE Loss:  [ 24908.0263919  659046.67506335 235877.38883563 ... 227144.55089329\n",
      " 277894.43254697 249227.95314725]\n",
      "(1, 5) [[ -3110.43971997 -24668.69887434   6910.60657569   9103.47346792\n",
      "    9282.86440248]]\n",
      "Training MSE Loss:  [ 24568.697082   653108.82885695 234057.31933315 ... 225156.51162198\n",
      " 275320.78858656 246720.111269  ]\n",
      "(1, 5) [[ -3081.73420578 -24531.21847061   6822.95273776   9055.60908216\n",
      "    9239.98672639]]\n",
      "Training MSE Loss:  [ 24234.91321622 647238.97220202 232250.08613261 ... 223181.7589967\n",
      " 272781.21433641 244243.2812186 ]\n",
      "(1, 5) [[ -3053.35276763 -24394.59066402   6736.43099411   9007.88673349\n",
      "    9197.20502029]]\n",
      "Training MSE Loss:  [ 23906.57304305 641436.19787866 230455.63930242 ... 221220.29178973\n",
      " 270275.15781714 241797.02707603]\n",
      "(1, 5) [[ -3025.29153998 -24258.80930438   6651.02642071   8960.30767886\n",
      "    9154.52060766]]\n",
      "Training MSE Loss:  [ 23583.57684795 635699.61241843 228673.92811165 ... 219272.1064043\n",
      " 267802.07720229 239380.91976485]\n",
      "(1, 5) [[ -2997.5467057  -24123.8682959    6566.72429194   8912.87313801\n",
      "    9111.93477992]]\n",
      "Training MSE Loss:  [ 23265.82690992 630028.33587343 226904.90105893 ... 217337.19693695\n",
      " 265361.44061421 236994.53693462]\n",
      "(1, 5) [[ -2970.11449537 -23989.76159653   6483.51007801   8865.58429423\n",
      "    9069.44879698]]\n",
      "Training MSE Loss:  [ 22953.22745904 624421.50158939 225148.50590082 ... 215415.55523884\n",
      " 262952.7259244  234637.46284545]\n",
      "(1, 5) [[ -2942.99118662 -23856.48321741   6401.36944233   8818.44229497\n",
      "    9027.0638878 ]]\n",
      "Training MSE Loss:  [ 22645.68463493 618878.25598301 223404.68967955 ... 213507.17097582\n",
      " 260575.42055805 232309.28825459]\n",
      "(1, 5) [[ -2916.17310347 -23724.02722216   6320.28823906   8771.44825251\n",
      "    8984.78125093]]\n",
      "Training MSE Loss:  [ 22343.10644613 613397.7583231  221673.39875037 ... 211612.0316874\n",
      " 258229.02130277 230009.61030509]\n",
      "(1, 5) [[ -2889.65661571 -23592.38772641   6240.25251054   8724.60324462\n",
      "    8942.60205505]]\n",
      "Training MSE Loss:  [ 22045.40273041 607979.18051582 219954.57880824 ... 209730.12284461\n",
      " 255913.03412141 227738.03241653]\n",
      "(1, 5) [[ -2863.43813827 -23461.55889712   6161.24848481   8677.90831515\n",
      "    8900.52743941]]\n",
      "Training MSE Loss:  [ 21752.48511588 602621.70689367 218248.17491405 ... 207861.42790674\n",
      " 253626.97396884 225494.16417768]\n",
      "(1, 5) [[ -2837.51413063 -23331.53495204   6083.26257321   8631.36447467\n",
      "    8858.55851442]]\n",
      "Training MSE Loss:  [ 21464.26698304 597324.53400836 216554.13152035 ... 206005.92837698\n",
      " 251370.36461268 223277.62124113]\n",
      "(1, 5) [[ -2811.88109618 -23202.31015916   6006.28136788   8584.97270103\n",
      "    8816.69636206]]\n",
      "Training MSE Loss:  [ 21180.66342764 592086.87042729 214872.39249644 ... 204163.6038571\n",
      " 249142.73845781 221088.02521985]\n",
      "(1, 5) [[ -2786.53558168 -23073.87883616   5930.29163945   8538.73394\n",
      "    8774.94203642]]\n",
      "Training MSE Loss:  [ 20901.59122433 586907.93653385 213202.90115309 ... 202334.43210094\n",
      " 246943.63637458 218925.00358564]\n",
      "(1, 5) [[ -2761.47417665 -22946.23534985   5855.28033459   8492.64910577\n",
      "    8733.2965641 ]]\n",
      "Training MSE Loss:  [ 20626.96879114 581786.96433117 211545.60026662 ... 200518.38906699\n",
      " 244772.60753082 216788.18956939]\n",
      "(1, 5) [[ -2736.69351283 -22819.37411566   5781.23457375   8446.71908158\n",
      "    8691.76094474]]\n",
      "Training MSE Loss:  [ 20356.71615479 576723.19724945 209900.43210257 ... 198715.44896986\n",
      " 242629.20922719 214677.22206323]\n",
      "(1, 5) [[ -2712.19026359 -22693.2895971    5708.14164879   8400.94472024\n",
      "    8650.33615141]]\n",
      "Training MSE Loss:  [ 20090.75491667 571715.88995679 208267.33843877 ... 196925.5843309\n",
      " 240513.00673625 212591.74552439]\n",
      "(1, 5) [[ -2687.96114343 -22567.97630526   5635.98902075   8355.32684466\n",
      "    8609.02313109]]\n",
      "Training MSE Loss:  [ 19829.00821965 566764.30817338 206646.26058802 ... 195148.76602765\n",
      " 238423.57314482 210531.40988088]\n",
      "(1, 5) [[ -2664.00290738 -22443.42879828   5564.76431756   8309.8662484\n",
      "    8567.82280505]]\n",
      "Training MSE Loss:  [ 19571.40071563 561867.728489   205037.13942022 ... 193384.96334245\n",
      " 236360.48919965 208495.87043883]\n",
      "(1, 5) [[ -2640.31235055 -22319.64168088   5494.45533185   8264.5636962\n",
      "    8526.73606937]]\n",
      "Training MSE Loss:  [ 19317.85853374 557025.43818385 203439.915384   ... 191634.14401011\n",
      " 234323.3431565  206484.78779159]\n",
      "(1, 5) [[ -2616.88630754 -22196.60960385   5425.05001873   8219.41992446\n",
      "    8485.76379524]]\n",
      "Training MSE Loss:  [ 19068.3092493  552236.73505259 201854.52852794 ... 189896.27426456\n",
      " 232311.73063225 204497.8277304 ]\n",
      "(1, 5) [[ -2593.72165195 -22074.32726357   5356.53649365   8174.43564179\n",
      "    8444.90682946]]\n",
      "Training MSE Loss:  [ 18822.68185344 547500.92723147 200280.91852124 ... 188171.31888462\n",
      " 230325.25446027 202534.66115679]\n",
      "(1, 5) [[ -2570.8152959  -21952.78940152   5288.90303024   8129.61152944\n",
      "    8404.1659948 ]]\n",
      "Training MSE Loss:  [ 18580.90672342 542817.33302863 198719.024674   ... 186459.24123891\n",
      " 228363.52454871 200594.96399647]\n",
      "(1, 5) [[ -2548.16418948 -21831.99080384   5222.13805822   8084.94824186\n",
      "    8363.54209041]]\n",
      "Training MSE Loss:  [ 18342.91559359 538185.28075737 197168.78595701 ... 184760.00332979\n",
      " 226426.15774193 198678.41711486]\n",
      "(1, 5) [[ -2525.76532032 -21711.92630082   5156.23016128   8040.44640712\n",
      "    8323.0358922 ]]\n",
      "Training MSE Loss:  [ 18108.64152694 533604.10857248 195630.14102108 ... 183073.56583645\n",
      " 224512.77768471 196784.70623415]\n",
      "(1, 5) [[ -2503.61571306 -21592.5907665    5091.16807509   7996.10662741\n",
      "    8282.64815323]]\n",
      "Training MSE Loss:  [ 17878.0188874  529073.16430939 194103.02821595 ... 181399.88815717\n",
      " 222623.01468937 194913.52185182]\n",
      "(1, 5) [[ -2481.7124289  -21473.97911814   5026.9406852    7951.92947951\n",
      "    8242.37960408]]\n",
      "Training MSE Loss:  [ 17650.98331262 524591.80532627 192587.38560875 ... 179738.92845069\n",
      " 220756.50560573 193064.55916071]\n",
      "(1, 5) [[ -2460.05256513 -21356.08631586   4963.53702509   7907.91551519\n",
      "    8202.23095325]]\n",
      "Training MSE Loss:  [ 17427.4716874  520159.39834891 191083.15100207 ... 178090.64367673\n",
      " 218912.89369373 191237.51797045]\n",
      "(1, 5) [[ -2438.63325464 -21238.90736212   4900.94627417   7864.06526175\n",
      "    8162.20288746]]\n",
      "Training MSE Loss:  [ 17207.42211778 515775.31931838 189590.26195155 ... 176454.98963573\n",
      " 217091.82849873 189432.10263041]\n",
      "(1, 5) [[ -2417.45166552 -21122.43730135   4839.15775582   7820.37922237\n",
      "    8122.29607209]]\n",
      "Training MSE Loss:  [ 16990.77390556 511438.95324135 188108.65578309 ... 174831.92100775\n",
      " 215292.96572946 187648.02195399]\n",
      "(1, 5) [[ -2396.50500053 -21006.67121947   4778.1609355    7776.8578766\n",
      "    8082.51115146]]\n",
      "Training MSE Loss:  [ 16777.46752347 507149.69404311 186638.26960965 ... 173221.39139058\n",
      " 213515.96713851 185884.98914427]\n",
      "(1, 5) [[ -2375.79049675 -20891.6042435    4717.94541881   7733.50168076\n",
      "    8042.84874924]]\n",
      "Training MSE Loss:  [ 16567.44459089 502906.9444232  185179.04034767 ... 171623.35333702\n",
      " 211760.50040524 184142.72172104]\n",
      "(1, 5) [[ -2355.30542506 -20777.23154113   4658.50094966   7690.31106838\n",
      "    8003.30946875]]\n",
      "Training MSE Loss:  [ 16360.64785006 498710.11571358 183730.90473304 ... 170037.75839148\n",
      " 210026.2390213  182420.94144914]\n",
      "(1, 5) [[ -2335.04708976 -20663.54832029   4599.81740838   7647.28645059\n",
      "    7963.8938933 ]]\n",
      "Training MSE Loss:  [ 16157.02114283 494558.62773927 182293.79933678 ... 168464.55712574\n",
      " 208312.86217833 180719.37426806]\n",
      "(1, 5) [[ -2315.01282814 -20550.54982876   4541.88480993   7604.42821654\n",
      "    7924.60258656]]\n",
      "Training MSE Loss:  [ 15956.50938793 490451.90868158 180867.66058021 ... 166903.69917397\n",
      " 206620.05465818 179037.75022285]\n",
      "(1, 5) [[ -2295.20001003 -20438.23135378   4484.69330206   7561.73673381\n",
      "    7885.43609285]]\n",
      "Training MSE Loss:  [ 15759.05855873 486389.39494356 179452.42474987 ... 165355.13326703\n",
      " 204947.50672529 177375.80339625]\n",
      "(1, 5) [[ -2275.60603741 -20326.58822162   4428.23316358   7519.21234877\n",
      "    7846.39493747]]\n",
      "Training MSE Loss:  [ 15564.61566146 482370.53101802 178048.02801202 ... 163818.80726608\n",
      " 203294.91402134 175733.27184202]\n",
      "(1, 5) [[ -2256.22834402 -20215.6157972    4372.49480258   7476.85538703\n",
      "    7807.47962702]]\n",
      "Training MSE Loss:  [ 15373.12871397 478394.76935772 176654.40642676 ... 162294.66819536\n",
      " 201661.97746214 174109.89751953]\n",
      "(1, 5) [[ -2237.0643949  -20105.30948369   4317.4687547    7434.66615373\n",
      "    7768.69064972]]\n",
      "Training MSE Loss:  [ 15184.54672488 474461.57024793 175271.49596185 ... 160782.66227444\n",
      " 200048.40313654 172505.42622946]\n",
      "(1, 5) [[ -2218.11168607 -19995.66472216   4263.14568143   7392.644934\n",
      "    7730.02847571]]\n",
      "Training MSE Loss:  [ 14998.81967324 470570.40168117 173899.23250614 ... 159282.73494961\n",
      " 198453.90220754 170919.60755066]\n",
      "(1, 5) [[ -2199.36774406 -19886.67699117   4209.51636843   7350.79199331\n",
      "    7691.49355736]]\n",
      "Training MSE Loss:  [ 14815.8984886  466720.73923424 172537.5518827  ... 157794.83092469\n",
      " 196878.19081541 169352.19477819]\n",
      "(1, 5) [[ -2180.83012557 -19778.34180638   4156.57172386   7309.1075778\n",
      "    7653.08632956]]\n",
      "Training MSE Loss:  [ 14635.73503155 462912.06594724 171186.38986157 ... 156318.89419117\n",
      " 195320.98998272 167802.9448624 ]\n",
      "(1, 5) [[ -2162.4964171  -19670.65472022   4104.30277675   7267.59191466\n",
      "    7614.80721002]]\n",
      "Training MSE Loss:  [ 14458.2820746  459143.87220489 169845.68217219 ... 154854.86805763\n",
      " 193782.02552151 166271.61834913]\n",
      "(1, 5) [[ -2144.36423451 -19563.6113215    4052.70067538   7226.2452125\n",
      "    7576.65659957]]\n",
      "Training MSE Loss:  [ 14283.4932836  455415.6556198  168515.36451559 ... 153402.69517861\n",
      " 192261.02794217 164757.97932098]\n",
      "(1, 5) [[ -2126.43122273 -19457.20723505   4001.75668569   7185.06766168\n",
      "    7538.63488242]]\n",
      "Training MSE Loss:  [ 14111.32319948 451726.92091778 167195.37257615 ... 151962.31758275\n",
      " 190757.73236433 163261.79533962]\n",
      "(1, 5) [[ -2108.69505533 -19351.43812137   3951.46218968   7144.05943464\n",
      "    7500.74242646]]\n",
      "Training MSE Loss:  [ 13941.72722037 448077.17982523 165885.64203309 ... 150533.67670034\n",
      " 189271.87842953 161782.8373891 ]\n",
      "(1, 5) [[ -2091.15343418 -19246.29967626   3901.8086839    7103.22068627\n",
      "    7462.97958356]]\n",
      "Training MSE Loss:  [ 13774.66158418 444465.95095841 164586.10857174 ... 149116.7133903\n",
      " 187803.21021566 160320.87982021]\n",
      "(1, 5) [[ -2073.8040891  -19141.78763048   3852.78777787   7062.55155421\n",
      "    7425.34668978]]\n",
      "Training MSE Loss:  [ 13610.08335149 440892.75971464 163296.70789435 ... 147711.36796653\n",
      " 186351.47615319 158875.7002958 ]\n",
      "(1, 5) [[ -2056.6447775  -19037.8977494    3804.39119263   7022.05215921\n",
      "    7387.8440657 ]]\n",
      "Training MSE Loss:  [ 13447.95038886 437357.13816545 162017.37573077 ... 146317.58022358\n",
      " 184916.42894311 157447.07973706]\n",
      "(1, 5) [[ -2039.67328404 -18934.62583269   3756.61075918   6981.72260541\n",
      "    7350.47201667]]\n",
      "Training MSE Loss:  [ 13288.22135246 433858.62495147 160748.04784875 ... 144935.28946192\n",
      " 183497.82547648 156034.80227077]\n",
      "(1, 5) [[ -2022.88742026 -18831.96771392   3709.43841705   6941.5629807\n",
      "    7313.23083306]]\n",
      "Training MSE Loss:  [ 13130.85567207 430396.76517928 159488.66006398 ... 143564.43451246\n",
      " 182095.42675571 154638.65517747]\n",
      "(1, 5) [[ -2006.28502429 -18729.91926028   3662.86621284   6901.573357\n",
      "    7276.12079051]]\n",
      "Training MSE Loss:  [ 12975.81353545 426971.11031988 158239.14824987 ... 142204.9537606\n",
      " 180708.99781739 153258.42884054]\n",
      "(1, 5) [[ -1989.86396044 -18628.47637222   3616.8862988    6861.75379057\n",
      "    7239.14215021]]\n",
      "Training MSE Loss:  [ 12823.05587298 423581.21810907 156999.44834703 ... 140856.78516973\n",
      " 179338.30765669 151893.91669622]\n",
      "(1, 5) [[ -1973.62211897 -18527.63498316   3571.49093138   6822.10432234\n",
      "    7202.29515915]]\n",
      "Training MSE Loss:  [ 12672.54434271 420226.65244942 155769.49637254 ... 139519.86630414\n",
      " 177983.12915332 150544.91518439]\n",
      "(1, 5) [[ -1957.55741568 -18427.39105911   3526.67246987   6782.62497817\n",
      "    7165.58005033]]\n",
      "Training MSE Loss:  [ 12524.24131566 416906.98331399 154549.22842889 ... 138194.13435144\n",
      " 176643.23899894 149211.22370039]\n",
      "(1, 5) [[ -1941.66779161 -18327.7405984    3482.42337502   6743.31576919\n",
      "    7128.99704303]]\n",
      "Training MSE Loss:  [ 12378.10986149 413621.78665169 153338.58071274 ... 136879.52614443\n",
      " 175318.41762613 147892.64454752]\n",
      "(1, 5) [[ -1925.95121275 -18228.67963137   3438.73620768   6704.17669202\n",
      "    7092.54634307]]\n",
      "Training MSE Loss:  [ 12234.11373446 410370.64429426 152137.48952336 ... 135575.97818244\n",
      " 174008.44913867 146588.98289048]\n",
      "(1, 5) [[ -1910.40566971 -18130.20421999   3395.60362744   6665.20772913\n",
      "    7056.22814301]]\n",
      "Training MSE Loss:  [ 12092.21735964 407153.14386484 150945.89127088 ... 134283.42665226\n",
      " 172713.12124335 145300.04670959]\n",
      "(1, 5) [[ -1895.02917738 -18032.31045765   3353.01839135   6626.40884906\n",
      "    7020.04262237]]\n",
      "Training MSE Loss:  [ 11952.38581953 403968.87868813 149763.72248427 ... 133001.80744841\n",
      " 171432.2251831  144025.64675575]\n",
      "(1, 5) [[ -1879.81977469 -17934.99446877   3310.97335258   6587.78000674\n",
      "    6983.98994794]]\n",
      "Training MSE Loss:  [ 11814.58484083 400817.44770203 148590.91981913 ... 131731.05619309\n",
      " 170165.55567142 142765.59650632]\n",
      "(1, 5) [[ -1864.77552425 -17838.25240855   3269.46145915   6549.32114373\n",
      "    6948.07027389]]\n",
      "Training MSE Loss:  [ 11678.78078159 397698.45537082 147427.42006517 ... 130471.10825553\n",
      " 168912.91082821 141519.71212165]\n",
      "(1, 5) [[ -1849.89451208 -17742.08046265   3228.47575267   6511.03218848\n",
      "    6912.28374209]]\n",
      "Training MSE Loss:  [ 11544.94061862 394611.51159984 146273.16015356 ... 129221.89877095\n",
      " 167674.09211686 140287.81240239]\n",
      "(1, 5) [[ -1835.1748473  -17646.47484693   3188.00936708   6472.91305665\n",
      "    6876.63048228]]\n",
      "Training MSE Loss:  [ 11413.0319351  391556.23165158 145128.07716399 ... 127983.362659\n",
      " 166448.90428259 139069.71874759]\n",
      "(1, 5) [[ -1820.61466186 -17551.43180708   3148.05552739   6434.96365129\n",
      "    6841.1106123 ]]\n",
      "Training MSE Loss:  [ 11283.02290854 388532.23606324 143992.10833152 ... 126755.43464175\n",
      " 165237.15529201 137865.25511349]\n",
      "(1, 5) [[ -1806.21211026 -17456.94761845   3108.60754852   6397.18386316\n",
      "    6805.7242383 ]]\n",
      "Training MSE Loss:  [ 11154.88229894 385539.15056559 142865.19105329 ... 125538.04926135\n",
      " 164038.65627392 136674.24797299]\n",
      "(1, 5) [[ -1791.96536923 -17363.01858564   3069.65883404   6359.57357094\n",
      "    6770.47145495]]\n",
      "Training MSE Loss:  [ 11028.57943724 382576.60600336 141747.26289493 ... 124331.14089703\n",
      " 162853.22146128 135496.52627592]\n",
      "(1, 5) [[ -1777.87263748 -17269.64104232   3031.20287502   6322.13264149\n",
      "    6735.35234564]]\n",
      "Training MSE Loss:  [ 10904.08421396 379644.23825682 140638.26159682 ... 123134.64378192\n",
      " 161680.66813435 134331.92140989]\n",
      "(1, 5) [[ -1763.93213542 -17176.81135089   2993.23324885   6284.86093011\n",
      "    6700.36698269]]\n",
      "Training MSE Loss:  [ 10781.36706816 376741.68816478 139538.12508016 ... 121948.49201926\n",
      " 160520.8165649  133180.26716189]\n",
      "(1, 5) [[ -1750.14210489 -17084.52590225   2955.74361809   6247.75828075\n",
      "    6665.51542755]]\n",
      "Training MSE Loss:  [ 10660.39897655 373868.60144888 138446.79145277 ... 120772.61959831\n",
      " 159373.48996165 132041.39968048]\n",
      "(1, 5) [[ -1736.50080889 -16992.78111548   2918.72772932   6210.82452625\n",
      "    6630.79773101]]\n",
      "Training MSE Loss:  [ 10541.15144289 371024.6286391  137364.19901484 ... 119606.96040978\n",
      " 158238.51441667 130915.15743873]\n",
      "(1, 5) [[ -1723.00653132 -16901.57343762   2882.17941202   6174.0594886\n",
      "    6596.21393337]]\n",
      "Training MSE Loss:  [ 10423.59648758 368209.42500057 136290.28626433 ... 118451.44826093\n",
      " 157115.71885287 129801.38119771]\n",
      "(1, 5) [[ -1709.65757669 -16810.89934336   2846.09257748   6137.46297913\n",
      "    6561.76406465]]\n",
      "Training MSE Loss:  [ 10307.70663747 365422.6504616  135224.99190233 ... 117306.01689024\n",
      " 156004.93497264 128699.91397061]\n",
      "(1, 5) [[ -1696.45226991 -16720.75533482   2810.46121767   6101.03479877\n",
      "    6527.44814476]]\n",
      "Training MSE Loss:  [ 10193.45491593 362663.96954291 134168.25483814 ... 116170.5999817\n",
      " 154905.99720732 127610.60098756]\n",
      "(1, 5) [[ -1683.38895601 -16631.13794124   2775.2794042    6064.77473823\n",
      "    6493.2661837 ]]\n",
      "Training MSE Loss:  [ 10080.81483307 359933.05128804 133120.01419425 ... 115045.13117874\n",
      " 153818.74266783 126533.28966096]\n",
      "(1, 5) [[ -1670.46599987 -16542.04371879   2740.54128725   6028.68257827\n",
      "    6459.21818177]]\n",
      "Training MSE Loss:  [  9969.76037618 357229.56919494 132080.20931108 ... 113929.54409777\n",
      " 152743.01109618 125467.82955141]\n",
      "(1, 5) [[ -1657.68178602 -16453.46925026   2706.24109448   5992.75808985\n",
      "    6425.30412968]]\n",
      "Training MSE Loss:  [  9860.26600041 354553.20114867 131048.77975162 ... 112823.77234141\n",
      " 151678.64481798 124414.0723343 ]\n",
      "(1, 5) [[ -1645.03471835 -16365.41114482   2672.37313005   5957.00103442\n",
      "    6391.5240088 ]]\n",
      "Training MSE Loss:  [  9752.30661958 351903.62935528 130025.66530586 ... 111727.74951129\n",
      " 150625.48869584 123371.87176687]\n",
      "(1, 5) [[ -1632.52321988 -16277.86603778   2638.93177355   5921.41116403\n",
      "    6357.87779128]]\n",
      "Training MSE Loss:  [  9645.85759725 349280.5402767  129010.80599507 ... 110641.40922056\n",
      " 149583.39008373 122341.08365592]\n",
      "(1, 5) [[ -1620.14573255 -16190.83059036   2605.91147905   5885.98822164\n",
      "    6324.36544029]]\n",
      "Training MSE Loss:  [  9540.89473791 346683.62456682 128004.14207597 ... 109564.68510606\n",
      " 148552.19878223 121321.56582598]\n",
      "(1, 5) [[ -1607.90071694 -16104.30148943   2573.30677402   5850.73194122\n",
      "    6290.9869101 ]]\n",
      "Training MSE Loss:  [  9437.39427841 344112.57700854 127005.61404464 ... 108497.51084012\n",
      " 147531.7669946  120313.17808809]\n",
      "(1, 5) [[ -1595.78665207 -16018.27544724   2541.11225843   5815.64204802\n",
      "    6257.74214632]]\n",
      "Training MSE Loss:  [  9335.33287956 341567.09645188 126015.1626404  ... 107439.82014206\n",
      " 146521.94928378 119315.78220908]\n",
      "(1, 5) [[ -1583.80203516 -15932.74920125   2509.32260374   5780.71825874\n",
      "    6224.63108603]]\n",
      "Training MSE Loss:  [  9234.68761786 339046.8857532  125032.72884947 ... 106391.54678937\n",
      " 145522.60253021 118329.24188136]\n",
      "(1, 5) [[ -1571.9453814  -15847.71951384   2477.93255197   5745.96028172\n",
      "    6191.65365798]]\n",
      "Training MSE Loss:  [  9135.43597743 336551.65171528 124058.25390853 ... 105352.62462857\n",
      " 144533.58589041 117353.42269327]\n",
      "(1, 5) [[ -1560.21522373 -15763.18317211   2446.93691471   5711.36781712\n",
      "    6158.80978268]]\n",
      "Training MSE Loss:  [  9037.55584216 334081.10502851 123091.67930807 ... 104322.98758576\n",
      " 143554.76075649 116388.19209985]\n",
      "(1, 5) [[ -1548.61011263 -15679.13698761   2416.33057224   5676.94055712\n",
      "    6126.09937264]]\n",
      "Training MSE Loss:  [  8941.0254879  331634.96021298 122132.94679572 ... 103302.56967686\n",
      " 142585.9907163  115433.41939419]\n",
      "(1, 5) [[ -1537.12861587 -15595.57779616   2386.10847261   5642.67818609\n",
      "    6093.52233245]]\n",
      "Training MSE Loss:  [  8845.82357497 329212.93556157 121181.99837931 ... 102291.3050176\n",
      " 141627.14151449 114488.97567917]\n",
      "(1, 5) [[ -1525.76931833 -15512.50245761   2356.2656307    5608.5803808\n",
      "    6061.078559  ]]\n",
      "Training MSE Loss:  [  8751.92914066 326814.75308393 120238.77632991 ... 101289.12783315\n",
      " 140678.08101422 113554.73383976]\n",
      "(1, 5) [[ -1514.53082178 -15429.9078556    2326.79712734   5574.64681054\n",
      "    6028.76794159]]\n",
      "Training MSE Loss:  [  8659.32159202 324440.1384514  119303.2231847  ... 100295.97246752\n",
      " 139738.67915967 112630.56851576]\n",
      "(1, 5) [[ -1503.41174466 -15347.79089735   2297.69810845   5540.87713734\n",
      "    5996.59036208]]\n",
      "Training MSE Loss:  [  8567.98069871 322088.82094284 118375.28174969 ...  99311.77339267\n",
      " 138808.80793925 111716.35607494]\n",
      "(1, 5) [[ -1492.41072186 -15266.14851346   2268.96378417   5507.27101614\n",
      "    5964.54569505]]\n",
      "Training MSE Loss:  [  8477.88658607 319760.53339132 117454.89510236 ...  98336.46521733\n",
      " 137888.34134953 110811.9745867 ]\n",
      "(1, 5) [[ -1481.52640455 -15184.97765768   2240.58942795   5473.82809492\n",
      "    5932.63380794]]\n",
      "Training MSE Loss:  [  8389.01972826 317455.01213173 116542.00659416 ...  97369.98269559\n",
      " 136977.15535988 109917.30379616]\n",
      "(1, 5) [[ -1470.75745996 -15104.27530667   2212.57037579   5440.54801491\n",
      "    5900.85456119]]\n",
      "Training MSE Loss:  [  8301.36094159 315171.99694917 115636.55985292 ...  96412.26073516\n",
      " 136075.12787774 109032.22509863]\n",
      "(1, 5) [[ -1460.10257119 -15024.03845985   2184.90202533   5407.43041073\n",
      "    5869.20780837]]\n",
      "Training MSE Loss:  [  8214.89137797 312911.23102825 114738.49878507 ...  95463.23440549\n",
      " 135182.13871461 108156.62151459]\n",
      "(1, 5) [[ -1449.56043699 -14944.26413915   2157.57983506   5374.47491054\n",
      "    5837.69339635]]\n",
      "Training MSE Loss:  [  8129.59251849 310672.46090318 113847.76757785 ...  94522.8389455\n",
      " 134298.06955265 107290.37766499]\n",
      "(1, 5) [[ -1439.12977159 -14864.9493888    2130.59932351   5341.68113623\n",
      "    5806.31116539]]\n",
      "Training MSE Loss:  [  8045.44616717 308455.43640867 112964.31070138 ...  93591.00977117\n",
      " 133422.80391199 106433.37974705]\n",
      "(1, 5) [[ -1428.80930449 -14786.09127517   2103.95606843   5309.04870355\n",
      "    5775.06094932]]\n",
      "Training MSE Loss:  [  7962.43444477 306259.91063164 112088.07291053 ...  92667.68248285\n",
      " 132556.22711855 105585.51551039]\n",
      "(1, 5) [[ -1418.5977803  -14707.68688651   2077.64570603   5276.57722226\n",
      "    5743.94257561]]\n",
      "Training MSE Loss:  [  7880.53978276 304085.6398637  111218.99924686 ...  91752.79287235\n",
      " 131698.2262726  104746.67423361]\n",
      "(1, 5) [[ -1408.49395853 -14629.7333328    2051.66393017   5244.26629629\n",
      "    5712.95586558]]\n",
      "Training MSE Loss:  [  7799.74491747 301932.38355441 110357.03504029 ...  90846.27692975\n",
      " 130848.69021782 103916.74670121]\n",
      "(1, 5) [[ -1398.49661339 -14552.22774552   2026.0064916    5212.11552389\n",
      "    5682.10063446]]\n",
      "Training MSE Loss:  [  7720.03288425 299799.90426526 109502.12591079 ...  89948.07085009\n",
      " 130007.50951101 103095.62518095]\n",
      "(1, 5) [[ -1388.60453365 -14475.16727747   2000.66919721   5180.12449777\n",
      "    5651.37669155]]\n",
      "Training MSE Loss:  [  7641.38701182 297687.96762444 108654.21776992 ...  89058.1110397\n",
      " 129174.57639236 102283.20340151]\n",
      "(1, 5) [[ -1378.81652244 -14398.54910258   1975.64790926   5148.29280524\n",
      "    5620.78384033]]\n",
      "Training MSE Loss:  [  7563.79091673 295596.34228233 107813.25682227 ...  88176.33412244\n",
      " 128349.78475626 101479.37653058]\n",
      "(1, 5) [[ -1369.13139706 -14322.37041571   1950.93854466   5116.62002836\n",
      "    5590.32187859]]\n",
      "Training MSE Loss:  [  7487.22849793 293524.79986766 106979.18956685 ...  87302.67694567\n",
      " 127533.03012271 100684.04115329]\n",
      "(1, 5) [[ -1359.54798881 -14246.62843248   1926.53707426   5085.10574407\n",
      "    5559.99059853]]\n",
      "Training MSE Loss:  [  7411.68393141 291473.11494441 106151.96279835 ...  86437.07658599\n",
      " 126724.20960921  99897.09525098]\n",
      "(1, 5) [[ -1350.06514284 -14171.32038905   1902.43952206   5053.74952431\n",
      "    5529.78978692]]\n",
      "Training MSE Loss:  [  7337.14166501 289441.06496943 105331.52360831 ...  85579.47035485\n",
      " 125923.22190327  99118.43818032]\n",
      "(1, 5) [[ -1340.68171796 -14096.44354197   1878.64196457   5022.5509362\n",
      "    5499.71922517]]\n",
      "Training MSE Loss:  [  7263.58641332 287428.43025063 104517.81938627 ...  84729.79580388\n",
      " 125129.96723532  98347.97065283]\n",
      "(1, 5) [[ -1331.39658648 -14021.99516798   1855.1405301    4991.50954213\n",
      "    5469.77868947]]\n",
      "Training MSE Loss:  [  7191.0031526  285434.99390601 103710.79782073 ...  83887.99073012\n",
      " 124344.34735225  97585.59471462]\n",
      "(1, 5) [[ -1322.20863401 -13947.97256383   1831.93139803   4960.62489988\n",
      "    5439.9679509 ]]\n",
      "Training MSE Loss:  [  7119.37711596 283460.54182314 102910.40690016 ...  83053.99318094\n",
      " 123566.26549137  96831.21372658]\n",
      "(1, 5) [[ -1313.11675936 -13874.37304609   1809.01079817   4929.8965628\n",
      "    5410.28677553]]\n",
      "Training MSE Loss:  [  7048.6937885  281504.86261949 102116.59491378 ...  82227.74145892\n",
      " 122795.62635488  96084.73234483]\n",
      "(1, 5) [[ -1304.11987431 -13801.19395102   1786.37501004   4899.32407988\n",
      "    5380.73492456]]\n",
      "Training MSE Loss:  [  6978.9389026  279567.74760319 101329.3104524  ...  81409.1741264\n",
      " 122032.33608484  95346.05650149]\n",
      "(1, 5) [[ -1295.21690352 -13728.43263432   1764.02036229   4868.90699592\n",
      "    5351.31215436]]\n",
      "Training MSE Loss:  [  6910.09843332 277648.99073462 100548.50240913 ...  80598.23000998\n",
      " 121276.30223857  94615.09338577]\n",
      "(1, 5) [[ -1286.4067843  -13656.08647103   1741.94323194   4838.64485159\n",
      "    5322.01821668]]\n",
      "Training MSE Loss:  [  6842.15859385 275748.3885884   99774.11997996 ...  79794.84820476\n",
      " 120527.43376456  93891.75142539]\n",
      "(1, 5) [[ -1277.6884665  -13584.15285532   1720.14004383   4808.53718362\n",
      "    5292.85285865]]\n",
      "Training MSE Loss:  [  6775.1058311  273865.74031616  99006.11266439 ...  78998.96807847\n",
      " 119785.64097876  93175.94026826]\n",
      "(1, 5) [[ -1269.06091235 -13512.62920033   1698.6072699    4778.58352486\n",
      "    5263.81582293]]\n",
      "Training MSE Loss:  [  6708.92682135 272000.8476098   98244.43026588 ...  78210.52927539\n",
      " 119050.8355414   92467.57076449]\n",
      "(1, 5) [[ -1260.52309629 -13441.51293798   1677.34142864   4748.78340443\n",
      "    5234.90684785]]\n",
      "Training MSE Loss:  [  6643.60846597 270153.51466533  97489.0228923  ...  77429.47172012\n",
      " 118322.93043417  91766.5549487 ]\n",
      "(1, 5) [[ -1252.07400484 -13370.80151885   1656.33908439   4719.1363478\n",
      "    5206.12566741]]\n",
      "Training MSE Loss:  [  6579.13788728 268323.54814736  96739.84095627 ...  76655.73562123\n",
      " 117601.83993786  91072.80602255]\n",
      "(1, 5) [[ -1243.71263646 -13300.49241198   1635.59684681   4689.64187694\n",
      "    5177.47201149]]\n",
      "Training MSE Loss:  [  6515.50242444 266510.75715402  95996.83517548 ...  75889.26147471\n",
      " 116887.4796104   90386.23833764]\n",
      "(1, 5) [[ -1235.43800135 -13230.58310472   1615.11137021   4660.29951041\n",
      "    5148.94560585]]\n",
      "Training MSE Loss:  [  6452.68962943 264714.95318256  95259.95657291 ...  75129.99006725\n",
      " 116179.76626531  89706.76737859]\n",
      "(1, 5) [[ -1227.24912138 -13161.07110256   1594.87935298   4631.10876344\n",
      "    5120.54617229]]\n",
      "Training MSE Loss:  [  6390.68726315 262935.95009543  94529.156477   ...  74377.86247948\n",
      " 115478.61795056  89034.30974653]\n",
      "(1, 5) [[ -1219.14502988 -13091.95392901   1574.89753702   4602.06914808\n",
      "    5092.27342873]]\n",
      "Training MSE Loss:  [  6329.48329157 261173.56408686  93804.38652178 ...  73632.82008893\n",
      " 114783.95392782  88368.78314268]\n",
      "(1, 5) [[ -1211.12477156 -13023.22912536   1555.16270713   4573.18017329\n",
      "    5064.12708928]]\n",
      "Training MSE Loss:  [  6269.06588192 259427.61365003  93085.59864693 ...  72894.80457293\n",
      " 114095.69465206  87710.10635234]\n",
      "(1, 5) [[ -1203.18740232 -12954.89425062   1535.67169045   4544.44134503\n",
      "    5036.10686433]]\n",
      "Training MSE Loss:  [  6209.42339902 257697.91954472  92372.74509775 ...  72163.75791138\n",
      " 113413.7617516   87058.19922908]\n",
      "(1, 5) [[ -1195.33198914 -12886.9468813    1516.4213559    4515.85216636\n",
      "    5008.21246068]]\n",
      "Training MSE Loss:  [  6150.54440166 255984.30476546  91665.77842514 ...  71439.62238933\n",
      " 112738.07800842  86412.98267913]\n",
      "(1, 5) [[ -1187.55760994 -12819.3846113    1497.40861361   4487.41213756\n",
      "    4980.44358158]]\n",
      "Training MSE Loss:  [  6092.41763902 254286.59451016  90964.65148546 ...  70722.34059947\n",
      " 112068.56733894  85774.37864615]\n",
      "(1, 5) [[ -1179.86335344 -12752.2050517    1478.63041441   4459.12075618\n",
      "    4952.79992685]]\n",
      "Training MSE Loss:  [  6035.03204722 252604.61614931  90269.31744041 ...  70011.85544451\n",
      " 111405.15477507  85142.31009609]\n",
      "(1, 5) [[ -1172.24831903 -12685.4058307    1460.08374922   4430.97751721\n",
      "    4925.28119295]]\n",
      "Training MSE Loss:  [  5978.37674588 250938.19919555  89579.72975681 ...  69308.11013936\n",
      " 110747.76644567  84516.70100243]\n",
      "(1, 5) [[ -1164.71161666 -12618.98459338   1441.76564857   4402.9819131\n",
      "    4897.88707306]]\n",
      "Training MSE Loss:  [  5922.44103476 249287.17527376  88895.84220632 ...  68611.04821328\n",
      " 110096.32955831  83897.47633156]\n",
      "(1, 5) [[ -1157.25236666 -12552.93900162   1423.67318208   4375.13343388\n",
      "    4870.61725718]]\n",
      "Training MSE Loss:  [  5867.21439053 247651.37809166  88217.60886516 ...  67920.61351185\n",
      " 109450.77238136  83284.56202842]\n",
      "(1, 5) [[ -1149.86969969 -12487.26673392   1405.80345788   4347.43156727\n",
      "    4843.47143218]]\n",
      "Training MSE Loss:  [  5812.6864635  246030.64341076  87544.98411374 ...  67236.75019883\n",
      " 108811.02422646  82677.88500238]\n",
      "(1, 5) [[ -1142.56275653 -12421.96548529   1388.15362214   4319.87579874\n",
      "    4816.44928191]]\n",
      "Training MSE Loss:  [  5758.84707451 244424.80901788  86877.9226363  ...  66559.40275795\n",
      " 108177.01543122  82077.37311334]\n",
      "(1, 5) [[ -1135.33068803 -12357.03296705   1370.72085858   4292.46561159\n",
      "    4789.55048726]]\n",
      "Training MSE Loss:  [  5705.68621179 242833.714697    86216.37942045 ...  65888.51599453\n",
      " 107548.67734232  81482.95515804]\n",
      "(1, 5) [[ -1128.17265492 -12292.46690676   1353.50238793   4265.20048708\n",
      "    4762.77472627]]\n",
      "Training MSE Loss:  [  5653.19402798 241257.2022016   85560.30975665 ...  65224.03503705\n",
      " 106925.94229887  80894.56085661]\n",
      "(1, 5) [[ -1121.08782775 -12228.26504803   1336.49546744   4238.07990448\n",
      "    4736.12167415]]\n",
      "Training MSE Loss:  [  5601.36083711 239695.11522744  84909.66923778 ...  64565.90533854\n",
      " 106308.7436161   80312.12083925]\n",
      "(1, 5) [[ -1114.07538673 -12164.42515041   1319.69739043   4211.10334113\n",
      "    4709.5910034 ]]\n",
      "Training MSE Loss:  [  5550.17711173 238147.29938572  84264.4137585  ...  63914.07267798\n",
      " 105697.0155693   79735.56663329]\n",
      "(1, 5) [[ -1107.13452165 -12100.94498925   1303.10548577   4184.27027259\n",
      "    4683.18238386]]\n",
      "Training MSE Loss:  [  5499.63348003 236613.60217664  83624.49951467 ...  63268.48316148\n",
      " 105090.6933781   79164.83065023]\n",
      "(1, 5) [[ -1100.2644317  -12037.82235554   1286.71711744   4157.58017265\n",
      "    4656.89548281]]\n",
      "Training MSE Loss:  [  5449.72072302 235093.87296344  82989.88300274 ...  62629.08322342\n",
      " 104489.71319102  78599.84617321]\n",
      "(1, 5) [[ -1093.46432544 -11975.05505581   1270.52968403   4131.03251345\n",
      "    4630.729965  ]]\n",
      "Training MSE Loss:  [  5400.42977185 233587.96294677  82360.52101906 ...  61995.81962754\n",
      " 103894.01207028  78040.54734447]\n",
      "(1, 5) [[ -1086.7334206  -11912.640912     1254.54061831   4104.62676554\n",
      "    4604.68549275]]\n",
      "Training MSE Loss:  [  5351.75170502 232095.72513942  81736.37065916 ...  61368.63946783\n",
      " 103303.52797688  77486.86915318]\n",
      "(1, 5) [[ -1080.07094405 -11850.57776129   1238.74738675   4078.36239796\n",
      "    4578.76172601]]\n",
      "Training MSE Loss:  [  5303.67774583 230617.01434156  81117.38931702 ...  60747.49016945\n",
      " 102718.19975602  76938.74742335]\n",
      "(1, 5) [[ -1073.47613161 -11788.86345602   1223.14748907   4052.2388783\n",
      "    4552.95832246]]\n",
      "Training MSE Loss:  [  5256.19925973 229151.68711624  80503.5346843  ...  60132.31948943\n",
      " 102137.96712265  76396.11880195]\n",
      "(1, 5) [[ -1066.94822802 -11727.49586352   1207.73845785   4026.25567281\n",
      "    4527.27493749]]\n",
      "Training MSE Loss:  [  5209.30775182 227699.60176531  79894.76474953 ...  59523.07551742\n",
      " 101562.77064741  75858.92074726]\n",
      "(1, 5) [[ -1060.48648676 -11666.47286602   1192.517858     4000.41224644\n",
      "    4501.71122437]]\n",
      "Training MSE Loss:  [  5162.99486431 226260.61830567  79291.03779726 ...  58919.70667628\n",
      " 100992.55174277  75327.09151735]\n",
      "(1, 5) [[ -1054.09017    -11605.79236049   1177.48328643   3974.70806289\n",
      "    4476.26683424]]\n",
      "Training MSE Loss:  [  5117.25237415 224834.59844597  78692.31240721 ...  58322.16172258\n",
      " 100427.25264936  74800.57015876]\n",
      "(1, 5) [[ -1047.75854845 -11545.45225855   1162.63237153   3949.14258475\n",
      "    4450.94141622]]\n",
      "Training MSE Loss:  [  5072.07219056 223421.40556356  78098.54745338 ...  57730.38974704\n",
      "  99866.81642265  74279.29649537]\n",
      "(1, 5) [[ -1041.4909013  -11485.45048633   1147.96277285   3923.71527348\n",
      "    4425.73461743]]\n",
      "Training MSE Loss:  [  5027.44635274 222020.9046818   77509.70210309 ...  57144.34017496\n",
      "  99311.18691979  73763.21111741]\n",
      "(1, 5) [[ -1035.28651608 -11425.78498433   1133.47218058   3898.42558956\n",
      "    4400.6460831 ]]\n",
      "Training MSE Loss:  [  4983.36702751 220632.96244782  76925.73581606 ...  56563.96276641\n",
      "  98760.30878673  73252.25537067]\n",
      "(1, 5) [[ -1029.14468858 -11366.45370734   1119.15831523   3873.27299248\n",
      "    4375.67545657]]\n",
      "Training MSE Loss:  [  4939.82650711 219257.44711046  76346.60834344 ...  55989.20761656\n",
      "  98214.12744552  72746.37134585]\n",
      "(1, 5) [[ -1023.06472274 -11307.4546243    1105.01892719   3848.25694086\n",
      "    4350.82237943]]\n",
      "Training MSE Loss:  [  4896.81720694 217894.22849867  75772.27972678 ...  55420.02515573\n",
      "  97672.5890819   72245.50186811]\n",
      "(1, 5) [[ -1017.04593055 -11248.78571818   1091.05179631   3823.37689248\n",
      "    4326.08649149]]\n",
      "Training MSE Loss:  [  4854.33166341 216543.17800016  75202.71029701 ...  54856.36614954\n",
      "  97135.64063302  71749.59048673]\n",
      "(1, 5) [[ -1011.08763198 -11190.44498586   1077.25473155   3798.63230437\n",
      "    4301.46743091]]\n",
      "Training MSE Loss:  [  4812.36253179 215204.16854038  74637.86067341 ...  54298.18169889\n",
      "  96603.22977549  71258.58146501]\n",
      "(1, 5) [[ -1005.18915485 -11132.43043804   1063.62557059   3774.02263283\n",
      "    4276.96483421]]\n",
      "Training MSE Loss:  [  4770.90258413 213877.07456186  74077.69176251 ...  53745.42323992\n",
      "  96075.30491354  70772.41977022]\n",
      "(1, 5) [[  -999.34983474 -11074.74009908   1050.16217941   3749.54733352\n",
      "    4252.57833635]]\n",
      "Training MSE Loss:  [  4729.9447072  212561.77200378  73522.16475703 ...  53198.04254388\n",
      "  95551.81516746  70291.05106379]\n",
      "(1, 5) [[  -993.5690149  -11017.37200694   1036.86245195   3725.20586153\n",
      "    4228.30757078]]\n",
      "Training MSE Loss:  [  4689.4819005  211258.13828196  72971.24113472 ...  52655.99171697\n",
      "  95032.71036218  69814.42169163]\n",
      "(1, 5) [[  -987.84604619 -10960.32421303   1023.72430971   3700.99767139\n",
      "    4204.15216948]]\n",
      "Training MSE Loss:  [  4649.50727426 209966.05226902  72424.88265726 ...  52119.22320011\n",
      "  94517.94101611  69342.47867454]\n",
      "(1, 5) [[  -982.18028692 -10903.59478212   1010.74570143   3676.92221717\n",
      "    4180.11176304]]\n",
      "Training MSE Loss:  [  4610.01404749 208685.39427489  71883.05136909 ...  51587.6897686\n",
      "  94007.45833014  68875.16969889]\n",
      "(1, 5) [[  -976.57110284 -10847.18179221    997.92460267   3652.97895251\n",
      "    4156.18598069]]\n",
      "Training MSE Loss:  [  4570.99554615 207416.04602769  71345.70959625 ...  51061.34453183\n",
      "  93501.21417686  68412.44310729]\n",
      "(1, 5) [[  -971.01786697 -10791.08333444    985.2590155    3629.1673307\n",
      "    4132.37445034]]\n",
      "Training MSE Loss:  [  4532.4452012  206157.89065472  70812.81994516 ...  50540.14093284\n",
      "  92999.16108992  67954.24788953]\n",
      "(1, 5) [[  -965.51995958 -10735.29751299    972.74696811   3605.4868047\n",
      "    4108.67679869]]\n",
      "Training MSE Loss:  [  4494.35654682 204910.8126639   70284.34530144 ...  50024.03274787\n",
      "  92501.25225363  67500.5336736 ]\n",
      "(1, 5) [[  -960.07676807 -10679.82244493    960.3865145    3581.9368272\n",
      "    4085.09265119]]\n",
      "Training MSE Loss:  [  4456.72321862 203674.6979254   69760.24882866 ...  49512.97408582\n",
      "  92007.44149272  67051.25071684]\n",
      "(1, 5) [[  -954.68768689 -10624.65626017    948.17573411   3558.51685071\n",
      "    4061.62163217]]\n",
      "Training MSE Loss:  [  4419.53895184 202449.43365353  69240.49396713 ...  49006.91938776\n",
      "  91517.68326227  66606.34989724]\n",
      "(1, 5) [[  -949.35211743 -10569.79710133    936.11273149   3535.22632754\n",
      "    4038.26336484]]\n",
      "Training MSE Loss:  [  4382.79757967 201234.90838896  68725.04443261 ...  48505.82342626\n",
      "  91031.93263784  66165.7827049 ]\n",
      "(1, 5) [[  -944.06946801 -10515.24312361    924.19563598   3512.0647099\n",
      "    4015.01747135]]\n",
      "Training MSE Loss:  [  4346.49303154 200031.01198114  68213.86421505 ...  48009.64130474\n",
      "  90550.1453057   65729.50123355]\n",
      "(1, 5) [[  -938.8391537  -10460.99249475    912.42260135   3489.03144996\n",
      "    3991.88357285]]\n",
      "Training MSE Loss:  [  4310.61933143 198837.63557101  67706.91757731 ...  47518.32845679\n",
      "  90072.27755337  65297.45817226]\n",
      "(1, 5) [[  -933.66059631 -10407.04339487    900.7918055    3466.12599985\n",
      "    3968.86128951]]\n",
      "Training MSE Loss:  [  4275.17059628 197654.67157397  67204.16905388 ...  47031.84064542\n",
      "  89598.28626015  64869.60679726]\n",
      "(1, 5) [[  -928.5332243  -10353.3940164     889.30145014   3443.34781171\n",
      "    3945.95024057]]\n",
      "Training MSE Loss:  [  4240.14103436 196482.01366308  66705.58344951 ...  46550.13396227\n",
      "  89128.12888797  64445.90096386]\n",
      "(1, 5) [[  -923.45647265 -10300.04256397    877.94976046   3420.6963378\n",
      "    3923.15004442]]\n",
      "Training MSE Loss:  [  4205.52494372 195319.5567525   66211.12583795 ...  46073.16482678\n",
      "  88661.76347229  64026.29509853]\n",
      "(1, 5) [[  -918.42978284 -10246.98725433    866.73498484   3398.17103046\n",
      "    3900.46031859]]\n",
      "Training MSE Loss:  [  4171.31671062 194167.19698122  65720.76156058 ...  45600.88998529\n",
      "  88199.14861321  63610.74419104]\n",
      "(1, 5) [[  -913.45260277 -10194.22631622    855.6553945    3375.7713422\n",
      "    3877.88067982]]\n",
      "Training MSE Loss:  [  4137.51080806 193024.83169696  65234.45622509 ...  45133.2665102\n",
      "  87740.2434667   63199.20378681]\n",
      "(1, 5) [[  -908.52438662 -10141.7579903     844.70928325   3353.49672574\n",
      "    3855.41074412]]\n",
      "Training MSE Loss:  [  4104.10179425 191892.35944036  64752.17570406 ...  44670.25179895\n",
      "  87285.00773605  62791.62997929]\n",
      "(1, 5) [[  -903.64459485 -10089.58052905    833.89496716   3331.34663405\n",
      "    3833.05012675]]\n",
      "Training MSE Loss:  [  4071.08431119 190769.67992938  64273.8861337  ...  44211.80357309\n",
      "  86833.40166334  62387.97940247]\n",
      "(1, 5) [[  -898.81269409 -10037.69219666    823.21078426   3309.32052037\n",
      "    3810.79844234]]\n",
      "Training MSE Loss:  [  4038.45308322 189656.6940439   63799.55391236 ...  43757.87987724\n",
      "  86385.38602121  61988.20922356]\n",
      "(1, 5) [[ -894.02815705 -9986.09126897   812.65509426  3287.41783829\n",
      "   3788.65530487]]\n",
      "Training MSE Loss:  [  4006.20291563 188553.3038106   63329.14569925 ...  43308.43907802\n",
      "  85940.92210464  61592.27713571]\n",
      "(1, 5) [[ -889.2904625  -9934.77603333   802.22627827  3265.63804174\n",
      "   3766.62032772]]\n",
      "Training MSE Loss:  [  3974.32869325 187459.41238803  62862.62841294 ...  42863.439863\n",
      "  85499.97172294  61200.14135086]\n",
      "(1, 5) [[ -884.59909513 -9883.74478856   791.92273849  3243.98058507\n",
      "   3744.69312375]]\n",
      "Training MSE Loss:  [  3942.82537915 186374.92405186  62399.96923006 ...  42422.84123959\n",
      "  85062.49719188  60811.7605927 ]\n",
      "(1, 5) [[ -879.95354554 -9832.9958448    781.74289797  3222.44492307\n",
      "   3722.87330527]]\n",
      "Training MSE Loss:  [  3911.68801328 185299.74418042  61941.13558383 ...  41986.60253384\n",
      "  84628.46132589  60427.09408979]\n",
      "(1, 5) [[ -875.35331013 -9782.52752349   771.68520028  3201.030511\n",
      "   3701.16048412]]\n",
      "Training MSE Loss:  [  3880.91171117 184233.77924038  61486.09516267 ...  41554.68338931\n",
      "  84197.8274305   60046.10156864]\n",
      "(1, 5) [[ -870.79789105 -9732.33815721   761.74810929  3179.73680465\n",
      "   3679.5542717 ]]\n",
      "Training MSE Loss:  [  3850.49166267 183176.93677268  61034.81590875 ...  41127.04376586\n",
      "  83770.55929479  59668.74324706]\n",
      "(1, 5) [[ -866.28679612 -9682.42608965   751.93010888  3158.56326036\n",
      "   3658.05427902]]\n",
      "Training MSE Loss:  [  3820.42313067 182129.12537864  60587.26601659 ...  40703.64393838\n",
      "  83346.62118403  59294.9798275 ]\n",
      "(1, 5) [[ -861.81953878 -9632.7896755    742.22970265  3137.50933503\n",
      "   3636.66011668]]\n",
      "Training MSE Loss:  [  3790.70144989 181090.25470625  60143.41393164 ...  40284.4444956\n",
      "  82925.97783248  58924.77249049]\n",
      "(1, 5) [[ -857.39563799 -9583.42728033   732.64541372  3116.57448621\n",
      "   3615.37139498]]\n",
      "Training MSE Loss:  [  3761.32202564 180060.23543673  59703.22834879 ...  39869.40633874\n",
      "  82508.59443623  58558.08288827]\n",
      "(1, 5) [[ -853.01461822 -9534.33728058   723.17578439  3095.75817208\n",
      "   3594.1877239 ]]\n",
      "Training MSE Loss:  [  3732.2803327  179038.97927121  59266.67821098 ...  39458.49068024\n",
      "  82094.43664621  58194.87313839]\n",
      "(1, 5) [[ -848.67600931 -9485.51806339   713.81937596  3075.05985152\n",
      "   3573.10871314]]\n",
      "Training MSE Loss:  [  3703.57191408 178026.39891764  58833.73270769 ...  39051.65904242\n",
      "  81683.47056133  57835.1058175 ]\n",
      "(1, 5) [[ -844.37934648 -9436.96802658   704.57476843  3054.47898411\n",
      "   3552.13397217]]\n",
      "Training MSE Loss:  [  3675.19237992 177022.40807782  58404.36127358 ...  38648.8732561\n",
      "  81275.66272167  57478.74395519]\n",
      "(1, 5) [[ -840.12417021 -9388.68557854   695.44056028  3034.0150302\n",
      "   3531.26311026]]\n",
      "Training MSE Loss:  [  3647.13740634 176026.92143476  57978.53358693 ...  38250.09545927\n",
      "  80870.98010187  57125.75102792]\n",
      "(1, 5) [[ -835.91002621 -9340.66913814   686.41536821  3013.66745089\n",
      "   3510.4957365 ]]\n",
      "Training MSE Loss:  [  3619.40273437 175039.85464004  57556.21956826 ...  37855.28809567\n",
      "  80469.39010457  56776.09095302]\n",
      "(1, 5) [[ -831.73646535 -9292.91713468   677.4978269   2993.43570812\n",
      "   3489.83145984]]\n",
      "Training MSE Loss:  [  3591.98416885 174061.1243015   57137.38937883 ...  37464.41391335\n",
      "  80070.860554    56429.72808287]\n",
      "(1, 5) [[ -827.6030436  -9245.42800777   668.68658878  2973.31926463\n",
      "   3469.2698891 ]]\n",
      "Training MSE Loss:  [  3564.87757736 173090.647971    56722.01341919 ...  37077.43596328\n",
      "  79675.35968962  56086.62719903]\n",
      "(1, 5) [[ -823.50932195 -9198.20020727   659.98032379  2953.31758405\n",
      "   3448.81063303]]\n",
      "Training MSE Loss:  [  3538.0788892  172128.34413241  56310.06232771 ...  36694.31759786\n",
      "  79282.85615993  55746.75350657]\n",
      "(1, 5) [[ -819.45486638 -9151.23219322   651.37771914  2933.43013091\n",
      "   3428.45330032]]\n",
      "Training MSE Loss:  [  3511.58409434 171174.13218979  55901.50697909 ...  36315.02246947\n",
      "  78893.31901635  55410.07262841]\n",
      "(1, 5) [[ -815.43924779 -9104.52243574   642.87747909  2913.65637063\n",
      "   3408.19749964]]\n",
      "Training MSE Loss:  [  3485.38924243 170227.93245566  55496.31848293 ...  35939.51452895\n",
      "  78506.71770721  55076.55059979]\n",
      "(1, 5) [[ -811.46204192 -9058.06941496   634.47832474  2893.9957696\n",
      "   3388.04283964]]\n",
      "Training MSE Loss:  [  3459.49044181 169289.66613948  55094.46818224 ...  35567.75802416\n",
      "  78123.02207183  54746.15386278]\n",
      "(1, 5) [[ -807.52282935 -9011.87162095   626.17899377  2874.44779519\n",
      "   3367.98892904]]\n",
      "Training MSE Loss:  [  3433.88385854 168359.25533633  54695.92765194 ...  35199.71749837\n",
      "  77742.20233473  54418.84926091]\n",
      "(1, 5) [[ -803.62119537 -8965.92755363   617.97824024  2855.01191573\n",
      "   3348.03537658]]\n",
      "Training MSE Loss:  [  3408.56571546 167436.62301568  54300.66869745 ...  34835.35778881\n",
      "  77364.22909987  54094.60403385]\n",
      "(1, 5) [[ -799.75672997 -8920.23572271   609.8748344   2835.68760062\n",
      "   3328.18179108]]\n",
      "Training MSE Loss:  [  3383.53229121 166521.69301035  53908.66335314 ...  34474.64402506\n",
      "  76989.07334506  53773.38581216]\n",
      "(1, 5) [[ -795.92902777 -8874.79464761   601.86756241  2816.47432028\n",
      "   3308.4277815 ]]\n",
      "Training MSE Loss:  [  3358.77991938 165614.39000565  53519.88388093 ...  34117.54162755\n",
      "  76616.70641644  53455.16261216]\n",
      "(1, 5) [[ -792.137688   -8829.60285737   593.95522621  2797.37154621\n",
      "   3288.77295691]]\n",
      "Training MSE Loss:  [  3334.30498755 164714.63952861  53134.30276875 ...  33764.01630593\n",
      "  76247.100023    53139.90283082]\n",
      "(1, 5) [[ -788.38231436 -8784.65889061   586.13664325  2778.37875099\n",
      "   3269.21692655]]\n",
      "Training MSE Loss:  [  3310.10393645 163822.36793743  52751.89272912 ...  33414.03405749\n",
      "  75880.22623126  52827.57524079]\n",
      "(1, 5) [[ -784.66251508 -8739.96129541   578.41064629  2759.49540834\n",
      "   3249.75929983]]\n",
      "Training MSE Loss:  [  3286.17325903 162937.50241104  52372.62669765 ...  33067.56116563\n",
      "  75516.05746002  52518.14898538]\n",
      "(1, 5) [[ -780.97790276 -8695.50862928   570.77608324  2740.7209931\n",
      "   3230.39968638]]\n",
      "Training MSE Loss:  [  3262.5094997  162059.97093878  51996.47783158 ...  32724.56419817\n",
      "  75154.56647515  52211.59357381]\n",
      "(1, 5) [[ -777.3280944  -8651.29945907   563.23181692  2722.05498128\n",
      "   3211.13769606]]\n",
      "Training MSE Loss:  [  3239.10925339 161189.7023103   51623.4195083  ...  32385.01000577\n",
      "  74795.72638452  51907.87887628]\n",
      "(1, 5) [[ -773.7127113  -8607.3323609    555.77672488  2703.49685007\n",
      "   3191.97293899]]\n",
      "Training MSE Loss:  [  3215.96916479 160326.62610553  51253.42532389 ...  32048.86572031\n",
      "  74439.51063301  51606.97511937]\n",
      "(1, 5) [[ -770.13137904 -8563.60592009   548.40969919  2685.04607787\n",
      "   3172.90502556]]\n",
      "Training MSE Loss:  [  3193.08592751 159470.67268481  50886.46909163 ...  31716.09875327\n",
      "  74085.89299755  51308.85288127]\n",
      "(1, 5) [[ -766.58372739 -8520.11873109   541.12964626  2666.70214427\n",
      "   3153.93356645]]\n",
      "Training MSE Loss:  [  3170.45628333 158621.77317918  50522.52484059 ...  31386.67679403\n",
      "  73734.84758231  51013.48308728]\n",
      "(1, 5) [[ -763.06939031 -8476.86939742   533.93548666  2648.46453015\n",
      "   3135.05817268]]\n",
      "Training MSE Loss:  [  3148.07702135 157779.85948078  50161.5668141  ...  31060.56780827\n",
      "  73386.34881391  50720.8370052 ]\n",
      "(1, 5) [[ -759.58800587 -8433.85653158   526.82615492  2630.33271761\n",
      "   3116.27845561]]\n",
      "Training MSE Loss:  [  3125.94497729 156944.86423338  49803.56946832 ...  30737.74003632\n",
      "  73040.37143675  50430.88624092]\n",
      "(1, 5) [[ -756.1392162  -8391.078755     519.80059934  2612.30619004\n",
      "   3097.59402695]]\n",
      "Training MSE Loss:  [  3104.0570327  156116.72082307  49448.50747082 ...  30418.16199146\n",
      "  72696.8905084   50143.60273402]\n",
      "(1, 5) [[ -752.72266745 -8348.53469797   512.85778181  2594.38443215\n",
      "   3079.00449881]]\n",
      "Training MSE Loss:  [  3082.41011421 155295.36336904  49096.35569903 ...  30101.80245825\n",
      "  72355.88139502  49858.95875339]\n",
      "(1, 5) [[ -749.33800976 -8306.22299959   505.99667764  2576.56692993\n",
      "   3060.50948371]]\n",
      "Training MSE Loss:  [  3061.00119284 154480.72671451  48747.08923888 ...  29788.63049091\n",
      "  72017.31976698  49576.92689299]\n",
      "(1, 5) [[ -745.98489718 -8264.14230766   499.21627539  2558.85317072\n",
      "   3042.10859457]]\n",
      "Training MSE Loss:  [  3039.82728327 153672.7464178   48400.68338331 ...  29478.61541157\n",
      "  71681.18159439  49297.48006766]\n",
      "(1, 5) [[ -742.66298764 -8222.29127864   492.51557667  2541.24264321\n",
      "   3023.80144479]]\n",
      "Training MSE Loss:  [  3018.88544313 152871.35874348  48057.11363081 ...  29171.72680864\n",
      "  71347.44314283  49020.59150888]\n",
      "(1, 5) [[ -739.37194293 -8180.66857762   485.89359598  2523.73483745\n",
      "   3005.58764819]]\n",
      "Training MSE Loss:  [  2998.17277232 152076.50065367  47716.35568401 ...  28867.93453507\n",
      "  71016.08096906  48746.23476079]\n",
      "(1, 5) [[ -736.11142859 -8139.27287818   479.34936056  2506.32924487\n",
      "   2987.4668191 ]]\n",
      "Training MSE Loss:  [  2977.68641233 151288.10979947  47378.38544822 ...  28567.20870673\n",
      "  70687.07191689  48474.38367604]\n",
      "(1, 5) [[ -732.88111394 -8098.1028624    472.88191017  2489.02535831\n",
      "   2969.43857236]]\n",
      "Training MSE Loss:  [  2957.42354556 150506.12451247  47043.17902999 ...  28269.51970063\n",
      "  70360.39311301  48205.01241188]\n",
      "(1, 5) [[ -729.68067199 -8057.15722074   466.49029698  2471.82267199\n",
      "   2951.50252329]]\n",
      "Training MSE Loss:  [  2937.38139472 149730.48379642  46710.7127357  ...  27974.83815328\n",
      "  70036.02196299  47938.09542624]\n",
      "(1, 5) [[ -726.50977941 -8016.43465203   460.17358539  2454.72068158\n",
      "   2933.65828779]]\n",
      "Training MSE Loss:  [  2917.55722211 148961.12731898  46380.96307011 ...  27683.13495899\n",
      "  69713.93614727  47673.6074738 ]\n",
      "(1, 5) [[ -723.36811648 -7975.93386335   453.93085183  2437.7188842\n",
      "   2915.90548229]]\n",
      "Training MSE Loss:  [  2897.94832906 148197.99540359  46053.90673495 ...  27394.38126812\n",
      "  69394.11361725  47411.52360224]\n",
      "(1, 5) [[ -720.25536707 -7935.65357004   447.76118467  2420.81677839\n",
      "   2898.24372378]]\n",
      "Training MSE Loss:  [  2878.55205525 147441.02902146  45729.52062749 ...  27108.54848542\n",
      "  69076.53259141  47151.81914845]\n",
      "(1, 5) [[ -717.17121857 -7895.59249556   441.663684    2404.01386418\n",
      "   2880.67262985]]\n",
      "Training MSE Loss:  [  2859.36577817 146690.16978367  45407.78183914 ...  26825.60826832\n",
      "  68761.17155156  46894.46973482]\n",
      "(1, 5) [[ -714.11536186 -7855.74937148   435.63746151  2387.30964308\n",
      "   2863.19181869]]\n",
      "Training MSE Loss:  [  2840.38691244 145945.35993337  45088.66765404 ...  26545.53252521\n",
      "  68448.00923907  46639.4512656 ]\n",
      "(1, 5) [[ -711.08749129 -7816.12293743   429.68164033  2370.70361808\n",
      "   2845.80090912]]\n",
      "Training MSE Loss:  [  2821.61290929 145206.54233806  44772.15554761 ...  26268.29341372\n",
      "  68137.0246512   46386.73992326]\n",
      "(1, 5) [[ -708.08730458 -7776.711941     423.79535488  2354.19529369\n",
      "   2828.49952056]]\n",
      "Training MSE Loss:  [  2803.04125596 144473.66048204  44458.22318524 ...  25993.86333906\n",
      "  67828.1970375   46136.31216501]\n",
      "(1, 5) [[ -705.11450287 -7737.5151377    417.9777507   2337.78417592\n",
      "   2811.28727312]]\n",
      "Training MSE Loss:  [  2784.66947512 143746.65845891  44146.8484208  ...  25722.21495227\n",
      "  67521.50589623  45888.14471919]\n",
      "(1, 5) [[ -702.16879059 -7698.53129091   412.22798435  2321.46977233\n",
      "   2794.16378753]]\n",
      "Training MSE Loss:  [  2766.49512432 143025.48096418  43838.00929531 ...  25453.32114853\n",
      "  67216.93097086  45642.21458192]\n",
      "(1, 5) [[ -699.2498755  -7659.75917182   406.54522321  2305.251592\n",
      "   2777.12868524]]\n",
      "Training MSE Loss:  [  2748.51579545 142310.07328796  43531.68403552 ...  25187.15506547\n",
      "  66914.45224665  45398.49901363]\n",
      "(1, 5) [[ -696.35746859 -7621.19755937   400.92864537  2289.12914556\n",
      "   2760.18158836]]\n",
      "Training MSE Loss:  [  2730.72911418 141600.38130784  43227.85105257 ...  24923.69008144\n",
      "  66614.04994722  45156.97553569]\n",
      "(1, 5) [[ -693.49128407 -7582.84524017   395.3774395   2273.10194522\n",
      "   2743.32211973]]\n",
      "Training MSE Loss:  [  2713.13273946 140896.35148171  42926.48894058 ...  24662.89981381\n",
      "  66315.70453122  44917.62192717]\n",
      "(1, 5) [[ -690.65103933 -7544.70100849   389.89080468  2257.16950474\n",
      "   2726.54990289]]\n",
      "Training MSE Loss:  [  2695.72436299 140197.93084086  42627.57647528 ...  24404.75811729\n",
      "  66019.39668906  44680.41622149]\n",
      "(1, 5) [[ -687.83645491 -7506.76366618   384.46795027  2241.33133948\n",
      "   2709.86456211]]\n",
      "Training MSE Loss:  [  2678.50170868 139505.06698298  42331.09261268 ...  24149.23908222\n",
      "  65725.10733967  44445.33670325]\n",
      "(1, 5) [[ -685.04725445 -7469.03202259   379.10809579  2225.58696636\n",
      "   2693.26572243]]\n",
      "Training MSE Loss:  [  2661.46253218 138817.70806542  42037.01648768 ...  23896.31703288\n",
      "  65432.81762729  44212.36190502]\n",
      "(1, 5) [[ -682.28316467 -7431.50489459   373.81047078  2209.93590394\n",
      "   2676.75300962]]\n",
      "Training MSE Loss:  [  2644.60462038 138135.80279846  41745.32741275 ...  23645.96652579\n",
      "  65142.50891838  43981.47060425]\n",
      "(1, 5) [[ -679.5439153  -7394.18110641   368.57431465  2194.37767235\n",
      "   2660.32605024]]\n",
      "Training MSE Loss:  [  2627.92579092 137459.30043865  41456.00487655 ...  23398.16234799\n",
      "  64854.16279848  43752.64182013]\n",
      "(1, 5) [[ -676.82923911 -7357.05948968   363.39887658  2178.91179335\n",
      "   2643.98447163]]\n",
      "Training MSE Loss:  [  2611.42389171 136788.1507823   41169.02854261 ...  23152.87951543\n",
      "  64567.76106922  43525.85481058]\n",
      "(1, 5) [[ -674.1388718  -7320.13888333   358.28341536  2163.53779035\n",
      "   2627.72790191]]\n",
      "Training MSE Loss:  [  2595.09680047 136122.30415901  40884.378248   ...  22910.09327122\n",
      "  64283.28574533  43301.08906922]\n",
      "(1, 5) [[ -671.47255202 -7283.41813354   353.22719931  2148.25518835\n",
      "   2611.55597002]]\n",
      "Training MSE Loss:  [  2578.94242424 135461.71142531  40602.03400199 ...  22669.77908395\n",
      "  64000.71905163  43078.32432245]\n",
      "(1, 5) [[ -668.8300213  -7246.89609372   348.22950608  2133.06351403\n",
      "   2595.46830571]]\n",
      "Training MSE Loss:  [  2562.95869898 134806.32395839  40321.97598473 ...  22431.91264605\n",
      "  63720.04342024  42857.54052646]\n",
      "(1, 5) [[ -666.21102405 -7210.57162441   343.28962262  2117.96229568\n",
      "   2579.46453956]]\n",
      "Training MSE Loss:  [  2547.14358906 134156.09364985  40044.18454594 ...  22196.46987211\n",
      "  63441.24148762  42638.71786439]\n",
      "(1, 5) [[ -663.6153075  -7174.44359325   338.40684497  2102.95106328\n",
      "   2563.54430299]]\n",
      "Training MSE Loss:  [  2531.49508689 133510.97289964  39768.64020363 ...  21963.42689719\n",
      "  63164.29609185  42421.83674349]\n",
      "(1, 5) [[ -661.04262169 -7138.51087495   333.58047821  2088.02934844\n",
      "   2547.70722827]]\n",
      "Training MSE Loss:  [  2516.01121243 132870.91461     39495.32364273 ...  21732.76007515\n",
      "  62889.19026979  42206.87779231]\n",
      "(1, 5) [[ -658.49271941 -7102.7723512    328.80983628  2073.19668447\n",
      "   2531.95294852]]\n",
      "Training MSE Loss:  [  2500.6900128  132235.8721795   39224.21571385 ...  21504.44597704\n",
      "  62615.90725443  41993.8218579 ]\n",
      "(1, 5) [[ -655.96535619 -7067.22691066   324.09424193  2058.45260633\n",
      "   2516.28109774]]\n",
      "Training MSE Loss:  [  2485.52956184 131605.79949714  38955.29743199 ...  21278.46138939\n",
      "  62344.43047216  41782.65000315]\n",
      "(1, 5) [[ -653.46029028 -7031.87344888   319.43302653  2043.79665067\n",
      "   2500.69131081]]\n",
      "Training MSE Loss:  [  2470.52795974 130980.65093661  38688.54997521 ...  21054.7833126\n",
      "  62074.74354015  41573.34350404]\n",
      "(1, 5) [[ -650.97728257 -6996.71086827   314.82553003  2029.22835582\n",
      "   2485.18322349]]\n",
      "Training MSE Loss:  [  2455.68333258 130360.3813505   38423.95468341 ...  20833.38895928\n",
      "  61806.83026373  41365.88384699]\n",
      "(1, 5) [[ -648.51609661 -6961.73807803   310.27110079  2014.74726182\n",
      "   2469.75647244]]\n",
      "Training MSE Loss:  [  2440.993832   129744.94606468  38161.49305704 ...  20614.25575259\n",
      "  61540.67463388  41160.2527263 ]\n",
      "(1, 5) [[ -646.07649856 -6926.95399412   305.7690955   2000.35291039\n",
      "   2454.41069523]]\n",
      "Training MSE Loss:  [  2426.45763476 129134.30087272  37901.14675581 ...  20397.36132466\n",
      "  61276.26082467  40956.43204148]\n",
      "(1, 5) [[ -643.65825716 -6892.35753922   301.31887906  1986.04484496\n",
      "   2439.14553035]]\n",
      "Training MSE Loss:  [  2412.07294237 128528.40203038  37642.89759748 ...  20182.68351493\n",
      "  61013.57319079  40754.40389478]\n",
      "(1, 5) [[ -641.26114368 -6857.94764266   296.91982449  1971.82261065\n",
      "   2423.96061719]]\n",
      "Training MSE Loss:  [  2397.83798073 127927.20625022  37386.7275566  ...  19970.2003685\n",
      "  60752.59626511  40554.15058863]\n",
      "(1, 5) [[ -638.88493195 -6823.72324038   292.57131279  1957.68575431\n",
      "   2408.85559611]]\n",
      "Training MSE Loss:  [  2383.75099975 127330.67069616  37132.61876322 ...  19759.89013457\n",
      "  60493.31475629  40355.65462318]\n",
      "(1, 5) [[ -636.52939825 -6789.68327489   288.27273285  1943.6338245\n",
      "   2393.83010837]]\n",
      "Training MSE Loss:  [  2369.81027296 126738.75297826  36880.55350172 ...  19551.73126482\n",
      "  60235.71354636  40158.89869386]\n",
      "(1, 5) [[ -634.19432137 -6755.82669522   284.02348136  1929.66637151\n",
      "   2378.88379622]]\n",
      "Training MSE Loss:  [  2356.01409723 126151.41114748  36630.51420956 ...  19345.70241176\n",
      "  59979.77768848  39963.86568895]\n",
      "(1, 5) [[ -631.87948249 -6722.15245686   279.8229627   1915.78294733\n",
      "   2364.01630283]]\n",
      "Training MSE Loss:  [  2342.36079231 125568.60369051  36382.48347603 ...  19141.78242719\n",
      "  59725.49240454  39770.5386872 ]\n",
      "(1, 5) [[ -629.58466522 -6688.65952176   275.67058884  1901.98310569\n",
      "   2349.22727235]]\n",
      "Training MSE Loss:  [  2328.84870057 124990.28952468  36136.44404107 ...  18939.95036059\n",
      "  59472.84308297  39578.90095552]\n",
      "(1, 5) [[ -627.30965557 -6655.34685822   271.56577922  1888.26640206\n",
      "   2334.51634991]]\n",
      "Training MSE Loss:  [  2315.47618662 124416.427993    35892.37879407 ...  18740.18545753\n",
      "  59221.81527649  39388.9359466 ]\n",
      "(1, 5) [[ -625.05424186 -6622.21344089   267.50796069  1874.63239363\n",
      "   2319.8831816 ]]\n",
      "Training MSE Loss:  [  2302.241637   123846.97885911  35650.27077261 ...  18542.46715809\n",
      "  58972.39469992  39200.6272967 ]\n",
      "(1, 5) [[ -622.81821477 -6589.25825073   263.49656736  1861.08063932\n",
      "   2305.32741449]]\n",
      "Training MSE Loss:  [  2289.14345982 123281.90230246  35410.10316134 ...  18346.77509531\n",
      "  58724.567228    39013.95882332]\n",
      "(1, 5) [[ -620.60136726 -6556.48027493   259.53104057  1847.61069982\n",
      "   2290.84869667]]\n",
      "Training MSE Loss:  [  2276.18008447 122721.15891346  35171.85929075 ...  18153.08909362\n",
      "  58478.3188933   38828.91452305]\n",
      "(1, 5) [[ -618.40349459 -6523.87850689   255.61082876  1834.22213753\n",
      "   2276.4466772 ]]\n",
      "Training MSE Loss:  [  2263.34996126 122164.70968871  34935.52263601 ...  17961.38916728\n",
      "  58233.63588407  38645.4785693 ]\n",
      "(1, 5) [[ -616.22439423 -6491.45194617   251.73538734  1820.9145166\n",
      "   2262.12100616]]\n",
      "Training MSE Loss:  [  2250.65156115 121612.5160263   34701.07681578 ...  17771.65551884\n",
      "  57990.50454224  38463.63531021]\n",
      "(1, 5) [[ -614.06386589 -6459.19959848   247.90417869  1807.68740295\n",
      "   2247.87133464]]\n",
      "Training MSE Loss:  [  2238.08337542 121064.53972116  34468.50559105 ...  17583.8685376\n",
      "  57748.91136135  38283.36926646]\n",
      "(1, 5) [[ -611.9217115  -6427.12047556   244.11667198  1794.54036422\n",
      "   2233.69731472]]\n",
      "Training MSE Loss:  [  2225.64391538 120520.74296049  34237.79286401 ...  17398.00879805\n",
      "  57508.84298453  38104.66512918]\n",
      "(1, 5) [[ -609.79773512 -6395.21359523   240.37234313  1781.47296982\n",
      "   2219.59859955]]\n",
      "Training MSE Loss:  [  2213.33171204 119981.08831922  34008.92267686 ...  17214.0570584\n",
      "  57270.28620257  37927.50775788]\n",
      "(1, 5) [[ -607.69174299 -6363.47798127   236.6706747   1768.48479089\n",
      "   2205.57484326]]\n",
      "Training MSE Loss:  [  2201.14531588 119445.53875554  33781.87921068 ...  17031.994259\n",
      "  57033.22795196  37751.88217839]\n",
      "(1, 5) [[ -605.60354345 -6331.91266343   233.01115584  1755.57540036\n",
      "   2191.62570104]]\n",
      "Training MSE Loss:  [  2189.08329648 118914.05760652  33556.64678431 ...  16851.80152089\n",
      "  56797.65531296  37577.77358083]\n",
      "(1, 5) [[ -603.53294695 -6300.51667737   229.39328213  1742.74437287\n",
      "   2177.75082911]]\n",
      "Training MSE Loss:  [  2177.1442423  118386.6085837   33333.2098532  ...  16673.46014425\n",
      "  56563.55550772  37405.16731763]\n",
      "(1, 5) [[ -601.47976602 -6269.28906462   225.81655558  1729.99128485\n",
      "   2163.94988473]]\n",
      "Training MSE Loss:  [  2165.32676038 117863.15576886  33111.5530083  ...  16496.95160694\n",
      "  56330.91589845  37234.04890154]\n",
      "(1, 5) [[ -599.44381522 -6238.22887254   222.28048448  1717.31571447\n",
      "   2150.22252622]]\n",
      "Training MSE Loss:  [  2153.62947607 117343.66360972  32891.66097494 ...  16322.25756299\n",
      "  56099.72398556  37064.4040037 ]\n",
      "(1, 5) [[ -597.42491116 -6207.33515428   218.78458338  1704.71724166\n",
      "   2136.56841294]]\n",
      "Training MSE Loss:  [  2142.05103274 116828.09691578  32673.51861172 ...  16149.35984112\n",
      "  55869.96740585  36896.21845172]\n",
      "(1, 5) [[ -595.42287245 -6176.60696873   215.32837293  1692.19544812\n",
      "   2122.98720532]]\n",
      "Training MSE Loss:  [  2130.59009155 116316.42085417  32457.11090942 ...  15978.24044331\n",
      "  55641.63393079  36729.47822777]\n",
      "(1, 5) [[ -593.43751967 -6146.04338051   211.91137987  1679.7499173\n",
      "   2109.47856482]]\n",
      "Training MSE Loss:  [  2119.24533116 115808.60094558  32242.42298988 ...  15808.88154328\n",
      "  55414.71146469  36564.16946673]\n",
      "(1, 5) [[ -591.46867536 -6115.64345989   208.53313693  1667.3802344\n",
      "   2096.04215401]]\n",
      "Training MSE Loss:  [  2108.01544748 115304.60306021  32029.44010498 ...  15641.26548509\n",
      "  55189.18804305  36400.27845433]\n",
      "(1, 5) [[ -589.51616403 -6085.4062828    205.19318272  1655.08598639\n",
      "   2082.6776365 ]]\n",
      "Training MSE Loss:  [  2096.89915343 114804.39341382  31818.14763546 ...  15475.37478166\n",
      "  54965.05183079  36237.79162536]\n",
      "(1, 5) [[ -587.57981206 -6055.33093073   201.89106169  1642.866762\n",
      "   2069.38467699]]\n",
      "Training MSE Loss:  [  2085.89517867 114307.93856375  31608.53108994 ...  15311.19211335\n",
      "  54742.29112066  36076.69556185]\n",
      "(1, 5) [[ -585.65944776 -6025.41649077   198.62632406  1630.72215171\n",
      "   2056.16294124]]\n",
      "Training MSE Loss:  [  2075.00226936 113815.20540509  31400.5761038  ...  15148.7003265\n",
      "  54520.89433151  35916.97699128]\n",
      "(1, 5) [[ -583.75490128 -5995.66205548   195.39852569  1618.65174776\n",
      "   2043.0120961 ]]\n",
      "Training MSE Loss:  [  2064.21918796 113326.16116681  31194.26843816 ...  14987.88243207\n",
      "  54300.85000672  35758.62278487]\n",
      "(1, 5) [[ -581.86600465 -5966.06672293   192.20722809  1606.65514417\n",
      "   2029.93180952]]\n",
      "Training MSE Loss:  [  2053.54471292 112840.77340801  30989.59397879 ...  14828.72160418\n",
      "  54082.14681259  35601.61995586]\n",
      "(1, 5) [[ -579.99259172 -5936.62959663   189.05199825  1594.73193668\n",
      "   2016.92175051]]\n",
      "Training MSE Loss:  [  2042.97763851 112359.01001416  30786.53873511 ...  14671.20117871\n",
      "  53864.77353672  35445.95565774]\n",
      "(1, 5) [[ -578.13449815 -5907.3497855    185.93240866  1582.88172282\n",
      "   2003.98158918]]\n",
      "Training MSE Loss:  [  2032.51677455 111880.83919339  30585.08883912 ...  14515.30465192\n",
      "  53648.71908653  35291.61718265]\n",
      "(1, 5) [[ -576.29156138 -5878.22640381   182.84803717  1571.10410186\n",
      "   1991.11099676]]\n",
      "Training MSE Loss:  [  2022.16094622 111406.22947292  30385.23054438 ...  14361.01567906\n",
      "  53433.97248766  35138.59195968]\n",
      "(1, 5) [[ -574.46362063 -5849.25857119   179.79846695  1559.39867482\n",
      "   1978.30964554]]\n",
      "Training MSE Loss:  [  2011.9089938  110935.14969539  30186.950225   ...  14208.318073\n",
      "  53220.5228825   34986.86755322]\n",
      "(1, 5) [[ -572.65051687 -5820.44541255   176.78328643  1547.76504449\n",
      "   1965.57720894]]\n",
      "Training MSE Loss:  [  2001.75977247 110467.56901533  29990.23437463 ...  14057.19580284\n",
      "  53008.3595287   34836.43166138]\n",
      "(1, 5) [[ -570.8520928  -5791.78605807   173.8020892   1536.2028154\n",
      "   1952.91336146]]\n",
      "Training MSE Loss:  [  1991.71215213 110003.45689567  29795.06960541 ...  13907.63299255\n",
      "  52797.47179766  34687.27211437]\n",
      "(1, 5) [[ -569.06819283 -5763.27964314   170.85447398  1524.71159384\n",
      "   1940.31777871]]\n",
      "Training MSE Loss:  [  1981.7650171  109542.78310426  29601.44264702 ...  13759.61391965\n",
      "  52587.84917318  34539.37687294]\n",
      "(1, 5) [[ -567.29866304 -5734.92530837   167.94004452  1513.29098784\n",
      "   1927.79013743]]\n",
      "Training MSE Loss:  [  1971.917266   109085.51771044  29409.34034567 ...  13613.12301384\n",
      "  52379.48124992  34392.73402681]\n",
      "(1, 5) [[ -565.54335122 -5706.7221995    165.05840955  1501.94060719\n",
      "   1915.33011543]]\n",
      "Training MSE Loss:  [  1962.16781151 108631.63108169  29218.7496631  ...  13468.14485567\n",
      "  52172.35773209  34247.33179316]\n",
      "(1, 5) [[ -563.80210679 -5678.6694674    162.20918271  1490.66006342\n",
      "   1902.93739168]]\n",
      "Training MSE Loss:  [  1952.51558015 108181.09388027  29029.65767561 ...  13324.66417521\n",
      "  51966.46843203  34103.15851509]\n",
      "(1, 5) [[ -562.0747808  -5650.76626804   159.3919825   1479.4489698\n",
      "   1890.61164622]]\n",
      "Training MSE Loss:  [  1942.95951212 107733.87705994  28842.05157311 ...  13182.66585073\n",
      "  51761.80326884  33960.20266015]\n",
      "(1, 5) [[ -560.36122593 -5623.01176242   156.60643221  1468.30694136\n",
      "   1878.35256024]]\n",
      "Training MSE Loss:  [  1933.49856107 107289.95186269  28655.9186581  ...  13042.13490743\n",
      "  51558.35226705  33818.45281885]\n",
      "(1, 5) [[ -558.66129645 -5595.40511657   153.85215984  1457.23359485\n",
      "   1866.15981603]]\n",
      "Training MSE Loss:  [  1924.13169392 106849.28981553  28471.24634477 ...  12903.05651608\n",
      "  51356.1055553   33677.8977032 ]\n",
      "(1, 5) [[ -556.97484821 -5567.94550152   151.12879805  1446.22854877\n",
      "   1854.03309699]]\n",
      "Training MSE Loss:  [  1914.85789069 106411.86272733  28288.02215803 ...  12765.41599177\n",
      "  51155.05336501  33538.5261453 ]\n",
      "(1, 5) [[ -555.30173864 -5540.63209323   148.43598412  1435.29142335\n",
      "   1841.97208768]]\n",
      "Training MSE Loss:  [  1905.67614428 105977.64268569  28106.23373253 ...  12629.19879261\n",
      "  50955.18602914  33400.3270959 ]\n",
      "(1, 5) [[ -553.64182668 -5513.46407259   145.77335985  1424.42184057\n",
      "   1829.97647374]]\n",
      "Training MSE Loss:  [  1896.58546031 105546.60205379  27925.86881178 ...  12494.39051848\n",
      "  50756.49398087  33263.28962299]\n",
      "(1, 5) [[ -551.99497284 -5486.44062537   143.14057154  1413.61942412\n",
      "   1818.04594196]]\n",
      "Training MSE Loss:  [  1887.58485693 105118.71346743  27746.91524718 ...  12360.97690973\n",
      "  50558.96775239  33127.40291045]\n",
      "(1, 5) [[ -550.36103911 -5459.56094221   140.53726992  1402.88379944\n",
      "   1806.18018025]]\n",
      "Training MSE Loss:  [  1878.67336466 104693.9498319   27569.36099711 ...  12228.94384594\n",
      "  50362.59797365  32992.65625666]\n",
      "(1, 5) [[ -548.73988899 -5432.82421855   137.96311005  1392.21459368\n",
      "   1794.37887764]]\n",
      "Training MSE Loss:  [  1869.85002617 104272.28431908  27393.19412601 ...  12098.2773447\n",
      "  50167.37537114  32859.03907315]\n",
      "(1, 5) [[ -547.13138744 -5406.22965463   135.41775135  1381.61143571\n",
      "   1782.6417243 ]]\n",
      "Training MSE Loss:  [  1861.11389616 103853.69036445  27218.40280348 ...  11968.96356031\n",
      "  49973.29076673  32726.54088332]\n",
      "(1, 5) [[ -545.5354009  -5379.77645546   132.90085746  1371.07395615\n",
      "   1770.96841152]]\n",
      "Training MSE Loss:  [  1852.46404116 103438.14166421  27044.97530335 ...  11840.98878262\n",
      "  49780.33507643  32595.15132104]\n",
      "(1, 5) [[ -543.95179725 -5353.46383075   130.41209623  1360.60178731\n",
      "   1759.35863172]]\n",
      "Training MSE Loss:  [  1843.89953937 103025.61217236  26872.90000282 ...  11714.33943575\n",
      "  49588.49930928  32464.86012944]\n",
      "(1, 5) [[ -542.38044579 -5327.29099494   127.95113965  1350.19456322\n",
      "   1747.81207847]]\n",
      "Training MSE Loss:  [  1835.41948051 102616.07609792  26702.16538154 ...  11589.00207692\n",
      "  49397.77456618  32335.65715958]\n",
      "(1, 5) [[ -540.82121723 -5301.2571671    125.5176638   1339.85191963\n",
      "   1736.32844643]]\n",
      "Training MSE Loss:  [  1827.02296564 102209.50790207  26532.76002074 ...  11464.96339521\n",
      "  49208.15203876  32207.53236922]\n",
      "(1, 5) [[ -539.2739837  -5275.36157096   123.11134881  1329.573494\n",
      "   1724.90743143]]\n",
      "Training MSE Loss:  [  1818.70910698 101805.88229542  26364.67260235 ...  11342.21021041\n",
      "  49019.62300826  32080.47582152]\n",
      "(1, 5) [[ -537.73861869 -5249.60343483   120.73187878  1319.35892548\n",
      "   1713.54873043]]\n",
      "Training MSE Loss:  [  1810.47702782 101405.17423525  26197.89190814 ...  11220.72947178\n",
      "  48832.17884444  31954.47768389]\n",
      "(1, 5) [[ -536.21499706 -5223.98199162   118.37894175  1309.20785494\n",
      "   1702.2520415 ]]\n",
      "Training MSE Loss:  [  1802.32586229 101007.3589228   26032.40681885 ...  11100.50825692\n",
      "  48645.81100453  31829.5282267 ]\n",
      "(1, 5) [[ -534.70299502 -5198.49647876   116.05222964  1299.11992494\n",
      "   1691.01706385]]\n",
      "Training MSE Loss:  [  1794.25475527 100612.41180064  25868.20631332 ...  10981.53377057\n",
      "  48460.51103207  31705.61782212]\n",
      "(1, 5) [[ -533.20249012 -5173.1461382    113.75143821  1289.09477973\n",
      "   1679.84349785]]\n",
      "Training MSE Loss:  [  1786.2628622  100220.30854999  25705.27946766 ...  10863.79334347\n",
      "  48276.27055596  31582.73694293]\n",
      "(1, 5) [[ -531.71336124 -5147.93021638   111.47626699  1279.13206528\n",
      "   1668.73104496]]\n",
      "Training MSE Loss:  [ 1778.34934894 99831.02508813 25543.6154544  ... 10747.27443118\n",
      " 48093.08128937 31460.87616133]\n",
      "(1, 5) [[ -530.23548854 -5122.84796418   109.22641924  1269.23142921\n",
      "   1657.67940782]]\n",
      "Training MSE Loss:  [ 1770.51339165 99444.53756581 25383.20354166 ... 10631.96461298\n",
      " 47910.93502869 31340.02614784]\n",
      "(1, 5) [[ -528.76875349 -5097.89863692   107.00160193  1259.39252084\n",
      "   1646.68829018]]\n",
      "Training MSE Loss:  [ 1762.7541766  99060.82236474 25224.03309229 ... 10517.85159069\n",
      " 47729.82365259 31220.17767008]\n",
      "(1, 5) [[ -527.31303885 -5073.08149431   104.80152564  1249.61499118\n",
      "   1635.75739691]]\n",
      "Training MSE Loss:  [ 1755.07090008 98679.85609505 25066.09356309 ... 10404.92318758\n",
      " 47549.73912097 31101.32159172]\n",
      "(1, 5) [[ -525.86822861 -5048.39580043   102.62590455  1239.8984929\n",
      "   1624.88643404]]\n",
      "Training MSE Loss:  [ 1747.46276823 98301.61559281 24909.37450395 ... 10293.16734722\n",
      " 47370.67347399 30983.44887132]\n",
      "(1, 5) [[ -524.43420803 -5023.84082369   100.47445639  1230.24268037\n",
      "   1614.07510872]]\n",
      "Training MSE Loss:  [ 1739.92899692 97926.0779176  24753.86555708 ... 10182.5721324\n",
      " 47192.61883113 30866.55056124]\n",
      "(1, 5) [[ -523.01086362 -4999.41583681    98.34690237  1220.6472096\n",
      "   1603.32312923]]\n",
      "Training MSE Loss:  [ 1732.46881159 97553.22035004 24599.55645617 ... 10073.12572402\n",
      " 47015.56739018 30750.61780659]\n",
      "(1, 5) [[ -521.59808307 -4975.12011682    96.24296718  1211.11173828\n",
      "   1592.63020501]]\n",
      "Training MSE Loss:  [ 1725.08144716 97183.02038944 24446.43702561 ...  9964.81641999\n",
      " 46839.51142636 30635.64184412]\n",
      "(1, 5) [[ -520.19575533 -4950.95294497    94.16237888  1201.63592576\n",
      "   1581.99604659]]\n",
      "Training MSE Loss:  [ 1717.76614785 96815.45575144 24294.49717969 ...  9857.63263415\n",
      " 46664.44329135 30521.61400119]\n",
      "(1, 5) [[ -518.8037705  -4926.91360675    92.10486893  1192.21943304\n",
      "   1571.42036567]]\n",
      "Training MSE Loss:  [ 1710.5221671  96450.50436561 24143.72692183 ...  9751.56289521\n",
      " 46490.35541239 30408.5256947 ]\n",
      "(1, 5) [[ -517.42201988 -4903.00139185    90.07017209  1182.86192278\n",
      "   1560.90287506]]\n",
      "Training MSE Loss:  [ 1703.34876741 96088.14437323 23994.11634376 ...  9646.59584564\n",
      " 46317.24029137 30296.36843008]\n",
      "(1, 5) [[ -516.05039596 -4879.21559415    88.05802641  1173.56305928\n",
      "   1550.4432887 ]]\n",
      "Training MSE Loss:  [ 1696.24522022 95728.35412493 23845.6556248  ...  9542.72024067\n",
      " 46145.09050395 30185.13380029]\n",
      "(1, 5) [[ -514.68879234 -4855.55551164    86.06817315  1164.32250851\n",
      "   1540.04132167]]\n",
      "Training MSE Loss:  [ 1689.21080581 95371.11217848 23698.33503106 ...  9439.9249472\n",
      " 45973.89869867 30074.81348478]\n",
      "(1, 5) [[ -513.3371038  -4832.02044646    84.1003568   1155.13993804\n",
      "   1529.69669018]]\n",
      "Training MSE Loss:  [ 1682.24481315 95016.39729656 23552.14491469 ...  9338.19894277\n",
      " 45803.6575961  29965.3992485 ]\n",
      "(1, 5) [[ -511.99522624 -4808.60970482    82.15432496  1146.0150171\n",
      "   1519.40911155]]\n",
      "Training MSE Loss:  [ 1675.34653979 94664.18844455 23407.07571309 ...  9237.53131452\n",
      " 45634.35998796 29856.88294096]\n",
      "(1, 5) [[ -510.66305668 -4785.32259703    80.22982838  1136.94741655\n",
      "   1509.17830426]]\n",
      "Training MSE Loss:  [ 1668.51529177 94314.46478834 23263.11794824 ...  9137.91125816\n",
      " 45465.9987363  29749.25649522]\n",
      "(1, 5) [[ -509.34049324 -4762.15843739    78.32662086  1127.93680888\n",
      "   1499.00398788]]\n",
      "Training MSE Loss:  [ 1661.75038345 93967.20569222 23120.26222587 ...  9039.328077\n",
      " 45298.56677267 29642.51192696]\n",
      "(1, 5) [[ -508.02743513 -4739.11654425    76.44445922  1118.98286818\n",
      "   1488.88588313]]\n",
      "Training MSE Loss:  [ 1655.05113746 93622.39071672 22978.49923477 ...  8941.77118086\n",
      " 45132.05709728 29536.64133352]\n",
      "(1, 5) [[ -506.72378266 -4716.19623994    74.58310331  1110.08527019\n",
      "   1478.82371186]]\n",
      "Training MSE Loss:  [ 1648.41688454 93279.99961655 22837.81974606 ...  8845.23008514\n",
      " 44966.4627782  29431.63689299]\n",
      "(1, 5) [[ -505.42943719 -4693.39685076    72.74231589  1101.24369223\n",
      "   1468.81719701]]\n",
      "Training MSE Loss:  [ 1641.84696344 92940.01233849 22698.21461244 ...  8749.69440978\n",
      " 44801.77695057 29327.49086327]\n",
      "(1, 5) [[ -504.14430116 -4670.71770692    70.92186267  1092.45781325\n",
      "   1458.86606267]]\n",
      "Training MSE Loss:  [ 1635.34072083 92602.40901935 22559.6747675  ...  8655.15387831\n",
      " 44637.99281582 29224.19558117]\n",
      "(1, 5) [[ -502.86827803 -4648.15814257    69.12151222  1083.72731379\n",
      "   1448.97003405]]\n",
      "Training MSE Loss:  [ 1628.89751117 92267.16998397 22422.19122497 ...  8561.59831684\n",
      " 44475.10364087 29121.7434615 ]\n",
      "(1, 5) [[ -501.60127231 -4625.71749574    67.34103595  1075.05187601\n",
      "   1439.12883747]]\n",
      "Training MSE Loss:  [ 1622.51669663 91934.27574318 22285.75507805 ...  8469.01765312\n",
      " 44313.10275737 29020.12699623]\n",
      "(1, 5) [[ -500.34318954 -4603.39510831    65.58020808  1066.43118363\n",
      "   1429.34220037]]\n",
      "Training MSE Loss:  [ 1616.19764696 91603.70699188 22150.3574987  ...  8377.40191557\n",
      " 44151.98356097 28919.33875354]\n",
      "(1, 5) [[ -499.09393625 -4581.190326      63.83880561  1057.864922\n",
      "   1419.60985131]]\n",
      "Training MSE Loss:  [ 1609.93973939 91275.44460703 22015.98973691 ...  8286.74123231\n",
      " 43991.73951054 28819.37137702]\n",
      "(1, 5) [[ -497.85342    -4559.10249837    62.11660825  1049.35277801\n",
      "   1409.93151996]]\n",
      "Training MSE Loss:  [ 1603.74235856 90949.46964573 21882.64312006 ...  8197.02583023\n",
      " 43832.36412746 28720.21758479]\n",
      "(1, 5) [[ -496.62154933 -4537.13097873    60.41339841  1040.89444016\n",
      "   1400.30693711]]\n",
      "Training MSE Loss:  [ 1597.60489639 90625.76334335 21750.30905219 ...  8108.24603407\n",
      " 43673.85099488 28621.87016867]\n",
      "(1, 5) [[ -495.39823374 -4515.27512418    58.72896118  1032.48959851\n",
      "   1390.73583466]]\n",
      "Training MSE Loss:  [ 1591.52675199 90304.30711162 21618.97901335 ...  8020.39226546\n",
      " 43516.19375702 28524.32199331]\n",
      "(1, 5) [[ -494.18338372 -4493.53429554    57.06308427  1024.1379447\n",
      "   1381.21794562]]\n",
      "Training MSE Loss:  [ 1585.50733156 89985.08253674 21488.64455893 ...  7933.45504199\n",
      " 43359.38611845 28427.56599541]\n",
      "(1, 5) [[ -492.97691072 -4471.90785738    55.41555798  1015.83917192\n",
      "   1371.75300409]]\n",
      "Training MSE Loss:  [ 1579.5460483  89668.07137758 21359.29731896 ...  7847.42497634\n",
      " 43203.42184338 28331.59518291]\n",
      "(1, 5) [[ -491.77872712 -4450.39517792    53.78617517  1007.59297494\n",
      "   1362.34074532]]\n",
      "Training MSE Loss:  [ 1573.64232234 89353.25556384 21230.92899748 ...  7762.29277535\n",
      " 43048.29475503 28236.40263415]\n",
      "(1, 5) [[ -490.58874627 -4428.99562909    52.17473124   999.39905006\n",
      "   1352.98090563]]\n",
      "Training MSE Loss:  [ 1567.79558059 89040.61719428 21103.53137186 ...  7678.04923909\n",
      " 42893.99873487 28141.9814971 ]\n",
      "(1, 5) [[ -489.4068824  -4407.70858643    50.58102407   991.25709514\n",
      "   1343.67322246]]\n",
      "Training MSE Loss:  [ 1562.00525671 88730.13853487 20977.09629217 ...  7594.68526004\n",
      " 42740.52772202 28048.32498857]\n",
      "(1, 5) [[ -488.2330507  -4386.53342915    49.00485402   983.16680959\n",
      "   1334.41743434]]\n",
      "Training MSE Loss:  [ 1556.27079098 88421.80201713 20851.61568051 ...  7512.19182213\n",
      " 42587.87571257 27955.42639348]\n",
      "(1, 5) [[ -487.06716724 -4365.46954       47.44602388   975.12789434\n",
      "   1325.2132809 ]]\n",
      "Training MSE Loss:  [ 1550.59163023 88115.59023632 20727.08153041 ...  7430.5599999\n",
      " 42436.0367589  27863.279064  ]\n",
      "(1, 5) [[ -485.90914901 -4344.51630536    45.90433882   967.14005187\n",
      "   1316.06050288]]\n",
      "Training MSE Loss:  [ 1544.96722775 87811.48594974 20603.48590612 ...  7349.78095765\n",
      " 42285.00496907 27771.8764189 ]\n",
      "(1, 5) [[ -484.75891386 -4323.67311515    44.37960641   959.20298619\n",
      "   1306.95884211]]\n",
      "Training MSE Loss:  [ 1539.39704321 87509.47207507 20480.82094208 ...  7269.84594855\n",
      " 42134.77450618 27681.21194275]\n",
      "(1, 5) [[ -483.61638055 -4302.9393628     42.87163653   951.31640282\n",
      "   1297.90804152]]\n",
      "Training MSE Loss:  [ 1533.88054256 87209.53168864 20359.07884221 ...  7190.74631377\n",
      " 41985.33958773 27591.27918518]\n",
      "(1, 5) [[ -482.48146868 -4282.31444529    41.38024141   943.4800088\n",
      "   1288.90784511]]\n",
      "Training MSE Loss:  [ 1528.41719795 86911.64802381 20238.25187933 ...  7112.47348169\n",
      " 41836.69448504 27502.07176017]\n",
      "(1, 5) [[ -481.35409873 -4261.79776306    39.90523552   935.6935127\n",
      "   1279.95799799]]\n",
      "Training MSE Loss:  [ 1523.00648767 86615.80446931 20118.33239453 ...  7035.018967\n",
      " 41688.83352258 27413.58334535]\n",
      "(1, 5) [[ -480.23419201 -4241.38872001    38.4464356    927.95662456\n",
      "   1271.05824635]]\n",
      "Training MSE Loss:  [ 1517.64789606 86321.98456765 19999.31279659 ...  6958.37436991\n",
      " 41541.75107741 27325.80768125]\n",
      "(1, 5) [[ -479.12167068 -4221.08672353    37.00366061   920.26905596\n",
      "   1262.20833748]]\n",
      "Training MSE Loss:  [ 1512.34091339 86030.17201349 19881.18556134 ...  6882.53137529\n",
      " 41395.44157859 27238.73857061]\n",
      "(1, 5) [[ -478.01645775 -4200.89118438    35.5767317    912.63051995\n",
      "   1253.40801972]]\n",
      "Training MSE Loss:  [ 1507.08503586 85740.35065206 19763.94323109 ...  6807.48175191\n",
      " 41249.89950658 27152.36987771]\n",
      "(1, 5) [[ -476.91847701 -4180.80151675    34.1654722    905.04073109\n",
      "   1244.65704253]]\n",
      "Training MSE Loss:  [ 1501.87976543 85452.50447761 19647.57841402 ...  6733.21735154\n",
      " 41105.11939265 27066.69552768]\n",
      "(1, 5) [[ -475.8276531  -4160.81713821    32.76970756   897.49940541\n",
      "   1235.95515642]]\n",
      "Training MSE Loss:  [ 1496.72460984 85166.61763186 19532.08378358 ...  6659.73010823\n",
      " 40961.09581835 26981.7095058 ]\n",
      "(1, 5) [[ -474.74391146 -4140.93746969    31.38926535   890.00626044\n",
      "   1227.30211299]]\n",
      "Training MSE Loss:  [ 1491.61908246 84882.67440247 19417.45207794 ...  6587.01203747\n",
      " 40817.8234149  26897.40585685]\n",
      "(1, 5) [[ -473.6671783  -4121.16193544    30.02397521   882.56101515\n",
      "   1218.69766491]]\n",
      "Training MSE Loss:  [ 1486.56270225 84600.65922155 19303.67609938 ...  6515.0552354\n",
      " 40675.29686267 26813.77868447]\n",
      "(1, 5) [[ -472.59738065 -4101.48996305    28.67366887   875.16339001\n",
      "   1210.14156592]]\n",
      "Training MSE Loss:  [ 1481.55499368 84320.5566641  19190.74871375 ...  6443.85187806\n",
      " 40533.5108906  26730.82215047]\n",
      "(1, 5) [[ -471.5344463  -4081.92098339    27.33818006   867.81310694\n",
      "   1201.63357083]]\n",
      "Training MSE Loss:  [ 1476.59548666 84042.35144663 19078.66284986 ...  6373.39422056\n",
      " 40392.46027568 26648.5304742 ]\n",
      "(1, 5) [[ -470.47830382 -4062.45443063    26.01734453   860.50988933\n",
      "   1193.17343552]]\n",
      "Training MSE Loss:  [ 1471.68371646 83766.02842563 18967.41149893 ...  6303.67459637\n",
      " 40252.13984244 26566.89793195]\n",
      "(1, 5) [[ -469.42888252 -4043.08974218    24.71099999   853.25346201\n",
      "   1184.76091694]]\n",
      "Training MSE Loss:  [ 1466.81922367 83491.57259613 18856.98771407 ...  6234.6854165\n",
      " 40112.54446235 26485.91885625]\n",
      "(1, 5) [[ -468.38611249 -4023.82635867    23.41898613   846.04355126\n",
      "   1176.39577308]]\n",
      "Training MSE Loss:  [ 1462.00155408 83218.96909034 18747.38460966 ...  6166.41916881\n",
      " 39973.66905338 26405.58763532]\n",
      "(1, 5) [[ -467.34992456 -4004.66372397    22.14114456   838.8798848\n",
      "   1168.07776301]]\n",
      "Training MSE Loss:  [ 1457.23025866 82948.20317615 18638.59536085 ...  6098.86841717\n",
      " 39835.50857943 26325.89871243]\n",
      "(1, 5) [[ -466.32025029 -3985.60128515    20.87731878   831.7621918\n",
      "   1159.80664685]]\n",
      "Training MSE Loss:  [ 1452.50489348 82679.2602558  18530.613203   ...  6032.02580083\n",
      " 39698.05804984 26246.84658528]\n",
      "(1, 5) [[ -465.29702197 -3966.63849243    19.62735419   824.69020284\n",
      "   1151.58218577]]\n",
      "Training MSE Loss:  [ 1447.82501962 82412.12586449 18423.43143113 ...  5965.8840336\n",
      " 39561.31251891 26168.4258054 ]\n",
      "(1, 5) [[ -464.28017261 -3947.77479921    18.39109805   817.66364993\n",
      "   1143.40414201]]\n",
      "Training MSE Loss:  [ 1443.19020314 82146.78566898 18317.0433994  ...  5900.43590312\n",
      " 39425.26708537 26090.63097761]\n",
      "(1, 5) [[ -463.26963596 -3929.00966202    17.16839942   810.68226651\n",
      "   1135.27227883]]\n",
      "Training MSE Loss:  [ 1438.60001499 81883.2254663  18211.44252057 ...  5835.67427022\n",
      " 39289.91689192 26013.45675938]\n",
      "(1, 5) [[ -462.26534643 -3910.34254049    15.95910921   803.74578743\n",
      "   1127.18636055]]\n",
      "Training MSE Loss:  [ 1434.05403097 81621.43118239 18106.62226547 ...  5771.5920681\n",
      " 39155.25712473 25936.89786025]\n",
      "(1, 5) [[ -461.26723917 -3891.77289737    14.76308009   796.85394895\n",
      "   1119.14615255]]\n",
      "Training MSE Loss:  [ 1429.55183164 81361.38887076 18002.5761625  ...  5708.18230169\n",
      " 39021.28301299 25860.94904132]\n",
      "(1, 5) [[ -460.27525    -3873.30019849    13.5801665    790.00648871\n",
      "   1111.15142122]]\n",
      "Training MSE Loss:  [ 1425.09300228 81103.08471125 17899.29779708 ...  5645.43804692\n",
      " 38887.98982838 25785.60511463]\n",
      "(1, 5) [[ -459.28931543 -3854.92391272    12.41022462   783.20314579\n",
      "   1103.20193401]]\n",
      "Training MSE Loss:  [ 1420.67713281 80846.50500867 17796.78081117 ...  5583.35245003\n",
      " 38755.37288471 25710.86094262]\n",
      "(1, 5) [[ -458.30937264 -3836.64351199    11.25311236   776.44366062\n",
      "   1095.2974594 ]]\n",
      "Training MSE Loss:  [ 1416.30381778 80591.6361916  17695.01890273 ...  5521.91872687\n",
      " 38623.42753734 25636.71143758]\n",
      "(1, 5) [[ -457.33535949 -3818.45847125    10.10868932   769.72777503\n",
      "   1087.4377669 ]]\n",
      "Training MSE Loss:  [ 1411.97265623 80338.46481106 17594.00582526 ...  5461.13016224\n",
      " 38492.14918285 25563.15156111]\n",
      "(1, 5) [[ -456.3672145  -3800.36826844     8.97681677   763.05523224\n",
      "   1079.62262705]]\n",
      "Training MSE Loss:  [ 1407.68325171 80086.97753934 17493.73538723 ...  5400.98010917\n",
      " 38361.53325848 25490.17632358]\n",
      "(1, 5) [[ -455.40487684 -3782.37238449     7.85735764   756.42577685\n",
      "   1071.85181143]]\n",
      "Training MSE Loss:  [ 1403.43521217 79837.16116869 17394.20145167 ...  5341.46198829\n",
      " 38231.57524177 25417.78078362]\n",
      "(1, 5) [[ -454.44828632 -3764.47030332     6.75017649   749.83915479\n",
      "   1064.12509261]]\n",
      "Training MSE Loss:  [ 1399.22814993 79589.0026102  17295.39793562 ...  5282.56928715\n",
      " 38102.2706501  25345.96004754]\n",
      "(1, 5) [[ -453.4973834  -3746.66151177     5.6551395    743.2951134\n",
      "   1056.44224423]]\n",
      "Training MSE Loss:  [ 1395.06168161 79342.48889249 17197.31880966 ...  5224.29555952\n",
      " 37973.61504026 25274.70926888]\n",
      "(1, 5) [[ -452.55210919 -3728.94549961     4.57211444   736.79340134\n",
      "   1048.8030409 ]]\n",
      "Training MSE Loss:  [ 1390.9354281  79097.60716061 17099.95809744 ...  5166.63442483\n",
      " 37845.60400801 25204.02364784]\n",
      "(1, 5) [[-4.51612405e+02 -3.71132176e+03  3.50097065e+00  7.30333769e+02\n",
      "   1.04120726e+03]]\n",
      "Training MSE Loss:  [ 1386.84901448 78854.34467482 17003.30987517 ...  5109.57956741\n",
      " 37718.23318769 25133.89843082]\n",
      "(1, 5) [[-4.50678214e+02 -3.69378979e+03  2.44157902e+00  7.23915967e+02\n",
      "   1.03365467e+03]]\n",
      "Training MSE Loss:  [ 1382.80206996 78612.68880945 16907.36827119 ...  5053.12473594\n",
      " 37591.49825178 25064.3289099 ]\n",
      "(1, 5) [[-4.49749479e+02 -3.67634908e+03  1.39381197e+00  7.17539748e+02\n",
      "   1.02614506e+03]]\n",
      "Training MSE Loss:  [ 1378.79422788 78372.62705172 16812.12746546 ...  4997.26374275\n",
      " 37465.39491052 24995.31042233]\n",
      "(1, 5) [[-4.48826143e+02 -3.65899914e+03  3.57543458e-01  7.11204867e+02\n",
      "   1.01867821e+03]]\n",
      "Training MSE Loss:  [ 1374.82512558 78134.14700062 16717.58168912 ...  4941.99046325\n",
      " 37339.91891149 24926.83835007]\n",
      "(1, 5) [[-4.47908150e+02 -3.64173948e+03 -6.67351100e-01  7.04911079e+02\n",
      "   1.01125388e+03]]\n",
      "Training MSE Loss:  [ 1370.89440442 77897.23636581 16623.72522403 ...  4887.29883523\n",
      " 37215.06603921 24858.90811931]\n",
      "(1, 5) [[-4.46995446e+02 -3.62456959e+03 -1.68099479e+00  6.98658141e+02\n",
      "   1.00387187e+03]]\n",
      "Training MSE Loss:  [ 1367.00170968 77661.88296647 16530.55240229 ...  4833.18285831\n",
      " 37090.83211474 24791.51519994]\n",
      "(1, 5) [[-4.46087975e+02 -3.60748899e+03 -2.68350925e+00  6.92445810e+02\n",
      "   9.96531961e+02]]\n",
      "Training MSE Loss:  [ 1363.14669055 77428.07473022 16438.05760581 ...  4779.6365933\n",
      " 36967.21299531 24724.65510515]\n",
      "(1, 5) [[ -445.18568392 -3590.49718759    -3.67501467   686.27384591\n",
      "    989.23392627]]\n",
      "Training MSE Loss:  [ 1359.32900005 77195.79969204 16346.23526584 ...  4726.65416158\n",
      " 36844.20457395 24658.32339093]\n",
      "(1, 5) [[ -444.28851857 -3573.59370497    -4.65562983   680.14200959\n",
      "    981.97755359]]\n",
      "Training MSE Loss:  [ 1355.548295   76965.04599317 16255.07986255 ...  4674.22974452\n",
      " 36721.80277905 24592.5156556 ]\n",
      "(1, 5) [[ -443.39642627 -3556.77805687    -5.62547211   674.05006281\n",
      "    974.76262768]]\n",
      "Training MSE Loss:  [ 1351.80423594 76735.80188008 16164.58592456 ...  4622.35758288\n",
      " 36600.00357407 24527.22753938]\n",
      "(1, 5) [[ -442.50935467 -3540.04976414    -6.58465749   667.9977688\n",
      "    967.58893417]]\n",
      "Training MSE Loss:  [ 1348.09648716 76508.05570339 16074.74802853 ...  4571.03197621\n",
      " 36478.80295708 24462.45472393]\n",
      "(1, 5) [[ -441.62725196 -3523.40835037    -7.53330061   661.9848921\n",
      "    960.4562597 ]]\n",
      "Training MSE Loss:  [ 1344.42471654 76281.79591685 15985.5607987  ...  4520.24728225\n",
      " 36358.19696049 24398.19293188]\n",
      "(1, 5) [[ -440.75006688 -3506.85334185    -8.47151473   656.01119852\n",
      "    953.36439187]]\n",
      "Training MSE Loss:  [ 1340.78859563 76057.0110763  15897.0189065  ...  4469.99791641\n",
      " 36238.18165062 24334.43792645]\n",
      "(1, 5) [[ -439.87774873 -3490.38426756    -9.39941179   650.07645514\n",
      "    946.31311925]]\n",
      "Training MSE Loss:  [ 1337.18779949 75833.68983867 15809.11707006 ...  4420.27835112\n",
      " 36118.75312735 24271.18551096]\n",
      "(1, 5) [[ -439.01024734 -3474.00065916   -10.31710241   644.18043034\n",
      "    939.30223141]]\n",
      "Training MSE Loss:  [ 1333.62200672 75611.82096094 15721.85005386 ...  4371.08311532\n",
      " 35999.90752382 24208.43152844]\n",
      "(1, 5) [[ -438.14751307 -3457.70205096   -11.22469594   638.32289374\n",
      "    932.33151888]]\n",
      "Training MSE Loss:  [ 1330.0908994  75391.3932992  15635.21266825 ...  4322.40679387\n",
      " 35881.64100602 24146.17186116]\n",
      "(1, 5) [[ -437.28949682 -3441.48797991   -12.12230039   632.50361624\n",
      "    925.40077313]]\n",
      "Training MSE Loss:  [ 1326.59416303 75172.39580762 15549.19976909 ...  4274.24402698\n",
      " 35763.94977249 24084.40243029]\n",
      "(1, 5) [[ -436.43615    -3425.35798559   -13.01002256   626.72236998\n",
      "    918.50978664]]\n",
      "Training MSE Loss:  [ 1323.1314865  74954.81753748 15463.80625729 ...  4226.5895097\n",
      " 35646.83005396 24023.11919538]\n",
      "(1, 5) [[ -435.58742454 -3409.3116102    -13.88796797   620.97892837\n",
      "    911.65835282]]\n",
      "Training MSE Loss:  [ 1319.70256205 74738.64763624 15379.02707844 ...  4179.43799132\n",
      " 35530.27811302 23962.31815406]\n",
      "(1, 5) [[ -434.7432729  -3393.34839853   -14.7562409    615.27306606\n",
      "    904.84626605]]\n",
      "Training MSE Loss:  [ 1316.30708521 74523.87534656 15294.85722237 ...  4132.78427486\n",
      " 35414.2902438  23901.99534156]\n",
      "(1, 5) [[ -433.90364802 -3377.46789792   -15.61494442   609.60455892\n",
      "    898.07332166]]\n",
      "Training MSE Loss:  [ 1312.94475479 74310.49000539 15211.29172279 ...  4086.62321654\n",
      " 35298.86277164 23842.14683033]\n",
      "(1, 5) [[ -433.06850335 -3361.66965832   -16.46418039   603.97318407\n",
      "    891.33931592]]\n",
      "Training MSE Loss:  [ 1309.61527282 74098.48104298 15128.32565687 ...  4040.94972522\n",
      " 35183.99205274 23782.76872966]\n",
      "(1, 5) [[ -432.23779285 -3345.9532322    -17.3040495    598.37871986\n",
      "    884.64404609]]\n",
      "Training MSE Loss:  [ 1306.31834451 73887.83798204 15045.95414486 ...  3995.75876188\n",
      " 35069.67447389 23723.85718528]\n",
      "(1, 5) [[ -431.41147095 -3330.31817455   -18.13465125   592.82094587\n",
      "    877.98731032]]\n",
      "Training MSE Loss:  [ 1303.05367822 73678.55043675 14964.17234966 ...  3951.04533911\n",
      " 34955.9064521  23665.40837898]\n",
      "(1, 5) [[ -430.58949258 -3314.76404291   -18.95608396   587.29964287\n",
      "    871.36890774]]\n",
      "Training MSE Loss:  [ 1299.8209854  73470.60811194 14882.9754765  ...  3906.80452059\n",
      " 34842.68443435 23607.41852821]\n",
      "(1, 5) [[ -429.77181313 -3299.2903973    -19.76844485   581.81459286\n",
      "    864.78863841]]\n",
      "Training MSE Loss:  [ 1296.61998059 73264.00080211 14802.35877252 ...  3863.03142058\n",
      " 34730.00489723 23549.88388572]\n",
      "(1, 5) [[ -428.95838848 -3283.89680022   -20.57182997   576.36557905\n",
      "    858.24630333]]\n",
      "Training MSE Loss:  [ 1293.45038134 73058.71839064 14722.31752637 ...  3819.72120338\n",
      " 34617.86434667 23492.80073918]\n",
      "(1, 5) [[ -428.149175   -3268.58281668   -21.36633427   570.95238583\n",
      "    851.74170441]]\n",
      "Training MSE Loss:  [ 1290.31190821 72854.75084884 14642.84706789 ...  3776.86908288\n",
      " 34506.25931763 23436.16541084]\n",
      "(1, 5) [[ -427.34412947 -3253.3480141    -22.1520516    565.57479882\n",
      "    845.27464451]]\n",
      "Training MSE Loss:  [ 1287.2042847  72652.08823515 14563.94276767 ...  3734.47032202\n",
      " 34395.18637378 23379.9742571 ]\n",
      "(1, 5) [[ -426.54320919 -3238.19196237   -22.92907471   560.23260479\n",
      "    838.84492743]]\n",
      "Training MSE Loss:  [ 1284.12723724 72450.72069425 14485.60003677 ...  3692.52023233\n",
      " 34284.64210726 23324.22366824]\n",
      "(1, 5) [[ -425.74637188 -3223.1142338    -23.69749529   554.92559171\n",
      "    832.45235785]]\n",
      "Training MSE Loss:  [ 1281.08049513 72250.63845622 14407.81432624 ...  3651.01417339\n",
      " 34174.62313834 23268.91006798]\n",
      "(1, 5) [[ -424.95357572 -3208.11440311   -24.45740395   549.65354875\n",
      "    826.09674141]]\n",
      "Training MSE Loss:  [ 1278.06379055 72051.83183571 14330.58112687 ...  3609.94755242\n",
      " 34065.12611516 23214.02991317]\n",
      "(1, 5) [[ -424.16477933 -3193.19204743   -25.20889026   544.41626621\n",
      "    819.77788465]]\n",
      "Training MSE Loss:  [ 1275.07685846 71854.29123116 14253.89596876 ...  3569.31582373\n",
      " 33956.14771345 23159.57969346]\n",
      "(1, 5) [[ -423.37994177 -3178.34674627   -25.95204275   539.21353559\n",
      "    813.49559502]]\n",
      "Training MSE Loss:  [ 1272.11943662 71658.00712388 14177.75442098 ...  3529.1144883\n",
      " 33847.68463623 23105.5559309 ]\n",
      "(1, 5) [[ -422.59902255 -3163.5780815    -26.68694895   534.04514954\n",
      "    807.24968088]]\n",
      "Training MSE Loss:  [ 1269.19126553 71462.97007735 14102.15209123 ...  3489.33909327\n",
      " 33739.73361356 23051.95517968]\n",
      "(1, 5) [[ -421.8219816  -3148.88563735   -27.41369534   528.91090187\n",
      "    801.03995151]]\n",
      "Training MSE Loss:  [ 1266.29208841 71269.17073637 14027.08462548 ...  3449.98523148\n",
      " 33632.29140225 22998.7740257 ]\n",
      "(1, 5) [[ -421.04877928 -3134.26900041   -28.13236744   523.81058753\n",
      "    794.86621709]]\n",
      "Training MSE Loss:  [ 1263.42165114 71076.59982626 13952.54770764 ...  3411.04854106\n",
      " 33525.3547856  22946.00908632]\n",
      "(1, 5) [[ -420.27937637 -3119.72775957   -28.84304975   518.74400264\n",
      "    788.72828868]]\n",
      "Training MSE Loss:  [ 1260.57970227 70885.24815211 13878.53705917 ...  3372.52470489\n",
      " 33418.92057314 22893.65701   ]\n",
      "(1, 5) [[ -419.51373408 -3105.26150606   -29.54582584   513.71094443\n",
      "    782.62597825]]\n",
      "Training MSE Loss:  [ 1257.76599297 70695.10659798 13805.04843882 ...  3334.40945022\n",
      " 33312.98560032 22841.71447594]\n",
      "(1, 5) [[ -418.75181401 -3090.86983338   -30.24077828   508.71121128\n",
      "    776.55909868]]\n",
      "Training MSE Loss:  [ 1254.98027696 70506.16612616 13732.0776422  ...  3296.69854818\n",
      " 33207.54672835 22790.17819385]\n",
      "(1, 5) [[ -417.9935782  -3076.55233736   -30.92798871   503.74460271\n",
      "    770.52746373]]\n",
      "Training MSE Loss:  [ 1252.22231055 70318.41777639 13659.62050152 ...  3259.38781334\n",
      " 33102.60084382 22739.04490353]\n",
      "(1, 5) [[ -417.23898906 -3062.30861606   -31.60753783   498.81091934\n",
      "    764.53088802]]\n",
      "Training MSE Loss:  [ 1249.49185255 70131.85266514 13587.67288523 ...  3222.47310331\n",
      " 32998.14485856 22688.31137464]\n",
      "(1, 5) [[ -416.48800943 -3048.13826982   -32.27950542   493.90996292\n",
      "    758.5691871 ]]\n",
      "Training MSE Loss:  [ 1246.78866427 69946.46198482 13516.23069768 ...  3185.95031825\n",
      " 32894.17570931 22637.97440632]\n",
      "(1, 5) [[ -415.74060253 -3034.04090122   -32.94397033   489.04153631\n",
      "    752.64217736]]\n",
      "Training MSE Loss:  [ 1244.11250949 69762.23700311 13445.28987882 ...  3149.81540047\n",
      " 32790.6903575  22588.03082695]\n",
      "(1, 5) [[ -414.99673199 -3020.01611509   -33.60101052   484.20544349\n",
      "    746.7496761 ]]\n",
      "Training MSE Loss:  [ 1241.4631544  69579.16906219 13374.84640387 ...  3114.06433401\n",
      " 32687.68578902 22538.47749381]\n",
      "(1, 5) [[ -414.25636181 -3006.06351846   -34.25070306   479.40148953\n",
      "    740.89150148]]\n",
      "Training MSE Loss:  [ 1238.84036761 69397.24957804 13304.89628298 ...  3078.69314418\n",
      " 32585.15901394 22489.31129278]\n",
      "(1, 5) [[ -413.51945638 -2992.18272056   -34.89312414   474.6294806\n",
      "    735.06747251]]\n",
      "Training MSE Loss:  [ 1236.2439201  69216.47003973 13235.43556095 ...  3043.6978972\n",
      " 32483.10706633 22440.52913805]\n",
      "(1, 5) [[ -412.78598046 -2978.37333285   -35.52834908   469.88922396\n",
      "    729.27740911]]\n",
      "Training MSE Loss:  [ 1233.67358521 69036.82200871 13166.46031688 ...  3009.07469974\n",
      " 32381.52700397 22392.12797185]\n",
      "(1, 5) [[ -412.05589922 -2964.63496893   -36.15645232   465.18052795\n",
      "    723.52113204]]\n",
      "Training MSE Loss:  [ 1231.12913857 68858.29711813 13097.9666639  ...  2974.8196985\n",
      " 32280.41590812 22344.10476411]\n",
      "(1, 5) [[ -411.32917816 -2950.96724459   -36.77750749   460.50320201\n",
      "    717.79846292]]\n",
      "Training MSE Loss:  [ 1228.61035813 68680.88707215 13029.95074883 ...  2940.92907987\n",
      " 32179.77088333 22296.45651223]\n",
      "(1, 5) [[ -410.60578317 -2937.36977776   -37.39158736   455.85705664\n",
      "    712.10922422]]\n",
      "Training MSE Loss:  [ 1226.11702408 68504.58364524 12962.40875187 ...  2907.39906945\n",
      " 32079.58905717 22249.18024076]\n",
      "(1, 5) [[ -409.88568051 -2923.84218853   -37.99876386   451.24190341\n",
      "    706.45323929]]\n",
      "Training MSE Loss:  [ 1223.64891885 68329.37868154 12895.33688633 ...  2874.2259317\n",
      " 31979.86758002 22202.27300113]\n",
      "(1, 5) [[ -409.16883679 -2910.38409911   -38.59910813   446.65755497\n",
      "    700.83033232]]\n",
      "Training MSE Loss:  [ 1221.20582711 68155.26409416 12828.73139831 ...  2841.40596953\n",
      " 31880.60362487 22155.7318714 ]\n",
      "(1, 5) [[ -408.45521897 -2896.99513382   -39.1926905    442.10382501\n",
      "    695.24032834]]\n",
      "Training MSE Loss:  [ 1218.78753566 67982.23186456 12762.58856641 ...  2808.93552392\n",
      " 31781.79438706 22109.55395593]\n",
      "(1, 5) [[ -407.74479438 -2883.6749191    -39.77958048   437.58052829\n",
      "    689.68305322]]\n",
      "Training MSE Loss:  [ 1216.3938335  67810.27404187 12696.90470143 ...  2776.81097351\n",
      " 31683.43708407 22063.73638517]\n",
      "(1, 5) [[ -407.0375307  -2870.42308347   -40.35984681   433.08748061\n",
      "    684.1583337 ]]\n",
      "Training MSE Loss:  [ 1214.02451176 67639.38274225 12631.67614607 ...  2745.02873424\n",
      " 31585.52895533 22018.27631535]\n",
      "(1, 5) [[ -406.33339593 -2857.23925752   -40.93355746   428.62449882\n",
      "    678.66599733]]\n",
      "Training MSE Loss:  [ 1211.67936365 67469.55014827 12566.89927469 ...  2713.58525895\n",
      " 31488.06726199 21973.17092824]\n",
      "(1, 5) [[ -405.63235844 -2844.12307394   -41.50077961   424.19140081\n",
      "    673.20587251]]\n",
      "Training MSE Loss:  [ 1209.35818448 67300.76850825 12502.57049294 ...  2682.47703705\n",
      " 31391.04928671 21928.41743089]\n",
      "(1, 5) [[ -404.93438693 -2831.07416744   -42.0615797    419.7880055\n",
      "    667.77778845]]\n",
      "Training MSE Loss:  [ 1207.06077163 67133.03013565 12438.68623755 ...  2651.70059408\n",
      " 31294.47233345 21884.01305535]\n",
      "(1, 5) [[ -404.23945043 -2818.09217479   -42.61602342   415.41413283\n",
      "    662.38157523]]\n",
      "Training MSE Loss:  [ 1204.78692448 66966.32740848 12375.24297603 ...  2621.25249139\n",
      " 31198.33372728 21839.95505844]\n",
      "(1, 5) [[ -403.54751829 -2805.1767348    -43.1641757    411.06960379\n",
      "    657.01706372]]\n",
      "Training MSE Loss:  [ 1202.53644446 66800.65276865 12312.23720636 ...  2591.12932576\n",
      " 31102.63081414 21796.24072148]\n",
      "(1, 5) [[ -402.85856023 -2792.32748827   -43.70610077   406.75424037\n",
      "    651.68408561]]\n",
      "Training MSE Loss:  [ 1200.30913494 66635.99872136 12249.66545678 ...  2561.32772906\n",
      " 31007.36096069 21752.86735004]\n",
      "(1, 5) [[ -402.17254624 -2779.54407804   -44.2418621    402.46786557\n",
      "    646.38247344]]\n",
      "Training MSE Loss:  [ 1198.1048013  66472.35783454 12187.52428542 ...  2531.84436784\n",
      " 30912.52155408 21709.83227372]\n",
      "(1, 5) [[ -401.48944668 -2766.82614892   -44.77152249   398.2103034\n",
      "    641.11206054]]\n",
      "Training MSE Loss:  [ 1195.92325083 66309.72273824 12125.81028014 ...  2502.67594304\n",
      " 30818.11000175 21667.13284588]\n",
      "(1, 5) [[ -400.80923219 -2754.17334773   -45.29514401   393.98137888\n",
      "    635.87268105]]\n",
      "Training MSE Loss:  [ 1193.76429274 66148.08612403 12064.52005818 ...  2473.81918961\n",
      " 30724.12373124 21624.76644338]\n",
      "(1, 5) [[ -400.13187375 -2741.58532323   -45.81278803   389.78091803\n",
      "    630.66416994]]\n",
      "Training MSE Loss:  [ 1191.62773814 65987.44074445 12003.65026593 ...  2445.27087613\n",
      " 30630.56019002 21582.73046642]\n",
      "(1, 5) [[ -399.45734264 -2729.06172616   -46.32451525   385.60874786\n",
      "    625.48636296]]\n",
      "Training MSE Loss:  [ 1189.51340001 65827.7794124  11943.19757865 ...  2417.02780454\n",
      " 30537.41684527 21541.0223382 ]\n",
      "(1, 5) [[ -398.78561045 -2716.60220922   -46.83038568   381.46469637\n",
      "    620.33909669]]\n",
      "Training MSE Loss:  [ 1187.42109319 65669.09500059 11883.15870022 ...  2389.08680974\n",
      " 30444.69118372 21499.63950476]\n",
      "(1, 5) [[ -398.11664906 -2704.20642701   -47.33045866   377.34859252\n",
      "    615.22220848]]\n",
      "Training MSE Loss:  [ 1185.35063434 65511.380441   11823.53036289 ...  2361.44475928\n",
      " 30352.38071143 21458.57943473]\n",
      "(1, 5) [[ -397.45043069 -2691.87403611   -47.82479287   373.26026629\n",
      "    610.13553651]]\n",
      "Training MSE Loss:  [ 1183.30184192 65354.6287243  11764.309327   ...  2334.09855301\n",
      " 30260.48295363 21417.83961911]\n",
      "(1, 5) [[ -396.78692782 -2679.60469496   -48.31344634   369.1995486\n",
      "    605.07891972]]\n",
      "Training MSE Loss:  [ 1181.27453619 65198.83289928 11705.49238076 ...  2307.04512276\n",
      " 30168.99545456 21377.417571  ]\n",
      "(1, 5) [[ -396.12611323 -2667.39806394   -48.79647645   365.16627136\n",
      "    600.05219785]]\n",
      "Training MSE Loss:  [ 1179.26853917 65043.98607235 11647.07633995 ...  2280.28143204\n",
      " 30077.91577723 21337.31082545]\n",
      "(1, 5) [[ -395.46796003 -2655.25380532   -49.27393993   361.16026742\n",
      "    595.05521144]]\n",
      "Training MSE Loss:  [ 1177.28367463 64890.08140697 11589.05804772 ...  2253.80447566\n",
      " 29987.24150331 21297.51693918]\n",
      "(1, 5) [[ -394.81244156 -2643.17158323   -49.74589291   357.1813706\n",
      "    590.0878018 ]]\n",
      "Training MSE Loss:  [ 1175.31976806 64737.11212314 11531.43437432 ...  2227.61127944\n",
      " 29896.9702329  21258.03349039]\n",
      "(1, 5) [[ -394.1595315  -2631.15106369   -50.21239086   353.22941569\n",
      "    585.14981102]]\n",
      "Training MSE Loss:  [ 1173.37664667 64585.07149685 11474.20221684 ...  2201.69889992\n",
      " 29807.09958439 21218.85807853]\n",
      "(1, 5) [[ -393.50920378 -2619.19191458   -50.67348866   349.3042384\n",
      "    580.24108198]]\n",
      "Training MSE Loss:  [ 1171.45413934 64433.95285956 11417.35849901 ...  2176.06442398\n",
      " 29717.62719427 21179.98832411]\n",
      "(1, 5) [[ -392.86143262 -2607.29380562   -51.12924057   345.4056754\n",
      "    575.3614583 ]]\n",
      "Training MSE Loss:  [ 1169.55207661 64283.74959771 11360.90017092 ...  2150.70496859\n",
      " 29628.55071699 21141.42186845]\n",
      "(1, 5) [[ -392.21619253 -2595.45640837   -51.57970026   341.53356429\n",
      "    570.51078442]]\n",
      "Training MSE Loss:  [ 1167.67029069 64134.45515218 11304.8242088  ...  2125.61768048\n",
      " 29539.86782474 21103.15637352]\n",
      "(1, 5) [[ -391.57345826 -2583.67939622   -52.0249208    337.68774362\n",
      "    565.68890551]]\n",
      "Training MSE Loss:  [ 1165.8086154  63986.06301778 11249.12761479 ...  2100.79973584\n",
      " 29451.57620734 21065.1895217 ]\n",
      "(1, 5) [[ -390.93320487 -2571.96244438   -52.46495468   333.86805285\n",
      "    560.89566751]]\n",
      "Training MSE Loss:  [ 1163.96688618 63838.56674279 11193.80741668 ...  2076.24833999\n",
      " 29363.67357205 21027.51901557]\n",
      "(1, 5) [[ -390.29540766 -2560.30522986   -52.89985379   330.07433238\n",
      "    556.13091712]]\n",
      "Training MSE Loss:  [ 1162.14494006 63691.95992841 11138.86066771 ...  2051.96072712\n",
      " 29276.15764338 20990.14257776]\n",
      "(1, 5) [[ -389.66004223 -2548.70743147   -53.32966948   326.30642351\n",
      "    551.39450182]]\n",
      "Training MSE Loss:  [ 1160.34261565 63546.2362283  11084.28444633 ...  2027.93415999\n",
      " 29189.026163   20953.05795068]\n",
      "(1, 5) [[ -389.0270844  -2537.16872981   -53.75445251   322.56416848\n",
      "    546.68626982]]\n",
      "Training MSE Loss:  [ 1158.55975309 63401.38934811 11030.07585596 ...  2004.16592961\n",
      " 29102.27688951 20916.26289637]\n",
      "(1, 5) [[ -388.39651028 -2525.68880724   -54.17425309   318.84741042\n",
      "    542.00607008]]\n",
      "Training MSE Loss:  [ 1156.79619408 63257.41304496 10976.23202478 ...  1980.65335497\n",
      " 29015.90759832 20879.75519629]\n",
      "(1, 5) [[ -387.76829622 -2514.2673479    -54.58912089   315.15599337\n",
      "    537.35375233]]\n",
      "Training MSE Loss:  [ 1155.05178183 63114.30112699 10922.75010551 ...  1957.39378275\n",
      " 28929.91608149 20843.53265113]\n",
      "(1, 5) [[ -387.14241886 -2502.90403768   -54.99910502   311.48976228\n",
      "    532.72916703]]\n",
      "Training MSE Loss:  [ 1153.32636106 62972.04745288 10869.62727516 ...  1934.38458705\n",
      " 28844.30014759 20807.59308063]\n",
      "(1, 5) [[ -386.51885504 -2491.59856421   -55.40425405   307.84856298\n",
      "    528.13216538]]\n",
      "Training MSE Loss:  [ 1151.61977797 62830.6459314  10816.86073486 ...  1911.62316907\n",
      " 28759.0576215  20771.93432336]\n",
      "(1, 5) [[ -385.89758189 -2480.35061686   -55.80461603   304.2322422\n",
      "    523.56259931]]\n",
      "Training MSE Loss:  [ 1149.93188024 62690.09052092 10764.4477096  ...  1889.10695687\n",
      " 28674.18634431 20736.55423655]\n",
      "(1, 5) [[ -385.27857677 -2469.15988672   -56.20023849   300.64064757\n",
      "    519.02032152]]\n",
      "Training MSE Loss:  [ 1148.26251698 62550.37522898 10712.38544801 ...  1866.83340509\n",
      " 28589.68417317 20701.45069595]\n",
      "(1, 5) [[ -384.66181729 -2458.0260666    -56.59116841   297.07362756\n",
      "    514.50518541]]\n",
      "Training MSE Loss:  [ 1146.61153874 62411.49411182 10660.67122221 ...  1844.79999465\n",
      " 28505.54898109 20666.62159555]\n",
      "(1, 5) [[ -384.0472813  -2446.94885101   -56.9774523    293.53103157\n",
      "    510.01704512]]\n",
      "Training MSE Loss:  [ 1144.97879752 62273.44127395 10609.30232753 ...  1823.0042325\n",
      " 28421.77865687 20632.0648475 ]\n",
      "(1, 5) [[ -383.4349469  -2435.92793614   -57.35913613   290.01270984\n",
      "    505.55575551]]\n",
      "Training MSE Loss:  [ 1143.36414668 62136.21086769 10558.27608233 ...  1801.44365138\n",
      " 28338.37110489 20597.77838185]\n",
      "(1, 5) [[ -382.82479239 -2424.9630199    -57.73626538   286.51851348\n",
      "    501.12117218]]\n",
      "Training MSE Loss:  [ 1141.76744099 61999.79709275 10507.5898278  ...  1780.11580948\n",
      " 28255.32424501 20563.76014646]\n",
      "(1, 5) [[ -382.21679636 -2414.05380185   -58.10888505   283.04829448\n",
      "    496.71315143]]\n",
      "Training MSE Loss:  [ 1140.1885366  61864.19419576 10457.24092772 ...  1759.01829026\n",
      " 28172.63601241 20530.00810672]\n",
      "(1, 5) [[ -381.61093759 -2403.19998321   -58.47703963   279.60190566\n",
      "    492.33155029]]\n",
      "Training MSE Loss:  [ 1138.627291   61729.39646991 10407.22676832 ...  1738.14870215\n",
      " 28090.30435746 20496.52024547]\n",
      "(1, 5) [[ -381.00719511 -2392.40126687   -58.84077313   276.17920072\n",
      "    487.9762265 ]]\n",
      "Training MSE Loss:  [ 1137.08356302 61595.39825445 10357.544758   ...  1717.50467828\n",
      " 28008.32724557 20463.29456277]\n",
      "(1, 5) [[ -380.40554816 -2381.65735737   -59.20012909   272.78003421\n",
      "    483.64703851]]\n",
      "Training MSE Loss:  [ 1135.55721283 61462.1939343  10308.19232721 ...  1697.08387626\n",
      " 27926.70265708 20430.32907579]\n",
      "(1, 5) [[ -379.80597622 -2370.96796087   -59.5551506    269.4042615\n",
      "    479.34384547]]\n",
      "Training MSE Loss:  [ 1134.04810189 61329.77793966 10259.16692818 ...  1676.88397792\n",
      " 27845.42858709 20397.62181856]\n",
      "(1, 5) [[ -379.208459   -2360.33278516   -59.90588023   266.05173883\n",
      "    475.06650726]]\n",
      "Training MSE Loss:  [ 1132.55609299 61198.14474555 10210.46603477 ...  1656.90268902\n",
      " 27764.50304535 20365.17084187]\n",
      "(1, 5) [[ -378.6129764  -2349.75153967   -60.25236015   262.72232326\n",
      "    470.81488442]]\n",
      "Training MSE Loss:  [ 1131.08105017 61067.28887144 10162.08714228 ...  1637.13773908\n",
      " 27683.92405612 20332.9742131 ]\n",
      "(1, 5) [[ -378.01950858 -2339.2239354    -60.59463204   259.41587268\n",
      "    466.58883824]]\n",
      "Training MSE Loss:  [ 1129.62283875 60937.20488082 10114.0277672  ...  1617.58688106\n",
      " 27603.68965806 20301.03001602]\n",
      "(1, 5) [[ -377.42803587 -2328.749685     -60.93273713   256.13224582\n",
      "    462.38823066]]\n",
      "Training MSE Loss:  [ 1128.1813253  60807.88738081 10066.28544709 ...  1598.24789118\n",
      " 27523.79790405 20269.33635068]\n",
      "(1, 5) [[ -376.83853884 -2318.32850265   -61.26671622   252.87130223\n",
      "    458.21292433]]\n",
      "Training MSE Loss:  [ 1126.75637763 60679.33102179 10018.85774037 ...  1579.11856862\n",
      " 27444.24686114 20237.89133322]\n",
      "(1, 5) [[ -376.25099828 -2307.96010416   -61.59660968   249.63290225\n",
      "    454.06278259]]\n",
      "Training MSE Loss:  [ 1125.3478648  60551.53049696  9971.74222609 ...  1560.19673535\n",
      " 27365.03461033 20206.6930957 ]\n",
      "(1, 5) [[ -375.66539517 -2297.64420689   -61.92245742   246.41690709\n",
      "    449.93766948]]\n",
      "Training MSE Loss:  [ 1123.95565703 60424.48054198  9924.93650381 ...  1541.48023583\n",
      " 27286.15924654 20175.739786  ]\n",
      "(1, 5) [[ -375.08171069 -2287.38052976   -62.24429894   243.22317872\n",
      "    445.83744969]]\n",
      "Training MSE Loss:  [ 1122.57962579 60298.17593459  9878.43819337 ...  1522.96693684\n",
      " 27207.6188784  20145.0295676 ]\n",
      "(1, 5) [[ -374.49992626 -2277.16879326   -62.56217331   240.05157993\n",
      "    441.76198863]]\n",
      "Training MSE Loss:  [ 1121.2196437  60172.61149422  9832.24493473 ...  1504.65472719\n",
      " 27129.41162821 20114.56061948]\n",
      "(1, 5) [[ -373.92002347 -2267.00871941   -62.8761192    236.90197432\n",
      "    437.71115235]]\n",
      "Training MSE Loss:  [ 1119.87558458 60047.78208162  9786.35438776 ...  1486.54151754\n",
      " 27051.53563174 20084.33113595]\n",
      "(1, 5) [[ -373.34198413 -2256.90003178   -63.18617484   233.77422629\n",
      "    433.6848076 ]]\n",
      "Training MSE Loss:  [ 1118.54732339 59923.68259847  9740.76423213 ...  1468.62524016\n",
      " 26973.98903818 20054.33932649]\n",
      "(1, 5) [[ -372.76579023 -2246.84245544   -63.49237806   230.66820101\n",
      "    429.68282178]]\n",
      "Training MSE Loss:  [ 1117.23473624 59800.30798704  9695.47216704 ...  1450.9038487\n",
      " 26896.77000996 20024.58341564]\n",
      "(1, 5) [[ -372.19142396 -2236.83571701   -63.7947663    227.58376445\n",
      "    425.70506299]]\n",
      "Training MSE Loss:  [ 1115.93770037 59677.65322982  9650.47591112 ...  1433.37531798\n",
      " 26819.87672268 19995.0616428 ]\n",
      "(1, 5) [[ -371.61886773 -2226.8795446    -64.0933766    224.52078339\n",
      "    421.75139997]]\n",
      "Training MSE Loss:  [ 1114.65609417 59555.71334912  9605.77320223 ...  1416.03764376\n",
      " 26743.30736498 19965.77226214]\n",
      "(1, 5) [[ -371.04810411 -2216.97366783   -64.38824559   221.47912534\n",
      "    417.82170213]]\n",
      "Training MSE Loss:  [ 1113.3897971  59434.48340676  9561.36179726 ...  1398.88884253\n",
      " 26667.06013841 19936.71354242]\n",
      "(1, 5) [[ -370.47911588 -2207.11781781   -64.67940952   218.45865862\n",
      "    413.91583953]]\n",
      "Training MSE Loss:  [ 1112.13868973 59313.95850369  9517.23947201 ...  1381.92695129\n",
      " 26591.13325733 19907.88376688]\n",
      "(1, 5) [[ -369.911886   -2197.31172713   -64.96690425   215.45925231\n",
      "    410.0336829 ]]\n",
      "Training MSE Loss:  [ 1110.90265373 59194.13377967  9473.404021   ...  1365.15002738\n",
      " 26515.52494879 19879.28123308]\n",
      "(1, 5) [[ -369.34639762 -2187.55512987   -65.25076528   212.48077627\n",
      "    406.17510362]]\n",
      "Training MSE Loss:  [ 1109.68157183 59075.00441287  9429.85325728 ...  1348.55614819\n",
      " 26440.23345245 19850.90425277]\n",
      "(1, 5) [[ -368.78263407 -2177.84776155   -65.53102771   209.5231011\n",
      "    402.33997371]]\n",
      "Training MSE Loss:  [ 1108.47532782 58956.56561956  9386.58501229 ...  1332.14341103\n",
      " 26365.25702042 19822.75115173]\n",
      "(1, 5) [[ -368.22057887 -2168.18935918   -65.80772629   206.58609818\n",
      "    398.52816587]]\n",
      "Training MSE Loss:  [ 1107.28380654 58838.81265379  9343.5971357  ...  1315.90993289\n",
      " 26290.59391717 19794.82026969]\n",
      "(1, 5) [[ -367.66021571 -2158.5796612    -66.08089539   203.66963963\n",
      "    394.7395534 ]]\n",
      "Training MSE Loss:  [ 1106.10689387 58721.74080699  9300.88749522 ...  1299.85385021\n",
      " 26216.24241946 19767.10996016]\n",
      "(1, 5) [[ -367.10152847 -2149.01840749   -66.35056901   200.77359833\n",
      "    390.97401027]]\n",
      "Training MSE Loss:  [ 1104.94447673 58605.34540772  9258.45397647 ...  1283.97331876\n",
      " 26142.20081616 19739.61859027]\n",
      "(1, 5) [[ -366.5445012  -2139.50533937   -66.61678081   197.8978479\n",
      "    387.23141109]]\n",
      "Training MSE Loss:  [ 1103.79644303 58489.62182126  9216.2944828  ...  1268.26651335\n",
      " 26068.46740822 19712.34454071]\n",
      "(1, 5) [[ -365.98911813 -2130.0401996    -66.8795641    195.04226271\n",
      "    383.51163109]]\n",
      "Training MSE Loss:  [ 1102.66268171 58374.56544933  9174.40693513 ...  1252.73162769\n",
      " 25995.04050852 19685.28620555]\n",
      "(1, 5) [[ -365.43536366 -2120.62273233   -67.13895181   192.20671787\n",
      "    379.81454615]]\n",
      "Training MSE Loss:  [ 1101.54308269 58260.17172976  9132.78927182 ...  1237.36687417\n",
      " 25921.91844175 19658.44199213]\n",
      "(1, 5) [[ -364.88322237 -2111.25268314   -67.39497655   189.3910892\n",
      "    376.14003277]]\n",
      "Training MSE Loss:  [ 1100.43753686 58146.43613615  9091.43944846 ...  1222.17048369\n",
      " 25849.09954439 19631.81032091]\n",
      "(1, 5) [[ -364.33267899 -2101.929799     -67.64767059   186.59525328\n",
      "    372.48796808]]\n",
      "Training MSE Loss:  [ 1099.34593612 58033.35417759  9050.3554378  ...  1207.14070544\n",
      " 25776.5821645  19605.3896254 ]\n",
      "(1, 5) [[ -363.78371843 -2092.65382828   -67.89706585   183.81908741\n",
      "    368.85822984]]\n",
      "Training MSE Loss:  [ 1098.2681733  57920.92139831  9009.5352295  ...  1192.27580673\n",
      " 25704.36466171 19579.17835197]\n",
      "(1, 5) [[ -363.23632577 -2083.42452074   -68.14319391   181.0624696\n",
      "    365.25069641]]\n",
      "Training MSE Loss:  [ 1097.20414219 57809.13337736  8968.97683005 ...  1177.5740728\n",
      " 25632.44540707 19553.17495977]\n",
      "(1, 5) [[ -362.69048626 -2074.24162749   -68.38608605   178.32527858\n",
      "    361.66524681]]\n",
      "Training MSE Loss:  [ 1096.15373753 57697.98572836  8928.6782626  ...  1163.03380664\n",
      " 25560.82278296 19527.37792061]\n",
      "(1, 5) [[ -362.1461853  -2065.10490105   -68.62577319   175.60739381\n",
      "    358.10176064]]\n",
      "Training MSE Loss:  [ 1095.11685497 57587.47409914  8888.6375668  ...  1148.65332878\n",
      " 25489.49518302 19501.78571881]\n",
      "(1, 5) [[ -361.60340844 -2056.01409527   -68.86228595   172.90869544\n",
      "    354.56011813]]\n",
      "Training MSE Loss:  [ 1094.09339111 57477.59417146  8848.85279867 ...  1134.43097714\n",
      " 25418.46101201 19476.39685111]\n",
      "(1, 5) [[ -361.06214142 -2046.96896536   -69.09565463   170.22906435\n",
      "    351.04020011]]\n",
      "Training MSE Loss:  [ 1093.08324344 57368.3416607   8809.32203045 ...  1120.36510685\n",
      " 25347.71868575 19451.20982655]\n",
      "(1, 5) [[ -360.52237012 -2037.9692679    -69.3259092    167.56838211\n",
      "    347.54188803]]\n",
      "Training MSE Loss:  [ 1092.08631036 57259.71231557  8770.04335043 ...  1106.45409004\n",
      " 25277.26663101 19426.22316633]\n",
      "(1, 5) [[ -359.98408058 -2029.01476078   -69.55307934   164.92653097\n",
      "    344.06506394]]\n",
      "Training MSE Loss:  [ 1091.10249116 57151.70191784  8731.01486284 ...  1092.69631569\n",
      " 25207.10328542 19401.43540374]\n",
      "(1, 5) [[ -359.44725899 -2020.10520324   -69.7771944    162.30339391\n",
      "    340.60961049]]\n",
      "Training MSE Loss:  [ 1090.13168602 57044.306282    8692.23468772 ...  1079.09018947\n",
      " 25137.22709739 19376.84508398]\n",
      "(1, 5) [[ -358.91189169 -2011.24035582   -69.99828344   159.69885457\n",
      "    337.17541093]]\n",
      "Training MSE Loss:  [ 1089.17379597 56937.52125503  8653.70096073 ...  1065.63413353\n",
      " 25067.63652599 19352.45076413]\n",
      "(1, 5) [[ -358.37796518 -2002.4199804    -70.21637523   157.11279731\n",
      "    333.76234912]]\n",
      "Training MSE Loss:  [ 1088.22872293 56831.34271605  8615.41183304 ...  1052.32658635\n",
      " 24998.33004087 19328.25101296]\n",
      "(1, 5) [[ -357.84546612 -1993.64384016   -70.43149822   154.54510714\n",
      "    330.37030948]]\n",
      "Training MSE Loss:  [ 1087.29636966 56725.7665761   8577.3654712  ...  1039.16600257\n",
      " 24929.3061222  19304.24441088]\n",
      "(1, 5) [[ -357.3143813  -1984.91169958   -70.64368058   151.99566977\n",
      "    326.99917706]]\n",
      "Training MSE Loss:  [ 1086.37663977 56620.78877782  8539.56005699 ...  1026.15085283\n",
      " 24860.56326054 19280.4295498 ]\n",
      "(1, 5) [[ -356.78469765 -1976.22332443   -70.85295019   149.46437158\n",
      "    323.64883748]]\n",
      "Training MSE Loss:  [ 1085.46943771 56516.40529521  8501.99378729 ...  1013.27962358\n",
      " 24792.09995678 19256.80503303]\n",
      "(1, 5) [[ -356.25640227 -1967.57848178   -71.05933465   146.95109962\n",
      "    320.31917694]]\n",
      "Training MSE Loss:  [ 1084.57466875 56412.6121333   8464.66487394 ...  1000.55081695\n",
      " 24723.91472204 19233.36947517]\n",
      "(1, 5) [[ -355.72948239 -1958.97693997   -71.26286125   144.45574162\n",
      "    317.01008224]]\n",
      "Training MSE Loss:  [ 1083.69223898 56309.40532795  8427.57154361 ...   987.96295056\n",
      " 24656.00607757 19210.12150201]\n",
      "(1, 5) [[ -355.20392538 -1950.4184686    -71.46355703   141.97818595\n",
      "    313.72144075]]\n",
      "Training MSE Loss:  [ 1082.8220553  56206.78094552  8390.71203769 ...   975.51455738\n",
      " 24588.37255472 19187.05975043]\n",
      "(1, 5) [[ -354.67971875 -1941.90283856   -71.66144874   139.51832167\n",
      "    310.45314041]]\n",
      "Training MSE Loss:  [ 1081.96402543 56104.73508263  8354.08461211 ...   963.20418555\n",
      " 24521.01269477 19164.18286828]\n",
      "(1, 5) [[ -354.15685016 -1933.42982197   -71.85656286   137.07603848\n",
      "    307.20506975]]\n",
      "Training MSE Loss:  [ 1081.11805785 56003.26386591  8317.68753727 ...   951.03039824\n",
      " 24453.92504893 19141.4895143 ]\n",
      "(1, 5) [[ -353.6353074  -1924.99919223   -72.04892559   134.65122674\n",
      "    303.97711787]]\n",
      "Training MSE Loss:  [ 1080.28406185 55902.36345171  8281.51909785 ...   938.9917735\n",
      " 24387.10817821 19118.97835801]\n",
      "(1, 5) [[ -353.11507841 -1916.61072395   -72.23856288   132.24377745\n",
      "    300.76917443]]\n",
      "Training MSE Loss:  [ 1079.4619475  55802.03002587  8245.57759274 ...   927.08690407\n",
      " 24320.56065335 19096.64807958]\n",
      "(1, 5) [[ -352.59615123 -1908.26419301   -72.42550039   129.85358227\n",
      "    297.58112966]]\n",
      "Training MSE Loss:  [ 1078.6516256  55702.25980345  8209.8613349  ...   915.31439729\n",
      " 24254.28105474 19074.49736979]\n",
      "(1, 5) [[ -352.07851406 -1899.9593765    -72.60976355   127.4805335\n",
      "    294.41287436]]\n",
      "Training MSE Loss:  [ 1077.85300776 55603.04902846  8174.36865119 ...   903.67287487\n",
      " 24188.26797232 19052.5249299 ]\n",
      "(1, 5) [[ -351.56215524 -1891.69605272   -72.79137751   125.12452407\n",
      "    291.2642999 ]]\n",
      "Training MSE Loss:  [ 1077.06600632 55504.39397365  8139.09788231 ...   892.16097282\n",
      " 24122.52000555 19030.72947153]\n",
      "(1, 5) [[ -351.04706323 -1883.47400122   -72.97036718   122.78544755\n",
      "    288.13529818]]\n",
      "Training MSE Loss:  [ 1076.29053435 55406.29094023  8104.04738265 ...   880.77734124\n",
      " 24057.03576328 19009.10971661]\n",
      "(1, 5) [[ -350.53322661 -1875.29300272   -73.14675719   120.46319814\n",
      "    285.02576168]]\n",
      "Training MSE Loss:  [ 1075.52650569 55308.73625764  8069.21552016 ...   869.52064422\n",
      " 23991.81386369 18987.66439726]\n",
      "(1, 5) [[ -350.02063409 -1867.15283918   -73.32057196   118.15767068\n",
      "    281.93558343]]\n",
      "Training MSE Loss:  [ 1074.77383488 55211.72628328  8034.60067625 ...   858.38955965\n",
      " 23926.85293424 18966.39225568]\n",
      "(1, 5) [[ -349.50927454 -1859.05329372   -73.49183562   115.86876063\n",
      "    278.86465701]]\n",
      "Training MSE Loss:  [ 1074.0324372  55115.2574023   8000.20124564 ...   847.38277912\n",
      " 23862.15161153 18945.29204411]\n",
      "(1, 5) [[ -348.9991369  -1850.99415068   -73.6605721    113.59636406\n",
      "    275.81287654]]\n",
      "Training MSE Loss:  [ 1073.30222863 55019.32602736  7966.0156363  ...   836.49900776\n",
      " 23797.70854129 18924.36252467]\n",
      "(1, 5) [[ -348.49021028 -1842.97519555   -73.82680506   111.34037766\n",
      "    272.78013671]]\n",
      "Training MSE Loss:  [ 1072.58312587 54923.92859837  7932.04226925 ...   825.7369641\n",
      " 23733.52237828 18903.60246934]\n",
      "(1, 5) [[ -347.98248388 -1834.99621501   -73.99055792   109.10069876\n",
      "    269.76633273]]\n",
      "Training MSE Loss:  [ 1071.8750463  54829.06158227  7898.27957852 ...   815.09537992\n",
      " 23669.59178621 18883.0106598 ]\n",
      "(1, 5) [[ -347.47594706 -1827.05699692   -74.15185389   106.87722526\n",
      "    266.77136035]]\n",
      "Training MSE Loss:  [ 1071.17790803 54734.7214728   7864.72601102 ...   804.57300012\n",
      " 23605.91543765 18862.5858874 ]\n",
      "(1, 5) [[ -346.97058927 -1819.15733028   -74.31071592   104.66985571\n",
      "    263.79511587]]\n",
      "Training MSE Loss:  [ 1070.49162981 54640.90479026  7831.38002638 ...   794.16858261\n",
      " 23542.49201401 18842.32695304]\n",
      "(1, 5) [[ -346.46640008 -1811.29700525   -74.46716674   102.47848922\n",
      "    260.83749613]]\n",
      "Training MSE Loss:  [ 1069.81613109 54547.60808129  7798.24009689 ...   783.88089814\n",
      " 23479.32020542 18822.23266707]\n",
      "(1, 5) [[ -345.96336918 -1803.47581316   -74.62122886   100.30302554\n",
      "    257.89839849]]\n",
      "Training MSE Loss:  [ 1069.15133201 54454.82791866  7765.30470736 ...   773.70873018\n",
      " 23416.39871068 18802.30184924]\n",
      "(1, 5) [[ -345.4614864  -1795.69354644   -74.77292455    98.143365\n",
      "    254.97772085]]\n",
      "Training MSE Loss:  [ 1068.49715333 54362.56090101  7732.57235503 ...   763.65087479\n",
      " 23353.72623718 18782.53332861]\n",
      "(1, 5) [[ -344.96074166 -1787.94999871   -74.92227587    95.99940852\n",
      "    252.07536162]]\n",
      "Training MSE Loss:  [ 1067.8535165  54270.80365266  7700.04154944 ...   753.70614049\n",
      " 23291.30150084 18762.92594344]\n",
      "(1, 5) [[ -344.46112499 -1780.24496466   -75.06930465    93.87105762\n",
      "    249.19121977]]\n",
      "Training MSE Loss:  [ 1067.22034361 54179.55282336  7667.71081234 ...   743.87334812\n",
      " 23229.12322602 18743.4785411 ]\n",
      "(1, 5) [[ -343.96262656 -1772.57824016   -75.21403252    91.7582144\n",
      "    246.32519477]]\n",
      "Training MSE Loss:  [ 1066.5975574  54088.80508813  7635.57867757 ...   734.15133073\n",
      " 23167.19014549 18724.18997803]\n",
      "(1, 5) [[ -343.46523662 -1764.94962214   -75.35648087    89.66078155\n",
      "    243.47718661]]\n",
      "Training MSE Loss:  [ 1065.98508124 53998.55714696  7603.64369095 ...   724.53893344\n",
      " 23105.50100033 18705.05911964]\n",
      "(1, 5) [[ -342.96894556 -1757.35890869   -75.49667089    87.57866233\n",
      "    240.64709579]]\n",
      "Training MSE Loss:  [ 1065.38283914 53908.80572469  7571.9044102  ...   715.03501332\n",
      " 23044.05453986 18686.0848402 ]\n",
      "(1, 5) [[ -342.47374386 -1749.80589898   -75.63462356    85.51176059\n",
      "    237.83482336]]\n",
      "Training MSE Loss:  [ 1064.79075572 53819.54757073  7540.3594048  ...   705.63843927\n",
      " 22982.8495216  18667.26602279]\n",
      "(1, 5) [[ -341.97962211 -1742.29039328   -75.77035966    83.45998075\n",
      "    235.04027085]]\n",
      "Training MSE Loss:  [ 1064.20875623 53730.77945887  7509.00725592 ...   696.3480919\n",
      " 22921.88471118 18648.60155923]\n",
      "(1, 5) [[ -341.48657103 -1734.81219294   -75.90389975    81.4232278\n",
      "    232.26334031]]\n",
      "Training MSE Loss:  [ 1063.63676653 53642.4981871   7477.84655629 ...   687.16286338\n",
      " 22861.15888229 18630.09034997]\n",
      "(1, 5) [[ -340.99458141 -1727.37110042   -76.0352642     79.4014073\n",
      "    229.50393431]]\n",
      "Training MSE Loss:  [ 1063.07471308 53554.70057736  7446.87591011 ...   678.08165736\n",
      " 22800.67081662 18611.73130403]\n",
      "(1, 5) [[ -340.50364416 -1719.96691925   -76.16447315    77.39442536\n",
      "    226.76195591]]\n",
      "Training MSE Loss:  [ 1062.52252295 53467.38347538  7416.09393297 ...   669.10338882\n",
      " 22740.41930375 18593.5233389 ]\n",
      "(1, 5) [[ -340.01375031 -1712.59945402   -76.29154659    75.40218868\n",
      "    224.03730868]]\n",
      "Training MSE Loss:  [ 1061.98012379 53380.54375044  7385.4992517  ...   660.22698399\n",
      " 22680.40314116 18575.46538052]\n",
      "(1, 5) [[ -339.52489098 -1705.26851041   -76.41650427    73.42460448\n",
      "    221.32989669]]\n",
      "Training MSE Loss:  [ 1061.44744385 53294.17829521  7355.09050433 ...   651.45138018\n",
      " 22620.62113411 18557.55636314]\n",
      "(1, 5) [[ -339.03705738 -1697.97389513   -76.53936576    71.46158057\n",
      "    218.63962451]]\n",
      "Training MSE Loss:  [ 1060.92441195 53208.28402552  7324.86633995 ...   642.77552571\n",
      " 22561.07209559 18539.79522929]\n",
      "(1, 5) [[ -338.55024083 -1690.71541598   -76.66015045    69.51302529\n",
      "    215.96639721]]\n",
      "Training MSE Loss:  [ 1060.4109575  53122.85788018  7294.82541863 ...   634.19837977\n",
      " 22501.75484629 18522.18092967]\n",
      "(1, 5) [[ -338.06443277 -1683.49288179   -76.77887753    67.57884754\n",
      "    213.31012035]]\n",
      "Training MSE Loss:  [ 1059.90701047 53037.89682077  7264.96641131 ...   625.71891234\n",
      " 22442.66821448 18504.71242311]\n",
      "(1, 5) [[ -337.57962471 -1676.30610243   -76.89556599    65.65895676\n",
      "    210.67069998]]\n",
      "Training MSE Loss:  [ 1059.41250139 52953.3978315   7235.28799973 ...   617.33610403\n",
      " 22383.81103602 18487.38867648]\n",
      "(1, 5) [[ -337.09580827 -1669.15488882   -77.01023466    63.75326293\n",
      "    208.04804264]]\n",
      "Training MSE Loss:  [ 1058.92736136 52869.35791893  7205.78887632 ...   609.048946\n",
      " 22325.18215423 18470.20866461]\n",
      "(1, 5) [[ -336.61297515 -1662.03905291   -77.12290216    61.86167658\n",
      "    205.44205536]]\n",
      "Training MSE Loss:  [ 1058.45152202 52785.77411188  7176.4677441  ...   600.85643987\n",
      " 22266.78041989 18453.17137026]\n",
      "(1, 5) [[ -336.13111718 -1654.95840767   -77.23358695    59.98410876\n",
      "    202.85264565]]\n",
      "Training MSE Loss:  [ 1057.98491558 52702.64346117  7147.32331661 ...   592.75759755\n",
      " 22208.60469114 18436.27578398]\n",
      "(1, 5) [[ -335.65022626 -1647.91276709   -77.34230729    58.12047106\n",
      "    200.27972151]]\n",
      "Training MSE Loss:  [ 1057.52747476 52619.96303947  7118.35431778 ...   584.75144121\n",
      " 22150.65383344 18419.52090412]\n",
      "(1, 5) [[ -335.17029438 -1640.90194618   -77.44908129    56.2706756\n",
      "    197.7231914 ]]\n",
      "Training MSE Loss:  [ 1057.07913285 52537.72994112  7089.55948187 ...   576.8370031\n",
      " 22092.92671951 18402.90573668]\n",
      "(1, 5) [[ -334.69131364 -1633.92576095   -77.55392685    54.43463503\n",
      "    195.18296428]]\n",
      "Training MSE Loss:  [ 1056.63982364 52455.94128191  7060.9375534  ...   569.0133255\n",
      " 22035.42222928 18386.42929532]\n",
      "(1, 5) [[ -334.21327621 -1626.98402842   -77.65686172    52.61226253\n",
      "    192.65894959]]\n",
      "Training MSE Loss:  [ 1056.20948148 52374.59419896  7032.487287   ...   561.2794606\n",
      " 21978.13924981 18370.09060124]\n",
      "(1, 5) [[ -333.73617437 -1620.07656662   -77.75790348    50.80347178\n",
      "    190.1510572 ]]\n",
      "Training MSE Loss:  [ 1055.7880412  52293.68585049  7004.20744737 ...   553.63447037\n",
      " 21921.07667527 18353.88868312]\n",
      "(1, 5) [[ -333.26000049 -1613.20319454   -77.85706951    49.008177\n",
      "    187.6591975 ]]\n",
      "Training MSE Loss:  [ 1055.37543818 52213.21341567  6976.09680916 ...   546.07742651\n",
      " 21864.23340684 18337.82257707]\n",
      "(1, 5) [[ -332.78474701 -1606.36373219   -77.95437707    47.22629291\n",
      "    185.18328133]]\n",
      "Training MSE Loss:  [ 1054.9716083  52133.17409446  6948.15415694 ...   538.6074103\n",
      " 21807.6083527  18321.89132657]\n",
      "(1, 5) [[ -332.31040648 -1599.55800054   -78.04984321    45.45773475\n",
      "    182.72321999]]\n",
      "Training MSE Loss:  [ 1054.57648794 52053.56510739  6920.37828503 ...   531.22351254\n",
      " 21751.20042794 18306.09398238]\n",
      "(1, 5) [[ -331.83697152 -1592.78582154   -78.14348483    43.70241826\n",
      "    180.27892524]]\n",
      "Training MSE Loss:  [ 1054.19001398 51974.38369541  6892.76799749 ...   523.92483341\n",
      " 21695.00855452 18290.42960247]\n",
      "(1, 5) [[ -331.36443484 -1586.04701812   -78.23531868    41.9602597\n",
      "    177.85030931]]\n",
      "Training MSE Loss:  [ 1053.81212381 51895.62711977  6865.32210798 ...   516.71048243\n",
      " 21639.03166123 18274.89725202]\n",
      "(1, 5) [[ -330.89278923 -1579.34141416   -78.32536134    40.23117581\n",
      "    175.43728491]]\n",
      "Training MSE Loss:  [ 1053.4427553  51817.29266175  6838.03943973 ...   509.57957829\n",
      " 21583.26868362 18259.49600326]\n",
      "(1, 5) [[ -330.42202759 -1572.66883451   -78.41362922    38.51508387\n",
      "    173.03976516]]\n",
      "Training MSE Loss:  [ 1053.0818468  51739.37762261  6810.91882541 ...   502.53124882\n",
      " 21527.71856395 18244.2249355 ]\n",
      "(1, 5) [[ -329.95214287 -1566.02910496   -78.50013859    36.81190162\n",
      "    170.65766367]]\n",
      "Training MSE Loss:  [ 1052.72933715 51661.87932331  6783.95910706 ...   495.56463087\n",
      " 21472.38025112 18229.083135  ]\n",
      "(1, 5) [[ -329.48312813 -1559.42205226   -78.58490555    35.1215473\n",
      "    168.29089449]]\n",
      "Training MSE Loss:  [ 1052.38516567 51584.79510445  6757.15913603 ...   488.67887019\n",
      " 21417.25270067 18214.06969496]\n",
      "(1, 5) [[ -329.01497649 -1552.84750411   -78.66794606    33.44393966\n",
      "    165.93937213]]\n",
      "Training MSE Loss:  [ 1052.04927214 51508.12232603  6730.51777288 ...   481.8731214\n",
      " 21362.33487467 18199.1837154 ]\n",
      "(1, 5) [[ -328.54768115 -1546.30528914   -78.74927592    31.77899792\n",
      "    163.60301154]]\n",
      "Training MSE Loss:  [ 1051.72159682 51431.85836733  6704.0338873  ...   475.14654784\n",
      " 21307.62574172 18184.42430318]\n",
      "(1, 5) [[ -328.08123542 -1539.7952369    -78.82891078    30.12664181\n",
      "    161.28172811]]\n",
      "Training MSE Loss:  [ 1051.40208043 51356.00062675  6677.70635804 ...   468.4983215\n",
      " 21253.12427685 18169.79057188]\n",
      "(1, 5) [[ -327.61563265 -1533.31717789   -78.90686613    28.4867915\n",
      "    158.97543769]]\n",
      "Training MSE Loss:  [ 1051.09066414 51280.54652163  6651.53407282 ...   461.92762293\n",
      " 21198.82946151 18155.28164173]\n",
      "(1, 5) [[ -327.1508663  -1526.87094351   -78.98315734    26.8593677\n",
      "    156.68405655]]\n",
      "Training MSE Loss:  [ 1050.78728958 51205.49348812  6625.51592827 ...   455.43364117\n",
      " 21144.74028352 18140.89663962]\n",
      "(1, 5) [[ -326.68692989 -1520.4563661    -79.05779961    25.24429154\n",
      "    154.40750142]]\n",
      "Training MSE Loss:  [ 1050.49189883 51130.83898099  6599.65082983 ...   449.01557364\n",
      " 21090.85573697 18126.63469899]\n",
      "(1, 5) [[ -326.22381701 -1514.0732789    -79.130808      23.64148466\n",
      "    152.14568946]]\n",
      "Training MSE Loss:  [ 1050.20443441 51056.58047353  6573.93769168 ...   442.67262603\n",
      " 21037.17482225 18112.49495978]\n",
      "(1, 5) [[ -325.76152135 -1507.72151605   -79.20219744    22.05086916\n",
      "    149.89853826]]\n",
      "Training MSE Loss:  [ 1049.9248393  50982.71545734  6548.3754367  ...   436.40401228\n",
      " 20983.69654594 18098.47656837]\n",
      "(1, 5) [[ -325.30003666 -1501.40091261   -79.2719827     20.47236762\n",
      "    147.66596585]]\n",
      "Training MSE Loss:  [ 1049.65305688 50909.24144223  6522.96299632 ...   430.20895443\n",
      " 20930.41992078 18084.57867756]\n",
      "(1, 5) [[ -324.83935675 -1495.11130451   -79.34017843    18.90590306\n",
      "    145.44789069]]\n",
      "Training MSE Loss:  [ 1049.38903101 50836.15595604  6497.69931054 ...   424.08668258\n",
      " 20877.34396566 18070.80044647]\n",
      "(1, 5) [[ -324.37947554 -1488.85252861   -79.40679912    17.351399\n",
      "    143.24423166]]\n",
      "Training MSE Loss:  [ 1049.13270593 50763.45654449  6472.58332777 ...   418.03643478\n",
      " 20824.46770549 18057.1410405 ]\n",
      "(1, 5) [[ -323.92038698 -1482.62442262   -79.47185914    15.8087794\n",
      "    141.05490808]]\n",
      "Training MSE Loss:  [ 1048.88402634 50691.14077105  6447.61400481 ...   412.05745697\n",
      " 20771.79017125 18043.59963129]\n",
      "(1, 5) [[ -323.46208513 -1476.42682515   -79.53537273    14.27796868\n",
      "    138.87983968]]\n",
      "Training MSE Loss:  [ 1048.64293734 50619.20621681  6422.79030675 ...   406.14900285\n",
      " 20719.31039987 18030.17539666]\n",
      "(1, 5) [[ -323.00456409 -1470.25957569   -79.59735397    12.75889171\n",
      "    136.71894662]]\n",
      "Training MSE Loss:  [ 1048.40938446 50547.65048028  6398.11120692 ...   400.31033388\n",
      " 20667.02743423 18016.86752054]\n",
      "(1, 5) [[ -322.54781805 -1464.12251459   -79.65781684    11.25147383\n",
      "    134.57214948]]\n",
      "Training MSE Loss:  [ 1048.18331363 50476.47117731  6373.57568682 ...   394.54071911\n",
      " 20614.94032309 18003.67519293]\n",
      "(1, 5) [[ -322.09184127 -1458.01548308   -79.71677517     9.75564083\n",
      "    132.43936927]]\n",
      "Training MSE Loss:  [ 1047.9646712  50405.6659409   6349.18273602 ...   388.83943519\n",
      " 20563.04812105 17990.59760985]\n",
      "(1, 5) [[ -321.63662807 -1451.93832325   -79.77424265     8.27131893\n",
      "    130.32052739]]\n",
      "Training MSE Loss:  [ 1047.75340392 50335.23242111  6324.93135211 ...   383.20576621\n",
      " 20511.34988852 17977.6339733 ]\n",
      "(1, 5) [[ -321.18217283 -1445.89087803   -79.83023288     6.7984348\n",
      "    128.21554568]]\n",
      "Training MSE Loss:  [ 1047.54945893 50265.16828486  6300.82054064 ...   377.63900368\n",
      " 20459.84469166 17964.78349118]\n",
      "(1, 5) [[ -320.72847003 -1439.87299123   -79.88475929     5.33691557\n",
      "    126.12434637]]\n",
      "Training MSE Loss:  [ 1047.35278378 50195.47121587  6276.84931504 ...   372.13844643\n",
      " 20408.53160234 17952.04537726]\n",
      "(1, 5) [[ -320.27551417 -1433.88450749   -79.93783521     3.8866888\n",
      "    124.04685212]]\n",
      "Training MSE Loss:  [ 1047.16332642 50126.13891443  6253.01669655 ...   366.70340051\n",
      " 20357.40969812 17939.41885112]\n",
      "(1, 5) [[ -319.82329985 -1427.9252723    -79.98947384     2.44768247\n",
      "    121.98298598]]\n",
      "Training MSE Loss:  [ 1046.98103516 50057.16909734  6229.32171417 ...   361.33317919\n",
      " 20306.47806215 17926.90313812]\n",
      "(1, 5) [[-3.19371822e+02 -1.42199513e+03 -8.00396883e+01  1.01982503e+00\n",
      "   1.19932671e+02]]\n",
      "Training MSE Loss:  [ 1046.80585874 49988.55949777  6205.76340455 ...   356.0271028\n",
      " 20255.73578319 17914.49746931]\n",
      "(1, 5) [[-3.18921075e+02 -1.41609393e+03 -8.00884914e+01 -3.96954659e-01\n",
      "   1.17895832e+02]]\n",
      "Training MSE Loss:  [ 1046.63774623 49920.30786508  6182.34081198 ...   350.78449871\n",
      " 20205.18195554 17902.20108143]\n",
      "(1, 5) [[ -318.47105303 -1410.22152548   -80.13589616    -1.80272731\n",
      "    115.8723929 ]]\n",
      "Training MSE Loss:  [ 1046.47664712 49852.41196474  6159.05298831 ...   345.60470122\n",
      " 20154.81567901 17890.01321683]\n",
      "(1, 5) [[ -318.02175208 -1404.37775608   -80.1819152     -3.19756319\n",
      "    113.86227788]]\n",
      "Training MSE Loss:  [ 1046.32251126 49784.86957815  6135.89899284 ...   340.48705155\n",
      " 20104.63605885 17877.93312344]\n",
      "(1, 5) [[ -317.57316659 -1398.56247517   -80.22656114    -4.58153217\n",
      "    111.86541233]]\n",
      "Training MSE Loss:  [ 1046.17528888 49717.67850258  6112.87789232 ...   335.43089768\n",
      " 20054.64220574 17865.9600547 ]\n",
      "(1, 5) [[ -317.12529153 -1392.77553319   -80.26984646    -5.95470366\n",
      "    109.88172169]]\n",
      "Training MSE Loss:  [ 1046.03493055 49650.83655097  6089.98876083 ...   330.43559436\n",
      " 20004.83323574 17854.09326955]\n",
      "(1, 5) [[ -316.67812193 -1387.01678142   -80.31178352    -7.31714667\n",
      "    107.91113182]]\n",
      "Training MSE Loss:  [ 1045.90138724 49584.34155184  6067.23067977 ...   325.50050299\n",
      " 19955.20827025 17842.33203233]\n",
      "(1, 5) [[ -316.23165289 -1381.28607191   -80.35238459    -8.66892981\n",
      "    105.95356898]]\n",
      "Training MSE Loss:  [ 1045.77461026 49518.19134918  6044.60273776 ...   320.62499158\n",
      " 19905.76643597 17830.67561279]\n",
      "(1, 5) [[ -315.78587956 -1375.58325754   -80.3916618    -10.01012125\n",
      "    104.00895981]]\n",
      "Training MSE Loss:  [ 1045.65455128 49452.38380229  6022.10403058 ...   315.80843467\n",
      " 19856.50686487 17819.12328602]\n",
      "(1, 5) [[ -315.34079716 -1369.90819198   -80.42962718   -11.34078874\n",
      "    102.07723134]]\n",
      "Training MSE Loss:  [ 1045.54116235 49386.91678567  5999.73366114 ...   311.05021324\n",
      " 19807.42869411 17807.67433239]\n",
      "(1, 5) [[ -314.89640094 -1364.2607297    -80.46629265   -12.66099965\n",
      "    100.15831097]]\n",
      "Training MSE Loss:  [ 1045.43439583 49321.78818892  5977.49073937 ...   306.34971469\n",
      " 19758.53106606 17796.32803754]\n",
      "(1, 5) [[ -314.45268626 -1358.64072595   -80.50167003   -13.97082092\n",
      "     98.25212651]]\n",
      "Training MSE Loss:  [ 1045.33420445 49256.99591657  5955.37438222 ...   301.70633274\n",
      " 19709.81312824 17785.0836923 ]\n",
      "(1, 5) [[ -314.00964848 -1353.04803676   -80.53577102   -15.27031909\n",
      "     96.35860613]]\n",
      "Training MSE Loss:  [ 1045.24054129 49192.53788801  5933.38371353 ...   297.11946738\n",
      " 19661.27403325 17773.94059267]\n",
      "(1, 5) [[ -313.56728306 -1347.48251896   -80.5686072    -16.5595603\n",
      "     94.47767841]]\n",
      "Training MSE Loss:  [ 1045.15335977 49128.41203737  5911.51786403 ...   292.58852478\n",
      " 19612.91293879 17762.89803977]\n",
      "(1, 5) [[ -313.12558548 -1341.94403012   -80.60019008   -17.83861028\n",
      "     92.60927227]]\n",
      "Training MSE Loss:  [ 1045.07261363 49064.61631334  5889.77597127 ...   288.11291726\n",
      " 19564.72900756 17751.95533979]\n",
      "(1, 5) [[ -312.68455132 -1336.43242863   -80.63053103   -19.10753436\n",
      "     90.75331704]]\n",
      "Training MSE Loss:  [ 1044.99825697 49001.14867914  5868.15717952 ...   283.69206321\n",
      " 19516.72140728 17741.11180398]\n",
      "(1, 5) [[ -312.24417617 -1330.94757361   -80.65964134   -20.36639749\n",
      "     88.9097424 ]]\n",
      "Training MSE Loss:  [ 1044.93024421 48938.00711233  5846.66063978 ...   279.32538701\n",
      " 19468.88931063 17730.36674854]\n",
      "(1, 5) [[ -311.80445571 -1325.48932496   -80.68753219   -21.61526422\n",
      "     87.07847842]]\n",
      "Training MSE Loss:  [ 1044.8685301  48875.18960474  5825.28550966 ...   275.01231902\n",
      " 19421.23189518 17719.71949466]\n",
      "(1, 5) [[ -311.36538564 -1320.05754332   -80.71421465   -22.85419871\n",
      "     85.25945554]]\n",
      "Training MSE Loss:  [ 1044.81306972 48812.69416236  5804.03095337 ...   270.75229545\n",
      " 19373.74834342 17709.16936843]\n",
      "(1, 5) [[ -310.92696174 -1314.65209012   -80.7396997    -24.0832647\n",
      "     83.45260455]]\n",
      "Training MSE Loss:  [ 1044.76381847 48750.51880518  5782.89614162 ...   266.54475835\n",
      " 19326.43784268 17698.71570079]\n",
      "(1, 5) [[ -310.48917984 -1309.2728275    -80.76399822   -25.3025256\n",
      "     81.65785664]]\n",
      "Training MSE Loss:  [ 1044.72073207 48688.66156713  5761.88025162 ...   262.38915553\n",
      " 19279.29958511 17688.35782752]\n",
      "(1, 5) [[ -310.0520358  -1303.91961838   -80.78712098   -26.51204438\n",
      "     79.87514333]]\n",
      "Training MSE Loss:  [ 1044.68376656 48627.12049594  5740.98246696 ...   258.2849405\n",
      " 19232.33276761 17678.09508921]\n",
      "(1, 5) [[ -309.61552557 -1298.5923264    -80.80907868   -27.71188367\n",
      "     78.10439653]]\n",
      "Training MSE Loss:  [ 1044.6528783  48565.89365306  5720.20197763 ...   254.23157241\n",
      " 19185.53659187 17667.92683116]\n",
      "(1, 5) [[ -309.1796451  -1293.29081594   -80.82988189   -28.90210569\n",
      "     76.34554849]]\n",
      "Training MSE Loss:  [ 1044.62802395 48504.9791135   5699.53797989 ...   250.22851601\n",
      " 19138.91026425 17657.85240341]\n",
      "(1, 5) [[ -308.74439044 -1288.01495214   -80.8495411    -30.08277229\n",
      "     74.59853185]]\n",
      "Training MSE Loss:  [ 1044.6091605  48444.37496578  5678.98967628 ...   246.27524156\n",
      " 19092.45299583 17647.87116066]\n",
      "(1, 5) [[ -308.30975766 -1282.76460082   -80.86806672   -31.25394496\n",
      "     72.86327957]]\n",
      "Training MSE Loss:  [ 1044.59624522 48384.07931178  5658.55627551 ...   242.3712248\n",
      " 19046.16400229 17637.98246224]\n",
      "(1, 5) [[ -307.87574289 -1277.53962858   -80.88546904   -32.4156848\n",
      "     71.139725  ]]\n",
      "Training MSE Loss:  [ 1044.58923569 48324.09026667  5638.23699246 ...   238.51594689\n",
      " 19000.04250394 17628.18567208]\n",
      "(1, 5) [[ -307.4423423  -1272.33990271   -80.90175828   -33.56805253\n",
      "     69.42780183]]\n",
      "Training MSE Loss:  [ 1044.58808981 48264.40595876  5618.03104809 ...   234.70889434\n",
      " 18954.08772568 17618.48015866]\n",
      "(1, 5) [[ -307.00955212 -1267.16529122   -80.91694455   -34.71110853\n",
      "     67.72744409]]\n",
      "Training MSE Loss:  [ 1044.59276576 48205.02452945  5597.93766942 ...   230.94955895\n",
      " 18908.29889691 17608.86529498]\n",
      "(1, 5) [[ -306.57736862 -1262.01566285   -80.93103787   -35.84491279\n",
      "     66.0385862 ]]\n",
      "Training MSE Loss:  [ 1044.60322203 48145.94413307  5577.95608945 ...   227.23743778\n",
      " 18862.67525159 17599.34045854]\n",
      "(1, 5) [[ -306.14578812 -1256.89088702   -80.9440482    -36.96952494\n",
      "     64.36116288]]\n",
      "Training MSE Loss:  [ 1044.61941737 48087.16293685  5558.08554712 ...   223.5720331\n",
      " 18817.21602813 17589.90503125]\n",
      "(1, 5) [[ -305.71480699 -1251.79083389   -80.95598538   -38.08500425\n",
      "     62.69510923]]\n",
      "Training MSE Loss:  [ 1044.64131087 48028.67912074  5538.32528727 ...   219.95285228\n",
      " 18771.92046938 17580.55839945]\n",
      "(1, 5) [[ -305.28442164 -1246.71537431   -80.96685917   -39.19140962\n",
      "     61.04036068]]\n",
      "Training MSE Loss:  [ 1044.66886188 47970.49087736  5518.67456057 ...   216.37940781\n",
      " 18726.78782263 17571.29995387]\n",
      "(1, 5) [[ -304.85462852 -1241.66437981   -80.97667924   -40.28879962\n",
      "     59.39685302]]\n",
      "Training MSE Loss:  [ 1044.70203002 47912.59641189  5499.13262348 ...   212.85121719\n",
      " 18681.81733952 17562.12908953]\n",
      "(1, 5) [[ -304.42542414 -1236.63772265   -80.98545519   -41.37723242\n",
      "     57.76452238]]\n",
      "Training MSE Loss:  [ 1044.74077522 47854.99394195  5479.69873823 ...   209.36780292\n",
      " 18637.00827607 17553.0452058 ]\n",
      "(1, 5) [[ -303.99680504 -1231.63527574   -80.99319652   -42.45676587\n",
      "     56.1433052 ]]\n",
      "Training MSE Loss:  [ 1044.78505769 47797.68169756  5460.3721727  ...   205.92869242\n",
      " 18592.3598926  17544.04770629]\n",
      "(1, 5) [[ -303.56876782 -1226.65691271   -80.99991265   -43.52745746\n",
      "     54.5331383 ]]\n",
      "Training MSE Loss:  [ 1044.8348379  47740.65792097  5441.15220047 ...   202.53341798\n",
      " 18547.87145373 17535.13599884]\n",
      "(1, 5) [[ -303.14130912 -1221.70250785   -81.00561292   -44.58936433\n",
      "     52.93395881]]\n",
      "Training MSE Loss:  [ 1044.8900766  47683.9208666   5422.03810067 ...   199.18151674\n",
      " 18503.54222833 17526.30949551]\n",
      "(1, 5) [[ -302.7144256  -1216.77193615   -81.01030657   -45.64254325\n",
      "     51.34570421]]\n",
      "Training MSE Loss:  [ 1044.95073482 47627.46880097  5403.029158   ...   195.8725306\n",
      " 18459.37148951 17517.5676125 ]\n",
      "(1, 5) [[ -302.288114   -1211.86507326   -81.01400278   -46.68705068\n",
      "     49.76831231]]\n",
      "Training MSE Loss:  [ 1045.01677386 47571.30000256  5384.12466268 ...   192.60600619\n",
      " 18415.35851457 17508.90977015]\n",
      "(1, 5) [[ -301.86237107 -1206.98179551   -81.01671065   -47.72294271\n",
      "     48.20172124]]\n",
      "Training MSE Loss:  [ 1045.08815529 47515.41276173  5365.32391038 ...   189.38149483\n",
      " 18371.50258497 17500.33539289]\n",
      "(1, 5) [[ -301.43719362 -1202.12197989   -81.01843918   -48.75027511\n",
      "     46.64586947]]\n",
      "Training MSE Loss:  [ 1045.16484093 47459.80538064  5346.62620216 ...   186.19855246\n",
      " 18327.80298633 17491.84390923]\n",
      "(1, 5) [[ -301.01257851 -1197.28550405   -81.0191973    -49.76910327\n",
      "     45.10069581]]\n",
      "Training MSE Loss:  [ 1045.24679288 47404.47617317  5328.03084449 ...   183.05673962\n",
      " 18284.25900836 17483.4347517 ]\n",
      "(1, 5) [[ -300.5885226  -1192.47224633   -81.01899387   -50.7794823\n",
      "     43.56613938]]\n",
      "Training MSE Loss:  [ 1045.33397351 47349.42346478  5309.53714912 ...   179.95562136\n",
      " 18240.86994485 17475.10735682]\n",
      "(1, 5) [[ -300.16502284 -1187.68208569   -81.01783766   -51.78146693\n",
      "     42.04213963]]\n",
      "Training MSE Loss:  [ 1045.42634542 47294.64559247  5291.14443311 ...   176.89476724\n",
      " 18197.63509367 17466.86116508]\n",
      "(1, 5) [[ -299.74207619 -1182.91490176   -81.01573737   -52.77511156\n",
      "     40.52863633]]\n",
      "Training MSE Loss:  [ 1045.52387149 47240.14090466  5272.85201873 ...   173.87375126\n",
      " 18154.55375669 17458.69562091]\n",
      "(1, 5) [[ -299.31967965 -1178.17057483   -81.01270162   -53.76047029\n",
      "     39.02556959]]\n",
      "Training MSE Loss:  [ 1045.62651484 47185.90776113  5254.65923345 ...   170.8921518\n",
      " 18111.62523976 17450.61017264]\n",
      "(1, 5) [[ -298.89783028 -1173.44898582   -81.00873897   -54.73759684\n",
      "     37.53287981]]\n",
      "Training MSE Loss:  [ 1045.73423886 47131.9445329   5236.5654099  ...   167.94955162\n",
      " 18068.84885273 17442.60427248]\n",
      "(1, 5) [[ -298.47652516 -1168.75001629   -81.00385787   -55.70654466\n",
      "     36.05050773]]\n",
      "Training MSE Loss:  [ 1045.84700717 47078.24960216  5218.56988579 ...   165.04553775\n",
      " 18026.22390937 17434.67737646]\n",
      "(1, 5) [[ -298.05576141 -1164.07354846   -80.99806674   -56.66736682\n",
      "     34.5783944 ]]\n",
      "Training MSE Loss:  [ 1045.96478366 47024.82136218  5200.67200389 ...   162.17970152\n",
      " 17983.74972736 17426.82894444]\n",
      "(1, 5) [[ -297.63553619 -1159.41946517   -80.99137388   -57.62011609\n",
      "     33.1164812 ]]\n",
      "Training MSE Loss:  [ 1046.08753245 46971.65821722  5182.87111202 ...   159.35163845\n",
      " 17941.42562828 17419.05844003]\n",
      "(1, 5) [[ -297.21584671 -1154.78764989   -80.98378757   -58.56484494\n",
      "     31.6647098 ]]\n",
      "Training MSE Loss:  [ 1046.2152179  46918.75858248  5165.16656293 ...   156.56094824\n",
      " 17899.25093754 17411.36533063]\n",
      "(1, 5) [[ -296.79669019 -1150.17798673   -80.97531597   -59.50160547\n",
      "     30.22302219]]\n",
      "Training MSE Loss:  [ 1046.34780463 46866.12088394  5147.55771434 ...   153.80723473\n",
      " 17857.22498441 17403.74908733]\n",
      "(1, 5) [[ -296.37806392 -1145.59036041   -80.9659672    -60.43044951\n",
      "     28.79136068]]\n",
      "Training MSE Loss:  [ 1046.48525749 46813.74355837  5130.04392886 ...   151.09010584\n",
      " 17815.34710195 17396.20918491]\n",
      "(1, 5) [[ -295.95996519 -1141.02465629   -80.95574929   -61.35142853\n",
      "     27.36966789]]\n",
      "Training MSE Loss:  [ 1046.62754156 46761.62505317  5112.62457392 ...   148.40917354\n",
      " 17773.61662698 17388.74510183]\n",
      "(1, 5) [[ -295.54239135 -1136.48076033   -80.94467022   -62.26459373\n",
      "     25.95788673]]\n",
      "Training MSE Loss:  [ 1046.77462217 46709.76382633  5095.29902181 ...   145.76405379\n",
      " 17732.03290011 17381.35632015]\n",
      "(1, 5) [[ -295.12533979 -1131.95855912   -80.93273789   -63.16999597\n",
      "     24.55596043]]\n",
      "Training MSE Loss:  [ 1046.92646488 46658.15834633  5078.06664955 ...   143.15436653\n",
      " 17690.59526563 17374.04232557]\n",
      "(1, 5) [[ -294.70880791 -1127.45793985   -80.91996012   -64.06768579\n",
      "     23.16383252]]\n",
      "Training MSE Loss:  [ 1047.08303547 46606.80709208  5060.92683891 ...   140.57973561\n",
      " 17649.30307156 17366.80260733]\n",
      "(1, 5) [[ -294.29279316 -1122.97879033   -80.90634469   -64.95771345\n",
      "     21.78144684]]\n",
      "Training MSE Loss:  [ 1047.24429996 46555.70855282  5043.87897637 ...   138.03978877\n",
      " 17608.15566958 17359.63665823]\n",
      "(1, 5) [[ -293.87729303 -1118.52099895   -80.89189929   -65.84012888\n",
      "     20.40874751]]\n",
      "Training MSE Loss:  [ 1047.41022461 46504.86122805  5026.92245302 ...   135.5341576\n",
      " 17567.15241502 17352.5439746 ]\n",
      "(1, 5) [[ -293.46230504 -1114.08445475   -80.87663155   -66.71498171\n",
      "     19.04567898]]\n",
      "Training MSE Loss:  [ 1047.58077588 46454.26362744  5010.05666461 ...   133.06247745\n",
      " 17526.29266683 17345.52405623]\n",
      "(1, 5) [[ -293.04782673 -1109.66904731   -80.86054904   -67.58232128\n",
      "     17.69218598]]\n",
      "Training MSE Loss:  [ 1047.75592047 46403.91427077  4993.28101144 ...   130.62438748\n",
      " 17485.57578756 17338.5764064 ]\n",
      "(1, 5) [[ -292.63385569 -1105.27466684   -80.84365926   -68.4421966\n",
      "     16.34821353]]\n",
      "Training MSE Loss:  [ 1047.9356253  46353.81168785  4976.59489836 ...   128.21953055\n",
      " 17445.00114335 17331.70053181]\n",
      "(1, 5) [[ -292.22038953 -1100.90120414   -80.82596965   -69.29465641\n",
      "     15.01370696]]\n",
      "Training MSE Loss:  [ 1048.11985752 46303.95441843  4959.99773472 ...   125.84755321\n",
      " 17404.56810384 17324.89594257]\n",
      "(1, 5) [[ -291.80742591 -1096.54855058   -80.80748757   -70.13974914\n",
      "     13.68861189]]\n",
      "Training MSE Loss:  [ 1048.30858447 46254.34101213  4943.48893431 ...   123.50810564\n",
      " 17364.27604226 17318.16215218]\n",
      "(1, 5) [[ -291.3949625  -1092.21659814   -80.78822034   -70.97752292\n",
      "     12.37287422]]\n",
      "Training MSE Loss:  [ 1048.50177374 46204.97002838  4927.06791539 ...   121.20084165\n",
      " 17324.12433527 17311.49867749]\n",
      "(1, 5) [[ -290.98299701 -1087.90523936   -80.76817521   -71.80802558\n",
      "     11.06644016]]\n",
      "Training MSE Loss:  [ 1048.69939312 46155.84003631  4910.73410057 ...   118.92541861\n",
      " 17284.11236307 17304.90503868]\n",
      "(1, 5) [[ -290.57152719 -1083.61436737   -80.74735936   -72.63130467\n",
      "      9.76925621]]\n",
      "Training MSE Loss:  [ 1048.9014106  46106.94961472  4894.48691684 ...   116.68149744\n",
      " 17244.23950926 17298.38075921]\n",
      "(1, 5) [[ -290.1605508  -1079.34387587   -80.72577991   -73.44740745\n",
      "      8.48126912]]\n",
      "Training MSE Loss:  [ 1049.10779441 46058.29735199  4878.32579549 ...   114.46874254\n",
      " 17204.5051609  17291.92536586]\n",
      "(1, 5) [[ -289.75006567 -1075.09365915   -80.70344393   -74.25638088\n",
      "      7.20242598]]\n",
      "Training MSE Loss:  [ 1049.31851297 46009.88184597  4862.25017211 ...   112.28682178\n",
      " 17164.90870844 17285.53838863]\n",
      "(1, 5) [[ -289.34006961 -1070.86361203   -80.68035842   -75.05827163\n",
      "      5.93267412]]\n",
      "Training MSE Loss:  [ 1049.53353491 45961.70170397  4846.25948651 ...   110.13540644\n",
      " 17125.44954572 17279.21936075]\n",
      "(1, 5) [[ -288.9305605  -1066.65362994   -80.65653033   -75.85312609\n",
      "      4.67196117]]\n",
      "Training MSE Loss:  [ 1049.75282907 45913.75554265  4830.35318274 ...   108.01417123\n",
      " 17086.12706993 17272.96781866]\n",
      "(1, 5) [[ -288.52153622 -1062.46360885   -80.63196653   -76.64099038\n",
      "      3.42023506]]\n",
      "Training MSE Loss:  [ 1049.9763645  45866.04198796  4814.530709   ...   105.92279417\n",
      " 17046.94068161 17266.78330196]\n",
      "(1, 5) [[ -288.11299471 -1058.29344528   -80.60667386   -77.42191031\n",
      "      2.17744398]]\n",
      "Training MSE Loss:  [ 1050.20411044 45818.55967507  4798.79151766 ...   103.86095664\n",
      " 17007.8897846  17260.66535344]\n",
      "(1, 5) [[-2.87704934e+02 -1.05414304e+03 -8.05806591e+01 -7.81959314e+01\n",
      "   9.43536385e-01]]\n",
      "Training MSE Loss:  [ 1050.43603634 45771.30724829  4783.13506517 ...   101.82834327\n",
      " 16968.97378603 17254.61351896]\n",
      "(1, 5) [[-2.87297352e+02 -1.05001228e+03 -8.05539289e+01 -7.89630990e+01\n",
      "  -2.81538961e-01]]\n",
      "Training MSE Loss:  [ 1050.67211186 45724.28336102  4767.56081208 ...    99.82464198\n",
      " 16930.1920963  17248.62734753]\n",
      "(1, 5) [[ -286.89024639 -1045.90107344   -80.52648999   -79.72345802\n",
      "     -1.49783304]]\n",
      "Training MSE Loss:  [ 1050.91230682 45677.48667566  4752.06822295 ...    97.84954387\n",
      " 16891.54412907 17242.70639122]\n",
      "(1, 5) [[ -286.48361572 -1041.80931641   -80.49834893   -80.47705318\n",
      "     -2.70539654]]\n",
      "Training MSE Loss:  [ 1051.15659129 45630.91586357  4736.65676637 ...    95.90274327\n",
      " 16853.0293012  17236.85020515]\n",
      "(1, 5) [[ -286.07745785 -1037.73690788   -80.46951227   -81.22392892\n",
      "     -3.9042799 ]]\n",
      "Training MSE Loss:  [ 1051.40493549 45584.56960496  4721.3259149  ...    93.98393763\n",
      " 16814.64703278 17231.05834748]\n",
      "(1, 5) [[ -285.67177088 -1033.68374766   -80.4399865    -81.96412941\n",
      "     -5.09453327]]\n",
      "Training MSE Loss:  [ 1051.65730985 45538.44658889  4706.07514504 ...    92.09282752\n",
      " 16776.39674706 17225.33037939]\n",
      "(1, 5) [[ -285.26655291 -1029.64973612   -80.40977804   -82.69769854\n",
      "     -6.27620653]]\n",
      "Training MSE Loss:  [ 1051.91368501 45492.54551311  4690.9039372  ...    90.22911663\n",
      " 16738.27787045 17219.66586501]\n",
      "(1, 5) [[ -284.8618021  -1025.63477416   -80.37889328   -83.42467993\n",
      "     -7.44934931]]\n",
      "Training MSE Loss:  [ 1052.17403177 45446.8650841   4675.81177566 ...    88.39251167\n",
      " 16700.28983251 17214.06437147]\n",
      "(1, 5) [[ -284.45751661 -1021.63876322   -80.34733854   -84.14511693\n",
      "     -8.61401094]]\n",
      "Training MSE Loss:  [ 1052.43832113 45401.40401691  4660.79814856 ...    86.5827224\n",
      " 16662.43206592 17208.52546881]\n",
      "(1, 5) [[ -284.05369464 -1017.66160527   -80.3151201    -84.85905265\n",
      "     -9.77024052]]\n",
      "Training MSE Loss:  [ 1052.70652428 45356.16103517  4645.86254783 ...    84.79946156\n",
      " 16624.70400643 17203.04873002]\n",
      "(1, 5) [[ -283.65033442 -1013.70320279   -80.28224416   -85.56652989\n",
      "    -10.91808685]]\n",
      "Training MSE Loss:  [ 1052.97861261 45311.13487097  4631.0044692  ...    83.04244485\n",
      " 16587.1050929  17197.63373095]\n",
      "(1, 5) [[ -283.24743419 -1009.76345881   -80.24871691   -86.26759122\n",
      "    -12.05759849]]\n",
      "Training MSE Loss:  [ 1053.25455765 45266.32426483  4616.22341216 ...    81.31139092\n",
      " 16549.63476722 17192.28005035]\n",
      "(1, 5) [[ -282.84499222 -1005.84227686   -80.21454446   -86.96227894\n",
      "    -13.18882374]]\n",
      "Training MSE Loss:  [ 1053.53433117 45221.72796564  4601.51887989 ...    79.60602131\n",
      " 16512.29247433 17186.98726979]\n",
      "(1, 5) [[ -282.44300682 -1001.93956099   -80.17973286   -87.65063509\n",
      "    -14.31181062]]\n",
      "Training MSE Loss:  [ 1053.81790508 45177.34473057  4586.89037928 ...    77.92606043\n",
      " 16475.07766217 17181.75497371]\n",
      "(1, 5) [[-282.0414763  -998.05521578  -80.14428814  -88.33270146  -15.42660691]]\n",
      "Training MSE Loss:  [ 1054.10525149 45133.17332501  4572.3374209  ...    76.27123554\n",
      " 16437.98978169 17176.58274932]\n",
      "(1, 5) [[-281.640399   -994.18914631  -80.10821626  -89.00851956  -16.53326012]]\n",
      "Training MSE Loss:  [ 1054.39634267 45089.21252256  4557.8595189  ...    74.6412767\n",
      " 16401.0282868  17171.47018663]\n",
      "(1, 5) [[-281.23977331 -990.34125818  -80.07152312  -89.67813067  -17.6318175 ]]\n",
      "Training MSE Loss:  [ 1054.69115109 45045.46110491  4543.45619107 ...    73.03591677\n",
      " 16364.19263438 17166.41687843]\n",
      "(1, 5) [[-280.83959761 -986.51145748  -80.03421461  -90.3415758   -18.72232605]]\n",
      "Training MSE Loss:  [ 1054.98964937 45001.91786178  4529.12695876 ...    71.45489136\n",
      " 16327.48228423 17161.42242023]\n",
      "(1, 5) [[-280.43987032 -982.69965082  -79.99629653  -90.99889572  -19.80483253]]\n",
      "Training MSE Loss:  [ 1055.29181034 44958.58159093  4514.87134686 ...    69.8979388\n",
      " 16290.89669907 17156.48641028]\n",
      "(1, 5) [[-280.04058988 -978.9057453   -79.95777465  -91.65013093  -20.87938343]]\n",
      "Training MSE Loss:  [ 1055.59760697 44915.451098    4500.68888377 ...    68.36480013\n",
      " 16254.43534452 17151.60844951]\n",
      "(1, 5) [[-279.64175476 -975.12964854  -79.91865469  -92.29532171  -21.94602499]]\n",
      "Training MSE Loss:  [ 1055.90701241 44872.52519655  4486.57910138 ...    66.85521905\n",
      " 16218.09768907 17146.78814157]\n",
      "(1, 5) [[-279.24336343 -971.37126864  -79.87894232  -92.93450807  -23.00480321]]\n",
      "Training MSE Loss:  [ 1056.22       44829.80270791  4472.54153503 ...    65.36894191\n",
      " 16181.88320409 17142.02509275]\n",
      "(1, 5) [[-278.84541441 -967.63051418  -79.83864317  -93.56772977  -24.05576382]]\n",
      "Training MSE Loss:  [ 1056.53654321 44787.2824612   4458.5757235  ...    63.90571767\n",
      " 16145.79136376 17137.31891196]\n",
      "(1, 5) [[-278.44790623 -963.90729425  -79.79776282  -94.19502635  -25.09895233]]\n",
      "Training MSE Loss:  [ 1056.85661572 44744.96329323  4444.68120895 ...    62.4652979\n",
      " 16109.82164509 17132.66921078]\n",
      "(1, 5) [[-278.05083743 -960.20151843  -79.75630681  -94.81643708  -26.13441398]]\n",
      "Training MSE Loss:  [ 1057.18019135 44702.84404844  4430.85753694 ...    61.0474367\n",
      " 16073.9735279  17128.07560336]\n",
      "(1, 5) [[-277.6542066  -956.51309677  -79.71428061  -95.43200101  -27.16219379]]\n",
      "Training MSE Loss:  [ 1057.5072441  44660.92357888  4417.10425634 ...    59.65189074\n",
      " 16038.2464948  17123.53770643]\n",
      "(1, 5) [[-277.25801232 -952.84193982  -79.67168967  -96.04175694  -28.1823365 ]]\n",
      "Training MSE Loss:  [ 1057.83774811 44619.20074412  4403.42091936 ...    58.27841916\n",
      " 16002.64003113 17119.0551393 ]\n",
      "(1, 5) [[-276.86225321 -949.18795859  -79.62853939  -96.64574342  -29.19488665]]\n",
      "Training MSE Loss:  [ 1058.17167771 44577.67441118  4389.8070815  ...    56.92678362\n",
      " 15967.15362503 17114.62752382]\n",
      "(1, 5) [[-276.46692791 -945.55106459  -79.58483512  -97.24399878  -30.19988851]]\n",
      "Training MSE Loss:  [ 1058.50900738 44536.34345454  4376.26230152 ...    55.59674823\n",
      " 15931.78676731 17110.25448436]\n",
      "(1, 5) [[-276.07203507 -941.93116978  -79.54058216  -97.83656111  -31.19738611]]\n",
      "Training MSE Loss:  [ 1058.84971176 44495.20675602  4362.78614142 ...    54.28807952\n",
      " 15896.53895155 17105.9356478 ]\n",
      "(1, 5) [[-275.67757336 -938.32818662  -79.4957858   -98.42346825  -32.18742326]]\n",
      "Training MSE Loss:  [ 1059.19376565 44454.26320476  4349.37816641 ...    53.00054644\n",
      " 15861.40967398 17101.67064352]\n",
      "(1, 5) [[-275.2835415  -934.74202801  -79.45045124  -99.00475783  -33.17004351]]\n",
      "Training MSE Loss:  [ 1059.54114401 44413.51169716  4336.03794487 ...    51.73392032\n",
      " 15826.39843353 17097.45910334]\n",
      "(1, 5) [[-274.88993818 -931.17260735  -79.40458366  -99.58046724  -34.1452902 ]]\n",
      "Training MSE Loss:  [ 1059.89182194 44372.9511368   4322.76504837 ...    50.48797484\n",
      " 15791.50473177 17093.30066155]\n",
      "(1, 5) [[-274.49676216 -927.61983846  -79.35818821 -100.15063363  -35.11320642]]\n",
      "Training MSE Loss:  [ 1060.24577473 44332.58043445  4309.5590516  ...    49.26248604\n",
      " 15756.72807293 17089.19495488]\n",
      "(1, 5) [[-274.10401217 -924.08363567  -79.31126998 -100.71529393  -36.07383501]]\n",
      "Training MSE Loss:  [ 1060.60297778 44292.39850795  4296.41953234 ...    48.05723224\n",
      " 15722.06796386 17085.14162246]\n",
      "(1, 5) [[-273.71168701 -920.56391373  -79.26383402 -101.27448485  -37.02721861]]\n",
      "Training MSE Loss:  [ 1060.96340668 44252.4042822   4283.34607148 ...    46.87199407\n",
      " 15687.52391401 17081.14030582]\n",
      "(1, 5) [[-273.31978545 -917.06058788  -79.21588534 -101.82824284  -37.97339961]]\n",
      "Training MSE Loss:  [ 1061.32703716 44212.59668908  4270.33825297 ...    45.7065544\n",
      " 15653.09543541 17077.19064887]\n",
      "(1, 5) [[-272.92830632 -913.57357379  -79.16742891 -102.37660417  -38.91242017]]\n",
      "Training MSE Loss:  [ 1061.6938451  44172.97466744  4257.39566376 ...    44.56069837\n",
      " 15618.7820427  17073.29229789]\n",
      "(1, 5) [[-272.53724843 -910.10278759  -79.11846967 -102.91960486  -39.84432222]]\n",
      "Training MSE Loss:  [1.06206381e+03 4.41335372e+04 4.24451789e+03 ... 4.34342133e+01\n",
      " 1.55845833e+04 1.70694449e+04]\n",
      "(1, 5) [[-272.14661065 -906.64814587  -79.0690125  -103.45728071  -40.76914748]]\n",
      "Training MSE Loss:  [1.06243690e+03 4.40942831e+04 4.23170454e+03 ... 4.23268887e+01\n",
      " 1.55504986e+04 1.70656481e+04]\n",
      "(1, 5) [[-271.75639183 -903.20956563  -79.01906226 -103.98966731  -41.68693741]]\n",
      "Training MSE Loss:  [1.06281309e+03 4.40552115e+04 4.21895519e+03 ... 4.12385164e+01\n",
      " 1.55165276e+04 1.70619016e+04]\n",
      "(1, 5) [[-271.36659087 -899.78696437  -78.96862375 -104.51680001  -42.59773328]]\n",
      "Training MSE Loss:  [1.06319237e+03 4.40163213e+04 4.20626944e+03 ... 4.01688901e+01\n",
      " 1.54826697e+04 1.70582050e+04]\n",
      "(1, 5) [[-270.97720665 -896.38025999  -78.91770175 -105.03871395  -43.50157611]]\n",
      "Training MSE Loss:  [1.06357471e+03 4.39776115e+04 4.19364691e+03 ... 3.91178059e+01\n",
      " 1.54489246e+04 1.70545579e+04]\n",
      "(1, 5) [[-270.5882381  -892.98937084  -78.86630099 -105.55544408  -44.39850672]]\n",
      "Training MSE Loss:  [1.06396009e+03 4.39390810e+04 4.18108719e+03 ... 3.80850617e+01\n",
      " 1.54152916e+04 1.70509601e+04]\n",
      "(1, 5) [[-270.19968416 -889.61421571  -78.81442617 -106.0670251   -45.28856567]]\n",
      "Training MSE Loss:  [1.06434847e+03 4.39007288e+04 4.16858989e+03 ... 3.70704579e+01\n",
      " 1.53817705e+04 1.70474112e+04]\n",
      "(1, 5) [[-269.81154377 -886.25471384  -78.76208194 -106.5734915   -46.17179335]]\n",
      "Training MSE Loss:  [1.06473985e+03 4.38625540e+04 4.15615462e+03 ... 3.60737966e+01\n",
      " 1.53483606e+04 1.70439108e+04]\n",
      "(1, 5) [[-269.42381591 -882.91078486  -78.70927292 -107.07487757  -47.0482299 ]]\n",
      "Training MSE Loss:  [1.06513420e+03 4.38245555e+04 4.14378100e+03 ... 3.50948821e+01\n",
      " 1.53150615e+04 1.70404587e+04]\n",
      "(1, 5) [[-269.03649956 -879.58234888  -78.6560037  -107.57121738  -47.91791524]]\n",
      "Training MSE Loss:  [1.06553148e+03 4.37867323e+04 4.13146864e+03 ... 3.41335207e+01\n",
      " 1.52818729e+04 1.70370544e+04]\n",
      "(1, 5) [[-268.64959372 -876.26932641  -78.60227881 -108.0625448   -48.78088907]]\n",
      "Training MSE Loss:  [1.06593169e+03 4.37490835e+04 4.11921716e+03 ... 3.31895207e+01\n",
      " 1.52487941e+04 1.70336978e+04]\n",
      "(1, 5) [[-268.26309741 -872.97163838  -78.54810277 -108.54889347  -49.6371909 ]]\n",
      "Training MSE Loss:  [1.06633480e+03 4.37116080e+04 4.10702617e+03 ... 3.22626923e+01\n",
      " 1.52158248e+04 1.70303883e+04]\n",
      "(1, 5) [[-267.87700966 -869.68920616  -78.49348005 -109.03029685  -50.48686   ]]\n",
      "Training MSE Loss:  [1.06674079e+03 4.36743049e+04 4.09489532e+03 ... 3.13528477e+01\n",
      " 1.51829645e+04 1.70271258e+04]\n",
      "(1, 5) [[-267.49132953 -866.42195154  -78.43841508 -109.50678816  -51.32993544]]\n",
      "Training MSE Loss:  [1.06714963e+03 4.36371732e+04 4.08282421e+03 ... 3.04598012e+01\n",
      " 1.51502127e+04 1.70239099e+04]\n",
      "(1, 5) [[-267.10605607 -863.16979672  -78.38291227 -109.97840044  -52.16645606]]\n",
      "Training MSE Loss:  [1.06756130e+03 4.36002120e+04 4.07081249e+03 ... 2.95833689e+01\n",
      " 1.51175691e+04 1.70207402e+04]\n",
      "(1, 5) [[-266.72118837 -859.93266431  -78.32697597 -110.44516652  -52.9964605 ]]\n",
      "Training MSE Loss:  [1.06797579e+03 4.35634203e+04 4.05885978e+03 ... 2.87233686e+01\n",
      " 1.50850331e+04 1.70176165e+04]\n",
      "(1, 5) [[-266.33672551 -856.71047735  -78.27061053 -110.90711902  -53.8199872 ]]\n",
      "Training MSE Loss:  [1.06839307e+03 4.35267971e+04 4.04696572e+03 ... 2.78796202e+01\n",
      " 1.50526043e+04 1.70145385e+04]\n",
      "(1, 5) [[-265.95266662 -853.50315928  -78.21382022 -111.36429036  -54.63707438]]\n",
      "Training MSE Loss:  [1.06881311e+03 4.34903415e+04 4.03512995e+03 ... 2.70519456e+01\n",
      " 1.50202823e+04 1.70115057e+04]\n",
      "(1, 5) [[-265.56901081 -850.31063397  -78.15660932 -111.81671277  -55.44776004]]\n",
      "Training MSE Loss:  [1.06923590e+03 4.34540526e+04 4.02335210e+03 ... 2.62401681e+01\n",
      " 1.49880666e+04 1.70085180e+04]\n",
      "(1, 5) [[-265.18575723 -847.13282566  -78.09898205 -112.26441828  -56.25208198]]\n",
      "Training MSE Loss:  [1.06966141e+03 4.34179294e+04 4.01163183e+03 ... 2.54441133e+01\n",
      " 1.49559568e+04 1.70055750e+04]\n",
      "(1, 5) [[-264.80290502 -843.96965903  -78.0409426  -112.70743869  -57.05007782]]\n",
      "Training MSE Loss:  [1.07008963e+03 4.33819711e+04 3.99996877e+03 ... 2.46636082e+01\n",
      " 1.49239525e+04 1.70026763e+04]\n",
      "(1, 5) [[-264.42045335 -840.82105914  -77.98249512 -113.14580566  -57.84178493]]\n",
      "Training MSE Loss:  [1.07052053e+03 4.33461766e+04 3.98836257e+03 ... 2.38984819e+01\n",
      " 1.48920532e+04 1.69998218e+04]\n",
      "(1, 5) [[-264.0384014  -837.68695146  -77.92364376 -113.57955059  -58.62724051]]\n",
      "Training MSE Loss:  [1.07095409e+03 4.33105451e+04 3.97681288e+03 ... 2.31485650e+01\n",
      " 1.48602585e+04 1.69970110e+04]\n",
      "(1, 5) [[-263.65674837 -834.56726186  -77.86439259 -114.00870474  -59.40648154]]\n",
      "Training MSE Loss:  [1.07139030e+03 4.32750756e+04 3.96531937e+03 ... 2.24136901e+01\n",
      " 1.48285680e+04 1.69942438e+04]\n",
      "(1, 5) [[-263.27549347 -831.46191661  -77.80474567 -114.43329914  -60.17954481]]\n",
      "Training MSE Loss:  [1.07182912e+03 4.32397673e+04 3.95388167e+03 ... 2.16936914e+01\n",
      " 1.47969812e+04 1.69915197e+04]\n",
      "(1, 5) [[-262.89463592 -828.37084237  -77.74470704 -114.85336465  -60.9464669 ]]\n",
      "Training MSE Loss:  [1.07227054e+03 4.32046193e+04 3.94249945e+03 ... 2.09884047e+01\n",
      " 1.47654977e+04 1.69888384e+04]\n",
      "(1, 5) [[-262.51417494 -825.29396617  -77.68428069 -115.26893192  -61.7072842 ]]\n",
      "Training MSE Loss:  [1.07271455e+03 4.31696306e+04 3.93117238e+03 ... 2.02976677e+01\n",
      " 1.47341171e+04 1.69861998e+04]\n",
      "(1, 5) [[-262.1341098  -822.23121547  -77.62347058 -115.68003143  -62.46203287]]\n",
      "Training MSE Loss:  [1.07316111e+03 4.31348004e+04 3.91990010e+03 ... 1.96213197e+01\n",
      " 1.47028390e+04 1.69836035e+04]\n",
      "(1, 5) [[-261.75443975 -819.18251808  -77.56228065 -116.08669346  -63.21074892]]\n",
      "Training MSE Loss:  [1.07361021e+03 4.31001278e+04 3.90868229e+03 ... 1.89592016e+01\n",
      " 1.46716629e+04 1.69810491e+04]\n",
      "(1, 5) [[-261.37516406 -816.14780223  -77.50071479 -116.48894811  -63.95346814]]\n",
      "Training MSE Loss:  [1.07406183e+03 4.30656120e+04 3.89751862e+03 ... 1.83111560e+01\n",
      " 1.46405885e+04 1.69785365e+04]\n",
      "(1, 5) [[-260.99628202 -813.12699651  -77.43877688 -116.88682527  -64.69022611]]\n",
      "Training MSE Loss:  [1.07451595e+03 4.30312520e+04 3.88640876e+03 ... 1.76770272e+01\n",
      " 1.46096154e+04 1.69760653e+04]\n",
      "(1, 5) [[-260.61779292 -810.12002989  -77.37647074 -117.28035467  -65.42105824]]\n",
      "Training MSE Loss:  [1.07497254e+03 4.29970470e+04 3.87535237e+03 ... 1.70566609e+01\n",
      " 1.45787430e+04 1.69736353e+04]\n",
      "(1, 5) [[-260.23969609 -807.12683174  -77.3138002  -117.66956585  -66.14599973]]\n",
      "Training MSE Loss:  [1.07543160e+03 4.29629961e+04 3.86434913e+03 ... 1.64499047e+01\n",
      " 1.45479711e+04 1.69712460e+04]\n",
      "(1, 5) [[-259.86199084 -804.14733178  -77.25076903 -118.05448816  -66.8650856 ]]\n",
      "Training MSE Loss:  [1.07589309e+03 4.29290985e+04 3.85339872e+03 ... 1.58566073e+01\n",
      " 1.45172991e+04 1.69688974e+04]\n",
      "(1, 5) [[-259.4846765  -801.18146013  -77.18738097 -118.43515078  -67.57835067]]\n",
      "Training MSE Loss:  [1.07635701e+03 4.28953534e+04 3.84250082e+03 ... 1.52766195e+01\n",
      " 1.44867268e+04 1.69665890e+04]\n",
      "(1, 5) [[-259.10775242 -798.22914727  -77.12363974 -118.8115827   -68.28582956]]\n",
      "Training MSE Loss:  [1.07682333e+03 4.28617599e+04 3.83165511e+03 ... 1.47097933e+01\n",
      " 1.44562536e+04 1.69643207e+04]\n",
      "(1, 5) [[-258.73121796 -795.29032405  -77.05954904 -119.18381272  -68.98755673]]\n",
      "Training MSE Loss:  [1.07729203e+03 4.28283171e+04 3.82086127e+03 ... 1.41559824e+01\n",
      " 1.44258792e+04 1.69620921e+04]\n",
      "(1, 5) [[-258.35507249 -792.36492169  -76.99511253 -119.55186947  -69.68356643]]\n",
      "Training MSE Loss:  [1.07776309e+03 4.27950242e+04 3.81011900e+03 ... 1.36150419e+01\n",
      " 1.43956033e+04 1.69599029e+04]\n",
      "(1, 5) [[-257.97931539 -789.45287179  -76.93033382 -119.91578143  -70.37389271]]\n",
      "Training MSE Loss:  [1.07823649e+03 4.27618805e+04 3.79942796e+03 ... 1.30868283e+01\n",
      " 1.43654253e+04 1.69577529e+04]\n",
      "(1, 5) [[-257.60394605 -786.55410629  -76.86521653 -120.27557685  -71.05856947]]\n",
      "Training MSE Loss:  [1.07871222e+03 4.27288850e+04 3.78878787e+03 ... 1.25711999e+01\n",
      " 1.43353449e+04 1.69556418e+04]\n",
      "(1, 5) [[-257.22896387 -783.66855751  -76.79976423 -120.63128385  -71.73763039]]\n",
      "Training MSE Loss:  [1.07919025e+03 4.26960370e+04 3.77819841e+03 ... 1.20680162e+01\n",
      " 1.43053618e+04 1.69535694e+04]\n",
      "(1, 5) [[-256.85436827 -780.79615814  -76.73398047 -120.98293034  -72.41110898]]\n",
      "Training MSE Loss:  [1.07967057e+03 4.26633356e+04 3.76765927e+03 ... 1.15771383e+01\n",
      " 1.42754755e+04 1.69515353e+04]\n",
      "(1, 5) [[-256.48015866 -777.93684121  -76.66786875 -121.33054409  -73.07903857]]\n",
      "Training MSE Loss:  [1.08015316e+03 4.26307801e+04 3.75717015e+03 ... 1.10984286e+01\n",
      " 1.42456856e+04 1.69495394e+04]\n",
      "(1, 5) [[-256.10633449 -775.09054012  -76.60143258 -121.67415268  -73.7414523 ]]\n",
      "Training MSE Loss:  [1.08063800e+03 4.25983697e+04 3.74673075e+03 ... 1.06317509e+01\n",
      " 1.42159917e+04 1.69475813e+04]\n",
      "(1, 5) [[-255.7328952  -772.25718861  -76.53467541 -122.01378352  -74.39838313]]\n",
      "Training MSE Loss:  [1.08112507e+03 4.25661035e+04 3.73634078e+03 ... 1.01769707e+01\n",
      " 1.41863935e+04 1.69456607e+04]\n",
      "(1, 5) [[-255.35984024 -769.43672078  -76.46760067 -122.34946384  -75.04986384]]\n",
      "Training MSE Loss:  [1.08161435e+03 4.25339807e+04 3.72599993e+03 ... 9.73395456e+00\n",
      " 1.41568906e+04 1.69437775e+04]\n",
      "(1, 5) [[-254.98716908 -766.6290711   -76.40021179 -122.68122072  -75.69592704]]\n",
      "Training MSE Loss:  [1.08210583e+03 4.25020006e+04 3.71570791e+03 ... 9.30257058e+00\n",
      " 1.41274825e+04 1.69419313e+04]\n",
      "(1, 5) [[-254.6148812  -763.83417437  -76.33251212 -123.00908106  -76.33660514]]\n",
      "Training MSE Loss:  [1.08259949e+03 4.24701625e+04 3.70546442e+03 ... 8.88268819e+00\n",
      " 1.40981690e+04 1.69401219e+04]\n",
      "(1, 5) [[-254.24297607 -761.05196573  -76.26450504 -123.33307159  -76.97193039]]\n",
      "Training MSE Loss:  [1.08309530e+03 4.24384654e+04 3.69526919e+03 ... 8.47417817e+00\n",
      " 1.40689496e+04 1.69383490e+04]\n",
      "(1, 5) [[-253.8714532  -758.28238068  -76.19619387 -123.65321889  -77.60193485]]\n",
      "Training MSE Loss:  [1.08359326e+03 4.24069087e+04 3.68512191e+03 ... 8.07691263e+00\n",
      " 1.40398240e+04 1.69366124e+04]\n",
      "(1, 5) [[-253.50031209 -755.52535507  -76.12758191 -123.96954935  -78.22665041]]\n",
      "Training MSE Loss:  [1.08409334e+03 4.23754916e+04 3.67502231e+03 ... 7.69076502e+00\n",
      " 1.40107918e+04 1.69349119e+04]\n",
      "(1, 5) [[-253.12955224 -752.78082508  -76.05867243 -124.28208922  -78.8461088 ]]\n",
      "Training MSE Loss:  [1.08459553e+03 4.23442133e+04 3.66497009e+03 ... 7.31561008e+00\n",
      " 1.39818526e+04 1.69332471e+04]\n",
      "(1, 5) [[-252.7591732  -750.04872722  -75.98946869 -124.59086457  -79.46034155]]\n",
      "Training MSE Loss:  [1.08509980e+03 4.23130731e+04 3.65496498e+03 ... 6.95132386e+00\n",
      " 1.39530060e+04 1.69316179e+04]\n",
      "(1, 5) [[-252.38917448 -747.32899837  -75.91997391 -124.89590132  -80.06938002]]\n",
      "Training MSE Loss:  [1.08560615e+03 4.22820702e+04 3.64500669e+03 ... 6.59778369e+00\n",
      " 1.39242517e+04 1.69300239e+04]\n",
      "(1, 5) [[-252.01955562 -744.62157571  -75.8501913  -125.19722521  -80.67325543]]\n",
      "Training MSE Loss:  [1.08611455e+03 4.22512039e+04 3.63509495e+03 ... 6.25486815e+00\n",
      " 1.38955894e+04 1.69284650e+04]\n",
      "(1, 5) [[-251.65031619 -741.92639678  -75.78012401 -125.49486184  -81.27199878]]\n",
      "Training MSE Loss:  [1.08662498e+03 4.22204734e+04 3.62522948e+03 ... 5.92245711e+00\n",
      " 1.38670185e+04 1.69269408e+04]\n",
      "(1, 5) [[-251.28145573 -739.24339945  -75.70977522 -125.78883665  -81.86564093]]\n",
      "Training MSE Loss:  [1.08713744e+03 4.21898779e+04 3.61541000e+03 ... 5.60043164e+00\n",
      " 1.38385389e+04 1.69254512e+04]\n",
      "(1, 5) [[-250.91297382 -736.5725219   -75.63914803 -126.0791749   -82.45421257]]\n",
      "Training MSE Loss:  [1.08765189e+03 4.21594169e+04 3.60563624e+03 ... 5.28867409e+00\n",
      " 1.38101501e+04 1.69239959e+04]\n",
      "(1, 5) [[-250.54487002 -733.91370267  -75.56824555 -126.36590171  -83.03774421]]\n",
      "Training MSE Loss:  [1.08816834e+03 4.21290894e+04 3.59590794e+03 ... 4.98706799e+00\n",
      " 1.37818517e+04 1.69225747e+04]\n",
      "(1, 5) [[-250.17714393 -731.26688059  -75.49707086 -126.64904204  -83.61626621]]\n",
      "Training MSE Loss:  [1.08868675e+03 4.20988949e+04 3.58622481e+03 ... 4.69549809e+00\n",
      " 1.37536435e+04 1.69211872e+04]\n",
      "(1, 5) [[-249.80979514 -728.63199486  -75.42562701 -126.9286207   -84.18980873]]\n",
      "Training MSE Loss:  [1.08920712e+03 4.20688325e+04 3.57658660e+03 ... 4.41385032e+00\n",
      " 1.37255250e+04 1.69198334e+04]\n",
      "(1, 5) [[-249.44282324 -726.00898497  -75.35391703 -127.20466234  -84.7584018 ]]\n",
      "Training MSE Loss:  [1.08972942e+03 4.20389016e+04 3.56699304e+03 ... 4.14201181e+00\n",
      " 1.36974959e+04 1.69185129e+04]\n",
      "(1, 5) [[-249.07622785 -723.39779075  -75.28194392 -127.47719145  -85.32207526]]\n",
      "Training MSE Loss:  [1.09025364e+03 4.20091015e+04 3.55744387e+03 ... 3.87987086e+00\n",
      " 1.36695558e+04 1.69172255e+04]\n",
      "(1, 5) [[-248.71000859 -720.79835233  -75.20971066 -127.74623237  -85.88085879]]\n",
      "Training MSE Loss:  [1.09077976e+03 4.19794314e+04 3.54793882e+03 ... 3.62731689e+00\n",
      " 1.36417045e+04 1.69159710e+04]\n",
      "(1, 5) [[-248.34416507 -718.21061018  -75.13722021 -128.01180931  -86.43478193]]\n",
      "Training MSE Loss:  [1.09130776e+03 4.19498907e+04 3.53847763e+03 ... 3.38424051e+00\n",
      " 1.36139415e+04 1.69147491e+04]\n",
      "(1, 5) [[-247.97869693 -715.63450507  -75.06447551 -128.27394629  -86.98387403]]\n",
      "Training MSE Loss:  [1.09183764e+03 4.19204786e+04 3.52906006e+03 ... 3.15053345e+00\n",
      " 1.35862665e+04 1.69135597e+04]\n",
      "(1, 5) [[-247.61360381 -713.06997811  -74.99147947 -128.53266721  -87.52816428]]\n",
      "Training MSE Loss:  [1.09236937e+03 4.18911944e+04 3.51968583e+03 ... 2.92608853e+00\n",
      " 1.35586791e+04 1.69124024e+04]\n",
      "(1, 5) [[-247.24888536 -710.51697068  -74.91823498 -128.78799582  -88.06768172]]\n",
      "Training MSE Loss:  [1.09290294e+03 4.18620376e+04 3.51035469e+03 ... 2.71079972e+00\n",
      " 1.35311791e+04 1.69112771e+04]\n",
      "(1, 5) [[-246.88454123 -707.97542453  -74.84474491 -129.03995569  -88.60245523]]\n",
      "Training MSE Loss:  [1.09343834e+03 4.18330073e+04 3.50106640e+03 ... 2.50456208e+00\n",
      " 1.35037660e+04 1.69101835e+04]\n",
      "(1, 5) [[-246.52057109 -705.44528166  -74.77101209 -129.2885703   -89.13251352]]\n",
      "Training MSE Loss:  [1.09397553e+03 4.18041029e+04 3.49182071e+03 ... 2.30727174e+00\n",
      " 1.34764396e+04 1.69091215e+04]\n",
      "(1, 5) [[-246.15697459 -702.92648442  -74.69703936 -129.53386292  -89.65788515]]\n",
      "Training MSE Loss:  [1.09451452e+03 4.17753238e+04 3.48261735e+03 ... 2.11882592e+00\n",
      " 1.34491994e+04 1.69080907e+04]\n",
      "(1, 5) [[-245.79375143 -700.41897546  -74.62282951 -129.77585672  -90.17859853]]\n",
      "Training MSE Loss:  [1.09505528e+03 4.17466692e+04 3.47345609e+03 ... 1.93912290e+00\n",
      " 1.34220452e+04 1.69070910e+04]\n",
      "(1, 5) [[-245.43090128 -697.92269771  -74.54838533 -130.0145747   -90.6946819 ]]\n",
      "Training MSE Loss:  [1.09559781e+03 4.17181385e+04 3.46433668e+03 ... 1.76806201e+00\n",
      " 1.33949766e+04 1.69061221e+04]\n",
      "(1, 5) [[-245.06842383 -695.43759443  -74.47370956 -130.25003974  -91.20616334]]\n",
      "Training MSE Loss:  [1.09614207e+03 4.16897310e+04 3.45525888e+03 ... 1.60554365e+00\n",
      " 1.33679933e+04 1.69051839e+04]\n",
      "(1, 5) [[-244.70631877 -692.96360917  -74.39880496 -130.48227455  -91.7130708 ]]\n",
      "Training MSE Loss:  [1.09668806e+03 4.16614460e+04 3.44622243e+03 ... 1.45146922e+00\n",
      " 1.33410950e+04 1.69042760e+04]\n",
      "(1, 5) [[-244.34458582 -690.50068578  -74.32367422 -130.71130171  -92.21543204]]\n",
      "Training MSE Loss:  [1.09723577e+03 4.16332830e+04 3.43722711e+03 ... 1.30574115e+00\n",
      " 1.33142813e+04 1.69033984e+04]\n",
      "(1, 5) [[-243.98322468 -688.04876842  -74.24832005 -130.93714368  -92.71327469]]\n",
      "Training MSE Loss:  [1.09778518e+03 4.16052413e+04 3.42827268e+03 ... 1.16826289e+00\n",
      " 1.32875519e+04 1.69025507e+04]\n",
      "(1, 5) [[-243.62223507 -685.60780153  -74.17274511 -131.15982274  -93.20662624]]\n",
      "Training MSE Loss:  [1.09833626e+03 4.15773201e+04 3.41935889e+03 ... 1.03893888e+00\n",
      " 1.32609064e+04 1.69017328e+04]\n",
      "(1, 5) [[-243.2616167  -683.17772986  -74.09695207 -131.37936105  -93.69551399]]\n",
      "Training MSE Loss:  [1.09888902e+03 4.15495189e+04 3.41048550e+03 ... 9.17674576e-01\n",
      " 1.32343446e+04 1.69009445e+04]\n",
      "(1, 5) [[-242.90136932 -680.75849844  -74.02094354 -131.59578065  -94.17996513]]\n",
      "Training MSE Loss:  [1.09944342e+03 4.15218370e+04 3.40165230e+03 ... 8.04376384e-01\n",
      " 1.32078661e+04 1.69001855e+04]\n",
      "(1, 5) [[-242.54149264 -678.35005259  -73.94472215 -131.80910341  -94.66000666]]\n",
      "Training MSE Loss:  [1.09999947e+03 4.14942739e+04 3.39285903e+03 ... 6.98951697e-01\n",
      " 1.31814706e+04 1.68994556e+04]\n",
      "(1, 5) [[-242.18198642 -675.95233795  -73.86829048 -132.01935108  -95.13566547]]\n",
      "Training MSE Loss:  [1.10055714e+03 4.14668287e+04 3.38410548e+03 ... 6.01308867e-01\n",
      " 1.31551577e+04 1.68987546e+04]\n",
      "(1, 5) [[-241.8228504  -673.5653004   -73.79165111 -132.22654527  -95.60696827]]\n",
      "Training MSE Loss:  [1.10111641e+03 4.14395010e+04 3.37539141e+03 ... 5.11357195e-01\n",
      " 1.31289272e+04 1.68980824e+04]\n",
      "(1, 5) [[-241.46408433 -671.18888616  -73.71480658 -132.43070745  -96.07394164]]\n",
      "Training MSE Loss:  [1.10167728e+03 4.14122901e+04 3.36671659e+03 ... 4.29006924e-01\n",
      " 1.31027788e+04 1.68974387e+04]\n",
      "(1, 5) [[-241.10568797 -668.82304169  -73.63775942 -132.63185898  -96.53661201]]\n",
      "Training MSE Loss:  [1.10223973e+03 4.13851954e+04 3.35808081e+03 ... 3.54169230e-01\n",
      " 1.30767120e+04 1.68968233e+04]\n",
      "(1, 5) [[-240.74766109 -666.46771377  -73.56051216 -132.83002105  -96.99500566]]\n",
      "Training MSE Loss:  [1.10280374e+03 4.13582163e+04 3.34948382e+03 ... 2.86756211e-01\n",
      " 1.30507267e+04 1.68962360e+04]\n",
      "(1, 5) [[-240.39000345 -664.12284943  -73.48306727 -133.02521474  -97.44914872]]\n",
      "Training MSE Loss:  [1.10336930e+03 4.13313521e+04 3.34092542e+03 ... 2.26680876e-01\n",
      " 1.30248225e+04 1.68956766e+04]\n",
      "(1, 5) [[-240.03271483 -661.78839601  -73.40542724 -133.21746099  -97.89906718]]\n",
      "Training MSE Loss:  [1.10393640e+03 4.13046022e+04 3.33240537e+03 ... 1.73857139e-01\n",
      " 1.29989990e+04 1.68951449e+04]\n",
      "(1, 5) [[-239.675795   -659.46430112  -73.32759452 -133.40678063  -98.3447869 ]]\n",
      "Training MSE Loss:  [1.10450502e+03 4.12779661e+04 3.32392346e+03 ... 1.28199807e-01\n",
      " 1.29732560e+04 1.68946407e+04]\n",
      "(1, 5) [[-239.31924376 -657.15051263  -73.24957154 -133.59319432  -98.78633358]]\n",
      "Training MSE Loss:  [1.10507515e+03 4.12514431e+04 3.31547947e+03 ... 8.96245729e-02\n",
      " 1.29475932e+04 1.68941638e+04]\n",
      "(1, 5) [[-238.9630609  -654.84697871  -73.17136071 -133.77672262  -99.22373278]]\n",
      "Training MSE Loss:  [1.10564678e+03 4.12250327e+04 3.30707318e+03 ... 5.80480056e-02\n",
      " 1.29220102e+04 1.68937140e+04]\n",
      "(1, 5) [[-238.60724621 -652.5536478   -73.09296444 -133.95738595  -99.65700992]]\n",
      "Training MSE Loss:  [1.10621988e+03 4.11987342e+04 3.29870438e+03 ... 3.33875404e-02\n",
      " 1.28965068e+04 1.68932912e+04]\n",
      "(1, 5) [[-238.25179948 -650.2704686   -73.01438511 -134.13520461 -100.08619027]]\n",
      "Training MSE Loss:  [1.10679444e+03 4.11725470e+04 3.29037284e+03 ... 1.55614710e-02\n",
      " 1.28710826e+04 1.68928950e+04]\n",
      "(1, 5) [[-237.89672054 -647.9973901   -72.93562507 -134.31019876 -100.51129899]]\n",
      "Training MSE Loss:  [1.10737046e+03 4.11464706e+04 3.28207837e+03 ... 4.48894028e-03\n",
      " 1.28457373e+04 1.68925254e+04]\n",
      "(1, 5) [[-237.54200918 -645.73436155  -72.85668667 -134.48238845 -100.93236107]]\n",
      "Training MSE Loss:  [1.10794792e+03 4.11205044e+04 3.27382074e+03 ... 8.99319664e-05\n",
      " 1.28204707e+04 1.68921822e+04]\n",
      "(1, 5) [[-237.18766523 -643.48133246  -72.77757223 -134.6517936  -101.34940137]]\n",
      "Training MSE Loss:  [1.10852681e+03 4.10946478e+04 3.26559975e+03 ... 2.28526171e-03\n",
      " 1.27952824e+04 1.68918651e+04]\n",
      "(1, 5) [[-236.83368849 -641.23825264  -72.69828407 -134.81843398 -101.76244461]]\n",
      "Training MSE Loss:  [1.10910710e+03 4.10689002e+04 3.25741519e+03 ... 1.09965687e-02\n",
      " 1.27701721e+04 1.68915739e+04]\n",
      "(1, 5) [[-236.48007881 -639.00507213  -72.61882445 -134.98232928 -102.17151538]]\n",
      "Training MSE Loss:  [1.10968879e+03 4.10432611e+04 3.24926685e+03 ... 2.61463073e-02\n",
      " 1.27451396e+04 1.68913085e+04]\n",
      "(1, 5) [[-236.126836   -636.78174126  -72.53919567 -135.14349903 -102.57663814]]\n",
      "Training MSE Loss:  [1.11027187e+03 4.10177299e+04 3.24115453e+03 ... 4.76577385e-02\n",
      " 1.27201845e+04 1.68910687e+04]\n",
      "(1, 5) [[-235.7739599  -634.5682106   -72.45939997 -135.30196265 -102.97783719]]\n",
      "Training MSE Loss:  [1.11085631e+03 4.09923059e+04 3.23307801e+03 ... 7.54549221e-02\n",
      " 1.26953066e+04 1.68908543e+04]\n",
      "(1, 5) [[-235.42145036 -632.364431    -72.37943958 -135.45773944 -103.37513671]]\n",
      "Training MSE Loss:  [1.11144212e+03 4.09669887e+04 3.22503711e+03 ... 1.09462708e-01\n",
      " 1.26705056e+04 1.68906651e+04]\n",
      "(1, 5) [[-235.0693072  -630.17035357  -72.29931673 -135.61084858 -103.76856076]]\n",
      "Training MSE Loss:  [1.11202927e+03 4.09417777e+04 3.21703161e+03 ... 1.49606728e-01\n",
      " 1.26457811e+04 1.68905009e+04]\n",
      "(1, 5) [[-234.71753028 -627.98592967  -72.21903362 -135.76130912 -104.15813323]]\n",
      "Training MSE Loss:  [1.11261775e+03 4.09166723e+04 3.20906131e+03 ... 1.95813390e-01\n",
      " 1.26211328e+04 1.68903616e+04]\n",
      "(1, 5) [[-234.36611945 -625.81111092  -72.13859244 -135.90914001 -104.54387792]]\n",
      "Training MSE Loss:  [1.11320755e+03 4.08916721e+04 3.20112603e+03 ... 2.48009865e-01\n",
      " 1.25965606e+04 1.68902469e+04]\n",
      "(1, 5) [[-234.01507456 -623.6458492   -72.05799534 -136.05436005 -104.92581846]]\n",
      "Training MSE Loss:  [1.11379865e+03 4.08667763e+04 3.19322555e+03 ... 3.06124085e-01\n",
      " 1.25720640e+04 1.68901568e+04]\n",
      "(1, 5) [[-233.66439547 -621.49009665  -71.97724449 -136.19698796 -105.30397838]]\n",
      "Training MSE Loss:  [1.11439105e+03 4.08419845e+04 3.18535968e+03 ... 3.70084733e-01\n",
      " 1.25476429e+04 1.68900909e+04]\n",
      "(1, 5) [[-233.31408204 -619.34380564  -71.89634201 -136.3370423  -105.67838105]]\n",
      "Training MSE Loss:  [1.11498472e+03 4.08172962e+04 3.17752823e+03 ... 4.39821234e-01\n",
      " 1.25232969e+04 1.68900492e+04]\n",
      "(1, 5) [[-232.96413415 -617.20692882  -71.81529003 -136.47454156 -106.04904973]]\n",
      "Training MSE Loss:  [1.11557967e+03 4.07927108e+04 3.16973101e+03 ... 5.15263749e-01\n",
      " 1.24990257e+04 1.68900314e+04]\n",
      "(1, 5) [[-232.61455166 -615.07941908  -71.73409065 -136.60950407 -106.41600755]]\n",
      "Training MSE Loss:  [1.11617587e+03 4.07682277e+04 3.16196782e+03 ... 5.96343165e-01\n",
      " 1.24748291e+04 1.68900374e+04]\n",
      "(1, 5) [[-232.26533444 -612.96122955  -71.65274595 -136.74194807 -106.7792775 ]]\n",
      "Training MSE Loss:  [1.11677330e+03 4.07438465e+04 3.15423846e+03 ... 6.82991093e-01\n",
      " 1.24507067e+04 1.68900670e+04]\n",
      "(1, 5) [[-231.91648237 -610.85231362  -71.57125801 -136.87189168 -107.13888245]]\n",
      "Training MSE Loss:  [1.11737197e+03 4.07195666e+04 3.14654276e+03 ... 7.75139855e-01\n",
      " 1.24266584e+04 1.68901201e+04]\n",
      "(1, 5) [[-231.56799534 -608.75262492  -71.48962887 -136.9993529  -107.49484515]]\n",
      "Training MSE Loss:  [1.11797186e+03 4.06953874e+04 3.13888052e+03 ... 8.72722480e-01\n",
      " 1.24026838e+04 1.68901964e+04]\n",
      "(1, 5) [[-231.21987322 -606.66211733  -71.40786058 -137.12434963 -107.8471882 ]]\n",
      "Training MSE Loss:  [1.11857295e+03 4.06713085e+04 3.13125156e+03 ... 9.75672693e-01\n",
      " 1.23787826e+04 1.68902958e+04]\n",
      "(1, 5) [[-230.87211591 -604.58074498  -71.32595515 -137.24689964 -108.19593409]]\n",
      "Training MSE Loss:  [1.11917523e+03 4.06473294e+04 3.12365568e+03 ... 1.08392491e+00\n",
      " 1.23549546e+04 1.68904181e+04]\n",
      "(1, 5) [[-230.5247233  -602.50846223  -71.2439146  -137.3670206  -108.5411052 ]]\n",
      "Training MSE Loss:  [1.11977869e+03 4.06234494e+04 3.11609271e+03 ... 1.19741425e+00\n",
      " 1.23311994e+04 1.68905631e+04]\n",
      "(1, 5) [[-230.17769528 -600.44522367  -71.16174091 -137.48473008 -108.88272376]]\n",
      "Training MSE Loss:  [1.12038333e+03 4.05996681e+04 3.10856245e+03 ... 1.31607647e+00\n",
      " 1.23075169e+04 1.68907308e+04]\n",
      "(1, 5) [[-229.83103176 -598.39098417  -71.07943607 -137.6000455  -109.22081187]]\n",
      "Training MSE Loss:  [1.12098911e+03 4.05759851e+04 3.10106474e+03 ... 1.43984804e+00\n",
      " 1.22839068e+04 1.68909208e+04]\n",
      "(1, 5) [[-229.48473262 -596.3456988   -70.99700203 -137.71298422 -109.55539155]]\n",
      "Training MSE Loss:  [1.12159604e+03 4.05523997e+04 3.09359938e+03 ... 1.56866606e+00\n",
      " 1.22603687e+04 1.68911331e+04]\n",
      "(1, 5) [[-229.13879778 -594.30932289  -70.91444074 -137.82356344 -109.88648465]]\n",
      "Training MSE Loss:  [1.12220410e+03 4.05289115e+04 3.08616619e+03 ... 1.70246831e+00\n",
      " 1.22369024e+04 1.68913675e+04]\n",
      "(1, 5) [[-228.79322714 -592.281812    -70.83175412 -137.9318003  -110.21411292]]\n",
      "Training MSE Loss:  [1.12281328e+03 4.05055199e+04 3.07876501e+03 ... 1.84119321e+00\n",
      " 1.22135077e+04 1.68916238e+04]\n",
      "(1, 5) [[-228.44802061 -590.26312191  -70.7489441  -138.0377118  -110.53829798]]\n",
      "Training MSE Loss:  [1.12342357e+03 4.04822246e+04 3.07139564e+03 ... 1.98477982e+00\n",
      " 1.21901842e+04 1.68919019e+04]\n",
      "(1, 5) [[-228.10317812 -588.25320866  -70.66601258 -138.14131485 -110.85906136]]\n",
      "Training MSE Loss:  [1.12403496e+03 4.04590249e+04 3.06405792e+03 ... 2.13316786e+00\n",
      " 1.21669317e+04 1.68922015e+04]\n",
      "(1, 5) [[-227.75869956 -586.2520285   -70.58296144 -138.24262623 -111.17642441]]\n",
      "Training MSE Loss:  [1.12464743e+03 4.04359204e+04 3.05675166e+03 ... 2.28629764e+00\n",
      " 1.21437500e+04 1.68925226e+04]\n",
      "(1, 5) [[-227.41458487 -584.25953793  -70.49979256 -138.34166264 -111.49040843]]\n",
      "Training MSE Loss:  [1.12526097e+03 4.04129106e+04 3.04947670e+03 ... 2.44411014e+00\n",
      " 1.21206387e+04 1.68928650e+04]\n",
      "(1, 5) [[-227.07083396 -582.27569367  -70.41650778 -138.43844067 -111.80103454]]\n",
      "Training MSE Loss:  [1.12587558e+03 4.03899949e+04 3.04223286e+03 ... 2.60654691e+00\n",
      " 1.20975977e+04 1.68932285e+04]\n",
      "(1, 5) [[-226.72744675 -580.30045265  -70.33310897 -138.53297679 -112.10832379]]\n",
      "Training MSE Loss:  [1.12649123e+03 4.03671730e+04 3.03501997e+03 ... 2.77355015e+00\n",
      " 1.20746266e+04 1.68936129e+04]\n",
      "(1, 5) [[-226.38442318 -578.33377207  -70.24959794 -138.62528737 -112.41229708]]\n",
      "Training MSE Loss:  [1.12710792e+03 4.03444444e+04 3.02783786e+03 ... 2.94506265e+00\n",
      " 1.20517252e+04 1.68940182e+04]\n",
      "(1, 5) [[-226.04176318 -576.37560932  -70.1659765  -138.7153887  -112.71297521]]\n",
      "Training MSE Loss:  [1.12772565e+03 4.03218085e+04 3.02068635e+03 ... 3.12102780e+00\n",
      " 1.20288932e+04 1.68944441e+04]\n",
      "(1, 5) [[-225.69946667 -574.42592203  -70.08224647 -138.80329694 -113.01037885]]\n",
      "Training MSE Loss:  [1.12834438e+03 4.02992649e+04 3.01356529e+03 ... 3.30138957e+00\n",
      " 1.20061304e+04 1.68948905e+04]\n",
      "(1, 5) [[-225.35753359 -572.48466805  -69.99840963 -138.88902815 -113.30452858]]\n",
      "Training MSE Loss:  [1.12896412e+03 4.02768131e+04 3.00647450e+03 ... 3.48609255e+00\n",
      " 1.19834366e+04 1.68953572e+04]\n",
      "(1, 5) [[-225.01596387 -570.55180545  -69.91446775 -138.97259831 -113.59544483]]\n",
      "Training MSE Loss:  [1.12958486e+03 4.02544526e+04 2.99941381e+03 ... 3.67508188e+00\n",
      " 1.19608114e+04 1.68958441e+04]\n",
      "(1, 5) [[-224.67475747 -568.62729253  -69.83042259 -139.05402328 -113.88314795]]\n",
      "Training MSE Loss:  [1.13020657e+03 4.02321830e+04 2.99238307e+03 ... 3.86830329e+00\n",
      " 1.19382546e+04 1.68963511e+04]\n",
      "(1, 5) [[-224.33391431 -566.71108781  -69.74627591 -139.13331882 -114.16765815]]\n",
      "Training MSE Loss:  [1.13082926e+03 4.02100038e+04 2.98538210e+03 ... 4.06570306e+00\n",
      " 1.19157659e+04 1.68968779e+04]\n",
      "(1, 5) [[-223.99343434 -564.80315001  -69.66202942 -139.2105006  -114.44899553]]\n",
      "Training MSE Loss:  [1.13145291e+03 4.01879146e+04 2.97841075e+03 ... 4.26722806e+00\n",
      " 1.18933452e+04 1.68974245e+04]\n",
      "(1, 5) [[-223.65331751 -562.90343808  -69.57768485 -139.28558418 -114.72718011]]\n",
      "Training MSE Loss:  [1.13207751e+03 4.01659148e+04 2.97146885e+03 ... 4.47282570e+00\n",
      " 1.18709921e+04 1.68979906e+04]\n",
      "(1, 5) [[-223.31356377 -561.01191121  -69.49324391 -139.35858503 -115.00223174]]\n",
      "Training MSE Loss:  [1.13270305e+03 4.01440041e+04 2.96455624e+03 ... 4.68244394e+00\n",
      " 1.18487065e+04 1.68985762e+04]\n",
      "(1, 5) [[-222.97417306 -559.12852876  -69.40870829 -139.42951852 -115.27417022]]\n",
      "Training MSE Loss:  [1.13332951e+03 4.01221820e+04 2.95767276e+03 ... 4.89603131e+00\n",
      " 1.18264880e+04 1.68991811e+04]\n",
      "(1, 5) [[-222.63514533 -557.25325034  -69.32407967 -139.49839993 -115.5430152 ]]\n",
      "Training MSE Loss:  [1.13395690e+03 4.01004479e+04 2.95081826e+03 ... 5.11353684e+00\n",
      " 1.18043364e+04 1.68998051e+04]\n",
      "(1, 5) [[-222.29648055 -555.38603575  -69.23935971 -139.56524442 -115.80878622]]\n",
      "Training MSE Loss:  [1.13458519e+03 4.00788016e+04 2.94399257e+03 ... 5.33491014e+00\n",
      " 1.17822515e+04 1.69004481e+04]\n",
      "(1, 5) [[-221.95817867 -553.52684502  -69.15455008 -139.63006708 -116.07150274]]\n",
      "Training MSE Loss:  [1.13521437e+03 4.00572424e+04 2.93719553e+03 ... 5.56010131e+00\n",
      " 1.17602330e+04 1.69011100e+04]\n",
      "(1, 5) [[-221.62023964 -551.67563838  -69.06965241 -139.69288289 -116.33118408]]\n",
      "Training MSE Loss:  [1.13584444e+03 4.00357701e+04 2.93042700e+03 ... 5.78906099e+00\n",
      " 1.17382806e+04 1.69017906e+04]\n",
      "(1, 5) [[-221.28266343 -549.83237628  -68.98466834 -139.75370675 -116.58784947]]\n",
      "Training MSE Loss:  [1.13647539e+03 4.00143841e+04 2.92368682e+03 ... 6.02174036e+00\n",
      " 1.17163942e+04 1.69024897e+04]\n",
      "(1, 5) [[-220.94544999 -547.99701936  -68.89959947 -139.81255343 -116.84151801]]\n",
      "Training MSE Loss:  [1.13710720e+03 3.99930840e+04 2.91697483e+03 ... 6.25809107e+00\n",
      " 1.16945735e+04 1.69032073e+04]\n",
      "(1, 5) [[-220.60859929 -546.16952849  -68.81444741 -139.86943766 -117.09220873]]\n",
      "Training MSE Loss:  [1.13773986e+03 3.99718694e+04 2.91029089e+03 ... 6.49806532e+00\n",
      " 1.16728183e+04 1.69039431e+04]\n",
      "(1, 5) [[-220.2721113  -544.34986472  -68.72921376 -139.92437403 -117.33994053]]\n",
      "Training MSE Loss:  [1.13837337e+03 3.99507398e+04 2.90363483e+03 ... 6.74161579e+00\n",
      " 1.16511282e+04 1.69046971e+04]\n",
      "(1, 5) [[-219.93598598 -542.53798934  -68.64390009 -139.97737705 -117.58473219]]\n",
      "Training MSE Loss:  [1.13900771e+03 3.99296948e+04 2.89700651e+03 ... 6.98869565e+00\n",
      " 1.16295032e+04 1.69054691e+04]\n",
      "(1, 5) [[-219.6002233  -540.7338638   -68.55850798 -140.02846116 -117.82660242]]\n",
      "Training MSE Loss:  [1.13964288e+03 3.99087340e+04 2.89040578e+03 ... 7.23925859e+00\n",
      " 1.16079429e+04 1.69062590e+04]\n",
      "(1, 5) [[-219.26482323 -538.93744979  -68.47303896 -140.07764068 -118.06556979]]\n",
      "Training MSE Loss:  [1.14027885e+03 3.98878569e+04 2.88383249e+03 ... 7.49325876e+00\n",
      " 1.15864470e+04 1.69070666e+04]\n"
     ]
    }
   ],
   "source": [
    "model.batch_gradient_descent_train(x_train,y_train,epochs=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[22.32777943]\n",
      " [59.18586808]\n",
      " [ 0.44239485]\n",
      " [ 1.97466326]\n",
      " [ 1.31309032]] 12.72822651396759\n"
     ]
    }
   ],
   "source": [
    "print(model.w,model.b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[84.09927678]] 82.0\n"
     ]
    }
   ],
   "source": [
    "print(model.predict(x_test.iloc[0].to_numpy()),y_test.iloc[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = []\n",
    "for i in range(x_test.shape[0]):\n",
    "    prediction.append(model.predict(x_test.iloc[i].to_numpy()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6154537342529274"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_mae(prediction,y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x15ca2aea0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(y_test,prediction)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
